{
  "hash": "fc3b1c907149a9e469b6fccb66b3e187",
  "result": {
    "markdown": "---\ntitle: \"Data Cleaning\"\nengine: knitr\nexecute:\n    freeze: true\n    eval: false\nproject:\n  execute-dir: project\n---\n\n\n## Structuring Data to Investigate Retail Activity\n\nThe most important structuring element when cleaning the datasets and tying them together is the timing of the data. The outcome variable, retail activity, is collected daily, but only on days where the stock market is open. The first input dataset, the investor sentiment survey, is collected at the end of every week about the following week. And the Reddit posts are scraped from the website with a date, but it is unknown when in the day the post was made. Ideally, if we were trying to predict retail activity on a particular day, such as a Tuesday, we would look at the survey data from the previous friday, but the activity and Reddit data from the proceeding Monday. If the Monday were a federal holiday when the market was closed, then we would look at the Reddit and activity data from the previous Friday. In cleaning the data, I chose to address this challenge at the inital stage, by moving the outcome data back a day so that all future references to the activity increase/decrease variable can be made without needing to adjust for a delay.\n\n\n## Retail Activity\n\nClean Record Data in R\n\n- [View Raw Data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/00-raw-data/ndaqRT.csv)\n- [View Clean Data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/joinedSentiment.csv)\n\n\nCleaning the retail activity record data meant:\n\n1. Reading in the data nad ordering by date (10 observations per day)\n\n2. Cleaning up various columns to ensure they were stored in the correct data format\n\n3. Dividing the data up by ticker, so that each stock can be compared against itself\n\n4. Looking at the next-most-recent day for each ticker, and subtracting that from today's total to get the difference\n\n5. Repeating the operation to get the change in sentiment\n\n6. Flag the ticker as new to the top 10 if no immediately proceeding date was found\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(reticulate)\n\ndata <- read.csv('../data/00-raw-data/ndaqRT.csv')\n\n# filter to the desired year\ncleanData <- data %>% \n    mutate(date = ymd(date)) %>%\n    filter(year(date) > 2019)\n\n# adjust data types as needed\ncleanData <- cleanData[order(cleanData$date),]\ncleanData$deltaActivity <- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$deltaSentiment <- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$newEntry <- vector(mode = 'logical', length = nrow(cleanData))\n\n\n# Looping through all entries in the dataset, generate the daily change in activity and lag the result by one day.\nfor(i in 1:nrow(cleanData)) {\n    if(i %% 1000 == 0) {\n        print(i)\n    }\n\n    # Make sure that we are comparing each stock against itself day over day\n    prevDay <- cleanData %>%\n        filter(day(date) == day(cleanData$date[i]) - 1) %>%\n        filter(ticker == cleanData$ticker[i])\n\n    if(nrow(prevDay) > 0) {\n\n        # calculate the day over day change and whether the stock is new to the top 10\n        cleanData$deltaActivity[i] = cleanData$activity[i] - prevDay$activity[1]\n        cleanData$deltaSentiment[i] = cleanData$sentiment[i] - prevDay$sentiment[1]\n        cleanData$newEntry[i] = FALSE\n    \n    }else {\n        cleanData$newEntry[i] = TRUE\n    }\n}\n\ncolnames(cleanData)\nhist(cleanData$deltaActivity)\n\nhead(cleanData, 100)\n```\n:::\n\n\n\n\n## Investor Sentiment\n\n- [View raw data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/00-raw-data/sentiment_aaii.csv)\n- [View clean data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/joinedSentiment.csv)\n\nGathering the investor sentiment data was easy, the challenging part was merging it with the daily retail trading activity, as the investor sentiment data was weekly. To properly join the data, I needed to have the investor sentiment survey apply to the previous week's observations. I did this by:\n\n1. Reading in the data and ensuring the date of the survey was stored in proper format.\n\n2. Calculating the week of the year that each survey was taken on.\n\n3. Adding one to the week, resetting past 53, and joining it with the year to form a unique week-year identifier.\n\n4. Joining this identifier with the week-year of the activity data as needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#redditIn <- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn <- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn <- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] <- \"date\"\n\nsentimentIn <- sentimentIn |>\n    mutate(date = mdy(date)) |>\n    mutate(week = week(date)) |>\n    mutate(lagweek = week + 1) |>\n    mutate(weekyear = paste(year(date), lagweek )) |>\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |>\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |>\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |>\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n\n\nrtatIn <- rtatIn %>%\n    mutate(date = ymd(date)) %>%\n    mutate(week = week(date)) %>%\n    mutate(weekyear = paste(year(date), week))\n\n#head(rtatIn)\n\n\n#joinedSentiment <- left_join(rtatIn, sentimentIn, by = \"weekyear\")\n```\n:::\n\n\n\n\n## Reddit Text Corpus in Python\n\n- [View Original Data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/topRedditText.xlsx)\n- [View Cleaned Data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/vectorizedReddit.csv)\n\n\n\nReddit forums dedicated to the stock market have millions of users, and organized impact on markets\nThe Reddit API promises to allow text scraping. However, it only allowed access to a small subset of posts, and timed out after a few hundred requests. Chaining many request together, and targeting the most active posts, I gathered 7,463 Reddit posts from the studied time period on r/stocks. Once vectorized, these posts contained around 56,000 unique words, reduced by about 7,000 through stemming. Then, I filtered out words/columns which had only appeared once in the dataset, and this ultimately reduced the size of the text data down to around 35,000 columns.\n\n\nCleaning the data meant:\n\n1. Starting from raw text data, which was largely contained in the title of the Reddit posts\n\n2. Split the text column into individual words and apply a stemmer\n\n3. Vectorize the resulting column \n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\nimport os\nimport subprocess\nimport sys\nfrom nltk.stem import PorterStemmer\n\n\n#subprocess.call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n\nos.getcwd()\n\nredditTexts = pd.read_excel(\"../data/01-modified-data/sampleRedditText.xlsx\", sheet_name=\"Sheet1\")\n\nredditTexts.head()\n#positiveLingo <- c(\"call\", \"calls\", \"long\", \"hold\", \"buy\", \"bull\", \"bullish\", \"bulls\", \"support\", \"strong\")\n#negativeLingo <- c(\"put\", \"puts\", \"short\", \"shorters\", \"short-sellers\", \"sell\", \"sellers\", \"bear\", \"bears\", \"bearish\", \"weakness\", \"weak\")\n\nremoveNAtexts = redditTexts.dropna(subset = ['text'])\n\n# initailize stemmer\nstemmer = PorterStemmer()\n\n# split strings into individual words\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: x.split())\n\n# stem the words\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x]) \n# join the words back together\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: ' '.join(x)) \n\n\n# use count vectorizer to combine the data\nvectorizer = CountVectorizer()\nXs  =  vectorizer.fit_transform(removeNAtexts[\"text\"])   \n\ncol_names=vectorizer.get_feature_names_out()\n\n# take only the needed columns\nsmallReddit = removeNAtexts[[\"date_utc\", \"title\", \"ticker\", \"comments\"]]\n\n#smallReddit.index\n\nb = pd.DataFrame(Xs.toarray(), columns=col_names, index= smallReddit.index)\n\nredditCV = pd.concat([smallReddit, b], axis=1)\n\n#redditCV.head()\n\n\n#redditCV.to_csv('./data/01-modified-data/vectorizedReddit.csv', index=False)\n#print(df)\n```\n:::\n",
    "supporting": [
      "datacleaning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}