{
  "hash": "46cc5a4dd9f5ce88c8b4d6216aa7572b",
  "result": {
    "markdown": "---\ntitle: \"Data Gathering\"\nengine: knitr\nexecute:\n    freeze: true\nproject:\n  execute-dir: project\n---\n\n\n\n\n\n<h2> Clean Record Data in R </h2>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n```\n:::\n\n\n\n\n<h2> Cleaning Reddit Text Corpus in Python </h2>\n\nMost of my work on preparing the text data is on the Naive Bayes tab. Below I take the reddit data and \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\n\nprint\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<built-in function print>\n```\n:::\n\n```{.python .cell-code}\nos.getcwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'C:\\\\Users\\\\corwi\\\\Documents\\\\dsan-5000-project-corwindark\\\\dsan-website\\\\5000-website\\\\datacleaning'\n```\n:::\n:::\n\n\n\nI ran the following code to clean my data: \n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\nimport os\nimport subprocess\nimport sys\nfrom nltk.stem import PorterStemmer\n\n\n#subprocess.call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n\nos.getcwd()\n\nredditTexts = pd.read_excel(\"./data/01-modified-data/sampleRedditText.xlsx\", sheet_name=\"Sheet1\")\n\nredditTexts.head()\n#positiveLingo <- c(\"call\", \"calls\", \"long\", \"hold\", \"buy\", \"bull\", \"bullish\", \"bulls\", \"support\", \"strong\")\n#negativeLingo <- c(\"put\", \"puts\", \"short\", \"shorters\", \"short-sellers\", \"sell\", \"sellers\", \"bear\", \"bears\", \"bearish\", \"weakness\", \"weak\")\n\nremoveNAtexts = redditTexts.dropna(subset = ['text'])\n\nstemmer = PorterStemmer()\n\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: x.split())\n\n#df['stemmed'] = df['unstemmed'].apply(lambda x: [stemmer.stem(y) for y in x]) \n\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x]) \n\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: ' '.join(x)) \n\nvectorizer = CountVectorizer()\nXs  =  vectorizer.fit_transform(removeNAtexts[\"text\"])   \n#print(type(Xs))\n\ncol_names=vectorizer.get_feature_names_out()\n\nsmallReddit = removeNAtexts[[\"date_utc\", \"title\", \"ticker\", \"comments\"]]\n\nsmallReddit.index\n\nb = pd.DataFrame(Xs.toarray(), columns=col_names, index= smallReddit.index)\n\nredditCV = pd.concat([smallReddit, b], axis=1)\n\nredditCV.head()\n\n\nredditCV.to_csv('./data/01-modified-data/vectorizedReddit.csv', index=False)\n#print(df)\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n# VOCABULARY DICTIONARY\n#print(\"vocabulary = \",vectorizer.vocabulary_)   \n# STOP WORDS \n#print(\"stop words =\", vectorizer.stop_words)\n#print(\"col_names=\",col_names)\n#print(\"CORPUS WIDE WORD COUNTS:\",np.sum(Xs,axis=0))\n#print(\"WORDS PER DOCUMENT:\\n\",np.sum(Xs,axis=1))\n\n```\n:::\n\n\n~51700 words without stemming\n~49692 words after stemming\n\n<h2> Now, let's\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}