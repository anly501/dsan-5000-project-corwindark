{
  "hash": "c3360fc1c7ccb8ee16b3975134fe3611",
  "result": {
    "markdown": "---\ntitle: \"Exploratory Data Analysis\"\nauthor: \"Corwin Dark\"\nengine: knitr\n---\n\n\n\n\n\n## EDA Overview\n\nAt the broadest level, there is a temporal component to the research question that will shape how we conduct the analysis. Namely, the outcome variable is retail trading activity in stocks, as measured by daily holding data on the top 10 stocks sourced from NASDAQ's API. As such, we are interested in data from the other datasets (reddit text information, investor sentiment survey, stocktwits ranking) as of one day before. I will merge the datasets in R, and then use R (ggplot) to create visuals and carry out EDA.\n\n## Exploring the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(Quandl)\nlibrary(lubridate)\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n```\n:::\n\n\nLet's join the data together into one dataset, with properly lagged predictor variables.\n\n::: {.cell}\n\n```{.r .cell-code}\nredditIn <- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn <- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn <- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] <- \"date\"\n\nsentimentIn <- sentimentIn |>\n    mutate(date = mdy(date)) |>\n    mutate(week = week(date)) |>\n    mutate(lagweek = week + 1) |>\n    mutate(weekyear = paste(year(date), lagweek )) |>\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |>\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |>\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |>\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n\n\nrtatIn <- rtatIn %>%\n    mutate(date = ymd(date)) %>%\n    mutate(week = week(date)) %>%\n    mutate(weekyear = paste(year(date), week))\n\n#head(rtatIn)\n\n\njoinedSentiment <- left_join(rtatIn, sentimentIn, by = \"weekyear\")\n\n#head(joinedSentiment)\n```\n:::\n\n\n\n### Numeric Summaries \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(\"Activity Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Activity Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$activity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00760 0.01720 0.02460 0.03103 0.03590 0.26150 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Sentiment Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Sentiment Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$sentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.0000  -1.0000   1.0000   0.9117   3.0000  19.0000 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Change in Activity Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Change in Activity Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$deltaActivity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.194000 -0.003700  0.002100  0.003997  0.011600  0.232000 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Change in Sentiment Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Change in Sentiment Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$deltaSentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.000  -3.000   0.000  -1.259   1.000  20.000 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Positive Sentiment Proportion Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Positive Sentiment Proportion Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Bullish)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1584  0.2483  0.3188  0.3262  0.3956  0.5691     290 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Negative Sentiment Proportion Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Negative Sentiment Proportion Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Bearish)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1976  0.2751  0.3805  0.3732  0.4490  0.6087     290 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Neutral Sentiment Proportion Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Neutral Sentiment Proportion Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Neutral)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1450  0.2715  0.3043  0.3005  0.3345  0.4091     290 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Spread in Sentiment Summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Spread in Sentiment Summary:\"\n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Spread)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-0.43100 -0.19200 -0.07500 -0.04695  0.11700  0.36500      290 \n```\n:::\n:::\n\n\n\n\n### Barplots for discrete variables:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = newEntry)) + geom_bar() + labs(x= \"Current Stock is in Top 10\", y = \"Count\", title = \"Distribution of New Stocks\\n Amongst Top-10 by Retail Activity\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(\"Most Popular Stocks by Days in Top 10:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Most Popular Stocks by Days in Top 10:\"\n```\n:::\n\n```{.r .cell-code}\nsort(table(joinedSentiment$ticker), decreasing = TRUE)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAAPL TSLA  SPY  AMD  QQQ TQQQ NVDA AMZN SQQQ MSFT \n 956  956  926  876  760  694  665  531  408  389 \n```\n:::\n:::\n\n\nIn addition to the porportion of new stocks in the top 10, the table shows up the top 10 stocks by number of days spent in the top-10 ranking of retail investor actvity. We can see that the distribution has a heavy right tail, as even amongst the top 10 stocks there is a large divergence, with the top stock (AAPL) having more than two times as many days in the top 10 as the 10th place stock (MSFT).\n\n\n### Outcome Variable Exploration\n\nFirst, lets explore the outcome variable table, of the top 10 retail-investor held stocks each day. Each stock has an 'activity' number which measures the percent of traded shares held by retail investors. Let's see how those scores are distributed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = activity)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily \\n Retail Activity Scores Amongst Top-10 Stocks\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nActivity scores seem to be skewed to the right, with a thin tail. I wonder which stocks compose the highest activity values. Let's look at which of these scores were from stocks that had just appeared in the top 10 that day:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = activity, fill = newEntry)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nFrom this chart, we can see that most of the high-activity scores are from stocks which previously entered the top 10. This makes sense, as we would expects stock to increase in activity over multiple days, before reaching the top of the list. Similar to songs on top music charts.\n\nMy next question is how the outcome variable has changed over time. Let's look at attention scores over time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoinedSentimentAvgs <- joinedSentiment %>%\n    group_by(date.x) %>%\n    summarize(dailyactivitytotal = sum(activity), dailyactivityavg = mean(activity),\n    dailynewtickers = sum(as.numeric(newEntry))) \n\nggplot(joinedSentimentAvgs, aes(x = date.x, y = dailyactivitytotal)) + geom_line() + labs(x= \"Date\", y = \"Total Retail Investor Activity\", title = \"Total of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis chart is interesting because it shows that our outcome variable has a clear upward trend over time. Retail investors appear to be a greater proportion of the total activity in the stock market today than they were in 2020.\n\nOne last visual I wanted to create to evaluate the outcome variables was the number of new top-10 stocks that entered the list each week\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggbeeswarm)\n\nggplot(joinedSentimentAvgs, aes(x = as.character(year(date.x)), y = dailynewtickers)) + geom_boxplot() + labs(x= \"Year\", y = \"New Stocks in the Top 10\", title = \"Number of New Stocks in the Top 10 of Retail Activity by Year\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nHere we can see there is not a huge difference in the distributions of new stocks between the years, except for 2021, when there was a lot more days with at least 1 new stock in the top 10. But far and away, at least 50% of days have no new stocks in the top 10 of retail trader activity.\n\n### Bivariate Analysis \n\n\nNow, lets look at some of the predictor variables in relation to the outcome variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"corrplot\")\n\nnoNAsentiment <- na.omit(joinedSentiment[,c(4,5,6,7,12,13,14,17)])\n\nsave <- cor(noNAsentiment)\ncorrplot(save)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsave \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  activity    sentiment deltaActivity deltaSentiment\nactivity        1.00000000 -0.068425337   0.620189979     0.06955678\nsentiment      -0.06842534  1.000000000   0.009215771     0.51408474\ndeltaActivity   0.62018998  0.009215771   1.000000000    -0.04522242\ndeltaSentiment  0.06955678  0.514084741  -0.045222415     1.00000000\nBullish        -0.07234842  0.111433821  -0.095796815     0.10398764\nNeutral         0.04440169  0.019445339   0.038646679    -0.01661078\nBearish         0.04285201 -0.111207408   0.067113719    -0.08588212\nSpread         -0.05902646  0.115454510  -0.083795941     0.09801129\n                   Bullish     Neutral     Bearish      Spread\nactivity       -0.07234842  0.04440169  0.04285201 -0.05902646\nsentiment       0.11143382  0.01944534 -0.11120741  0.11545451\ndeltaActivity  -0.09579681  0.03864668  0.06711372 -0.08379594\ndeltaSentiment  0.10398764 -0.01661078 -0.08588212  0.09801129\nBullish         1.00000000 -0.09615506 -0.85871354  0.96041621\nNeutral        -0.09615506  1.00000000 -0.42751147  0.18492522\nBearish        -0.85871354 -0.42751147  1.00000000 -0.96747469\nSpread          0.96041621  0.18492522 -0.96747469  1.00000000\n```\n:::\n:::\n\n\nThe correlations between weekly investor sentiment polling and investor activity in the following week are displayed in the plot. While many crosstabs are highly correlated, these are only because the variables have been calculated together (i.e. the sentiment polling includes % with positive sentiment and % with negative sentiment, which are perfectly correlated since they are two sides of the same measure). The highest correlation value recorded outside these collinear predictors was ~0.115 for the spread in bullish and bearish opinion vs. sentiment towards top10 stocks. \n\nThis correlation makes sense, because both measures are capturing the same underlying phenomena: investor sentiment. If retail investor sentiment is bearish about the market in general, then it would likely also be negative about particular stocks.\n\n\n### Outliers and Segmentation \n\n\nWe already saw in the overtime chart that perhaps 2021 was a different type of year than the other 3, as it had a high percentage of days with new stocks in the top 10.\n\nLets look at the key outcome variables and see if any values violate Tukey's rule\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(joinedSentiment$activity)\n\ncutoff <- mean(joinedSentiment$activity) + (1.5 * IQR(joinedSentiment$activity))\n\nabline(h = cutoff, col = \"Red\")\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWith the red line above denoting the 1.5*IQR + mean threshold, we can see that many values lie above this line, but they do not appear to be separated from the main distribution. Rather, it looks like the main distribution has a fat right tail. Perhaps it would be well approximated by a Student's distribution.\n\n### Text Data\n\nLet's use the TM package to make a cloud of the most popular words in the Reddit posts:\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"tm\")\nlibrary(tm)\nlibrary(wordcloud)\n#Create a vector containing only the text\ntext <- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\ntext <- text$text\n\n# Create a corpus  \ndocs <- Corpus(VectorSource(text))\ndocs <- docs %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\ndocs <- tm_map(docs, content_transformer(tolower))\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\n\n\ndtm <- TermDocumentMatrix(docs) \nmatrix <- as.matrix(dtm) \nwords <- sort(rowSums(matrix),decreasing=TRUE) \ndf <- data.frame(word = names(words),freq=words)\n\nset.seed(1234) # for reproducibility \nwordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe wordcloud is interesting: it contains far more formal language than I would have predicted. Stock trading forums online usually discuss a variety of issues with a varying degree of informalism. I believe the narrow and professional vocabulary in the posts may be due to the particular forum I chose, as the r/stocks subreddit is hyperfocused on discussions of particular stocks. I would hypothesize that the vocabulary and most used words would differ greatly between internet forums, even amongst those that are focused only on investing. \n\nA few notable points from the words displayed: earnings are by far the most common point of discussion, and seem to play an outsize role in why individual investors on the forum are interested in a particular stock. This could indicate than comparing retail investor conversations with earnings events could be a fruitful point of investigation in the future.\n\n## Findings from EDA\n\nOverall, we can see that activity scores are somewhat similar to a normal distribution which is censored at 0 and centered around 0.02. Albeit with a taller mean and wider right tail than expected from a normal distribution. Activity scores have steadily risen over time, although 2021 was a particularly substantial year for increases in retail trading activity. We also didn't see much obvious correlation between our tabular data and outcome variables, leaving it up to question whether we will be able to predict increases and decreases in retail activity with later models. Finally, the textual data does appear to be of good quality, and we notice substantive vocabulary centering on investments. \n\n\n",
    "supporting": [
      "dataexploration_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}