{
  "hash": "1710cd19c9c7680ff935bbadf4be7269",
  "result": {
    "markdown": "---\ntitle: \"Exploratory Data Analysis\"\nauthor: \"Corwin Dark\"\nengine: knitr\n---\n\n\n\n<h2> EDA Overview <h2>\n\nAt the broadest level, there is a temporal component to the research question that will shape how we conduct the analysis. Namely, the outcome variable is retail trading activity in stocks, as measured by daily holding data on the top 10 stocks sourced from NASDAQ's API. As such, we are interested in data from the other datasets (reddit text information, investor sentiment survey, stocktwits ranking) as of one day before. I will merge the datasets in R, and then use R (ggplot) to create visuals and carry out EDA.\n\n<h2> Joining the Data </h2>\n(Will be moved to data cleaning eventually)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(Quandl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: xts\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: zoo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'zoo'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xts'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n```\n:::\n\n\nLet's join the data together into one dataset, with properly lagged predictor variables.\n\n::: {.cell}\n\n```{.r .cell-code}\nredditIn <- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn <- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn <- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] <- \"date\"\n\nsentimentIn <- sentimentIn |>\n    mutate(date = mdy(date)) |>\n    mutate(week = week(date)) |>\n    mutate(lagweek = week + 1) |>\n    mutate(weekyear = paste(year(date), lagweek )) |>\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |>\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |>\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |>\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n```\n:::\n\n```{.r .cell-code}\nrtatIn <- rtatIn %>%\n    mutate(date = ymd(date)) %>%\n    mutate(week = week(date)) %>%\n    mutate(weekyear = paste(year(date), week))\n\nhead(rtatIn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     X       date ticker activity sentiment deltaActivity deltaSentiment\n1 9551 2020-01-02   TSLA   0.0226         0       -0.0014              0\n2 9552 2020-01-02    SPY   0.0147         3       -0.0341             -7\n3 9553 2020-01-02   ROKU   0.0119         0        0.0000              0\n4 9554 2020-01-02    QQQ   0.0093         6       -0.0111              1\n5 9555 2020-01-02   MSFT   0.0116         2       -0.0181              1\n6 9556 2020-01-02     FB   0.0093        -2       -0.0078             -2\n  newEntry week weekyear\n1    FALSE    1   2020 1\n2    FALSE    1   2020 1\n3     TRUE    1   2020 1\n4    FALSE    1   2020 1\n5    FALSE    1   2020 1\n6    FALSE    1   2020 1\n```\n:::\n\n```{.r .cell-code}\njoinedSentiment <- left_join(rtatIn, sentimentIn, by = \"weekyear\")\n\nhead(joinedSentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     X     date.x ticker activity sentiment deltaActivity deltaSentiment\n1 9551 2020-01-02   TSLA   0.0226         0       -0.0014              0\n2 9552 2020-01-02    SPY   0.0147         3       -0.0341             -7\n3 9553 2020-01-02   ROKU   0.0119         0        0.0000              0\n4 9554 2020-01-02    QQQ   0.0093         6       -0.0111              1\n5 9555 2020-01-02   MSFT   0.0116         2       -0.0181              1\n6 9556 2020-01-02     FB   0.0093        -2       -0.0078             -2\n  newEntry week.x weekyear date.y Bullish Neutral Bearish Total X8.week Spread\n1    FALSE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n2    FALSE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n3     TRUE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n4    FALSE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n5    FALSE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n6    FALSE      1   2020 1   <NA>      NA      NA      NA  <NA>    <NA>     NA\n  SPYHighWk SPYLowWK SPYCloseWK week.y lagweek\n1      <NA>     <NA>       <NA>     NA      NA\n2      <NA>     <NA>       <NA>     NA      NA\n3      <NA>     <NA>       <NA>     NA      NA\n4      <NA>     <NA>       <NA>     NA      NA\n5      <NA>     <NA>       <NA>     NA      NA\n6      <NA>     <NA>       <NA>     NA      NA\n```\n:::\n:::\n\n\n\n<h2> Numeric Summaries </h2>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(joinedSentiment$activity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00760 0.01720 0.02460 0.03103 0.03590 0.26150 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$sentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.0000  -1.0000   1.0000   0.9117   3.0000  19.0000 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$deltaActivity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.194000 -0.003700  0.002100  0.003997  0.011600  0.232000 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$deltaSentiment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.000  -3.000   0.000  -1.259   1.000  20.000 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Bullish)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1584  0.2483  0.3188  0.3262  0.3956  0.5691     290 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Bearish)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1976  0.2751  0.3805  0.3732  0.4490  0.6087     290 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Neutral)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1450  0.2715  0.3043  0.3005  0.3345  0.4091     290 \n```\n:::\n\n```{.r .cell-code}\nsummary(joinedSentiment$Spread)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-0.43100 -0.19200 -0.07500 -0.04695  0.11700  0.36500      290 \n```\n:::\n:::\n\n\n\n\nBarplots for discrete variables:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = newEntry)) + geom_bar() + labs(x= \"Current Stock is in Top 10\", y = \"Count\", title = \"Distribution of New Stocks Amongst Top-10 Stocks by Retail Activity\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsort(table(joinedSentiment$ticker), decreasing = TRUE)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAAPL TSLA  SPY  AMD  QQQ TQQQ NVDA AMZN SQQQ MSFT \n 956  956  926  876  760  694  665  531  408  389 \n```\n:::\n:::\n\n\nIn addition to the porportion of new stocks in the top 10, the table shows up the top 10 stocks by number of days spent in the top-10 ranking of retail investor actvity. We can see that the distribution has a heavy right tail, as even amongst the top 10 stocks there is a large divergence, with the top stock (AAPL) having more than two times as many days in the top 10 as the 10th place stock (MSFT).\n\n\n<h2> Outcome Variable Exploration </h2>\n\nFirst, lets explore the outcome variable table, of the top 10 retail-investor held stocks each day. Each stock has an 'activity' number which measures the percent of traded shares held by retail investors. Let's see how those scores are distributed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = activity)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nActivity scores seem to be skewed to the right, with a thin tail. I wonder which stocks compose the highest activity values. Let's look at which of these scores were from stocks that had just appeared in the top 10 that day:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(joinedSentiment, aes(x = activity, fill = newEntry)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nFrom this chart, we can see that most of the high-activity scores are from stocks which previously entered the top 10. This makes sense, as we would expects stock to increase in activity over multiple days, before reaching the top of the list. Similar to songs on top music charts.\n\nMy next question is how the outcome variable has changed over time. Let's look at attention scores over time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoinedSentimentAvgs <- joinedSentiment %>%\n    group_by(date.x) %>%\n    summarize(dailyactivitytotal = sum(activity), dailyactivityavg = mean(activity),\n    dailynewtickers = sum(as.numeric(newEntry))) \n\nggplot(joinedSentimentAvgs, aes(x = date.x, y = dailyactivitytotal)) + geom_line() + labs(x= \"Date\", y = \"Total Retail Investor Activity\", title = \"Total of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis chart is interesting because it shows that our outcome variable has a clear upward trend over time. Retail investors appear to be a greater proportion of the total activity in the stock market today than they were in 2020.\n\nOne last visual I wanted to create to evaluate the outcome variables was the number of new top-10 stocks that entered the list each week\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggbeeswarm)\n\nggplot(joinedSentimentAvgs, aes(x = as.character(year(date.x)), y = dailynewtickers)) + geom_boxplot() + labs(x= \"Year\", y = \"New Stocks in the Top 10\", title = \"Number of New Stocks in the Top 10 of Retail Activity by Year\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nHere we can see there is not a huge difference in the distributions of new stocks between the years, except for 2021, when there was a lot more days with at least 1 new stock in the top 10. But far and away, at least 50% of days have no new stocks in the top 10 of retail trader activity.\n\n<h2> Bivariate Analysis </h2>\n\n\nNow, lets look at some of the predictor variables in relation to the outcome variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"corrplot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ncorrplot 0.92 loaded\n```\n:::\n\n```{.r .cell-code}\nnoNAsentiment <- na.omit(joinedSentiment[,c(4,5,6,7,12,13,14,17)])\n\nsave <- cor(noNAsentiment)\ncorrplot(save)\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsave \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  activity    sentiment deltaActivity deltaSentiment\nactivity        1.00000000 -0.068425337   0.620189979     0.06955678\nsentiment      -0.06842534  1.000000000   0.009215771     0.51408474\ndeltaActivity   0.62018998  0.009215771   1.000000000    -0.04522242\ndeltaSentiment  0.06955678  0.514084741  -0.045222415     1.00000000\nBullish        -0.07234842  0.111433821  -0.095796815     0.10398764\nNeutral         0.04440169  0.019445339   0.038646679    -0.01661078\nBearish         0.04285201 -0.111207408   0.067113719    -0.08588212\nSpread         -0.05902646  0.115454510  -0.083795941     0.09801129\n                   Bullish     Neutral     Bearish      Spread\nactivity       -0.07234842  0.04440169  0.04285201 -0.05902646\nsentiment       0.11143382  0.01944534 -0.11120741  0.11545451\ndeltaActivity  -0.09579681  0.03864668  0.06711372 -0.08379594\ndeltaSentiment  0.10398764 -0.01661078 -0.08588212  0.09801129\nBullish         1.00000000 -0.09615506 -0.85871354  0.96041621\nNeutral        -0.09615506  1.00000000 -0.42751147  0.18492522\nBearish        -0.85871354 -0.42751147  1.00000000 -0.96747469\nSpread          0.96041621  0.18492522 -0.96747469  1.00000000\n```\n:::\n:::\n\n\nThe correlations between weekly investor sentiment polling and investor activity in the following week are displayed in the plot. While many crosstabs are highly correlated, these are only because the variables have been calculated together (i.e. the sentiment polling includes % with positive sentiment and % with negative sentiment, which are perfectly correlated since they are two sides of the same measure). The highest correlation value recorded outside these collinear predictors was ~0.115 for the spread in bullish and bearish opinion vs. sentiment towards top10 stocks. \n\nThis correlation makes sense, because both measures are capturing the same underlying phenomena: investor sentiment. If retail investor sentiment is bearish about the market in general, then it would likely also be negative about particular stocks.\n\n\n<h2> Outliers and Segmentation </h2>\n\n\nWe already saw in the overtime chart that perhaps 2021 was a different type of year than the other 3, as it had a high percentage of days with new stocks in the top 10.\n\nLets look at the key outcome variables and see if any values violate Tukey's rule\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(joinedSentiment$activity)\n\ncutoff <- mean(joinedSentiment$activity) + (1.5 * IQR(joinedSentiment$activity))\n\nabline(h = cutoff, col = \"Red\")\n```\n\n::: {.cell-output-display}\n![](dataexploration_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWith the red line above denoting the 1.5*IQR + mean threshold, we can see that many values lie above this line, but they do not appear to be separated from the main distribution. Rather, it looks like the main distribution has a fat right tail. Perhaps it would be well approximated by a Student's distribution.\n\n\n<h2> Hypothesis Testing </h2>\n\n\n\n\n",
    "supporting": [
      "dataexploration_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}