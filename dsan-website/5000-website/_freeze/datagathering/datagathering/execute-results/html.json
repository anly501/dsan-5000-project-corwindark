{
  "hash": "db64ba429e347a6b00246c81d77b54f3",
  "result": {
    "markdown": "---\ntitle: \"Data Gathering\"\nengine: knitr\nexecute:\n    freeze: true\n---\n\n\n\nFor this project, I need to gather a range of data on retail investor sentiment. Reviewing the literature, it does not appear that a consesus has been reached on which mediums are most useful for measuring retail investor trades, let alone their sentiment. After an extensive review, I decided to try and construct a set of trading signals from the following:\n1. Daily NASDAQ data on the top-10 retail investor held companies (API through R Quandl Package)\n2. Text data from popular stock-trading subreddits (Reddit API through R)\n3. Stocktwits mentions of popular companies through (Stocksera API in Python)\n4. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV)\n5. Text data from major financial new companies\n\nStarting with dataset 1: Nasdaq retail investor holdings\n``{r}\nlibrary(tidyverse)\nlibrary(Quandl)\nlibrary(lubridate)\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n\nQuandl.api_key(\"psq4nx69HDimf2kQcxZJ\")\ndata <- Quandl.datatable(\"NDAQ/RTAT10\", paginate = TRUE)\n\ncleanData <- data %>% \n    mutate(date = ymd(date)) %>%\n    filter(year(date) > 2019)\n\ncleanData <- cleanData[order(cleanData$date),]\ncleanData$deltaActivity <- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$deltaSentiment <- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$newEntry <- vector(mode = 'logical', length = nrow(cleanData))\n\nfor(i in 1:nrow(cleanData)) {\n    if(i %% 1000 == 0) {\n        print(i)\n    }\n    prevDay <- cleanData %>%\n        filter(day(date) == day(cleanData$date[i]) - 1) %>%\n        filter(ticker == cleanData$ticker[i])\n\n    if(nrow(prevDay) > 0) {\n\n        cleanData$deltaActivity[i] = cleanData$activity[i] - prevDay$activity[1]\n        cleanData$deltaSentiment[i] = cleanData$sentiment[i] - prevDay$sentiment[1]\n        cleanData$newEntry[i] = FALSE\n    \n    }else {\n        cleanData$newEntry[i] = TRUE\n    }\n}\n\ncolnames(cleanData)\nhist(cleanData$deltaActivity)\n\nhead(cleanData, 100)\ngetwd()\n#write.csv(cleanData, \"./data/01-modified-data/cleanRTAT.csv\")\n``\n\nHere I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.\n\nDataset 2: Text data from reddit\n\n``{r}\n\n# get a particular stock's mentions in titles in each of the 7 subreddits\n# found subreddits by measuring overlap from https://subredditstats.com/subreddit-user-overlaps/superstonk\n# which is calculated based on how likely users are to post on one or the other subs\n# targeting investment related subs with >400k users\n# loop through subreddits and get titles from past month\n\n\n#install.packages(\"RedditExtractoR\", repos='https://cloud.r-project.org/')\n\n\nstockSubreddits <- c(\n    \"wallstreetbets\",\n    \"stocks\",\n    \"options\",\n    \"investing\",\n    \"stockamrket\",\n    \"superstonk\",\n    \"wallstreetbetsnew\"\n)\n\n\n\nlinks <- find_thread_urls(keywords = \"QQQ\", subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n\nlinks <- tibble(links)\n\nlinks$ticker = \"QQQ\"\n\n# get all unique stocks in the dataset of top retail-traded securities\ntempClean <- read.csv('../data/01-modified-data/cleanRTAT.csv')\n\n\ntopStocks <- sort(table(tempClean$ticker), decreasing = TRUE)[1:100]\n\nuqTickers <- rownames(topStocks)\n\nprint(uqTickers)\n\nfor(i in 1:length(uqTickers)) {\n    print(i / nrow(uqTickers))\n    \n    templinks <- find_thread_urls(keywords = uqTickers[i], subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n    templinks$ticker <- uqTickers[i]\n    links <- rbind(links, templinks)\n}\n\n\n\nwrite.xlsx(links,\"../data/01-modified-data/sampleRedditText.xlsx\")\n\n``\n\nHere I have gathered a list of 7 stock trading subreddits, which I created by searching for subreddits which mentioned individual stocks and investing, which also had more than ~400k subscribers. Then, I can use the find_thread_urls command to get a list of threads which mention particular keywords. For this initial gathering step, I simply gather posts with they keyword \"SPY,\" a ticker for the S&P 500, from r/stocks, a major investing subreddit.\n\nUpdated Method: using r/stocks only, get posts for the 100 most common tickers in the most-held datatable.\n\n\n3. Stocktwits rankings\n``{python}\n# stocksera\n# https://pypi.org/project/stocksera/\n# use this for news ratings, stocktwits website mentions and ranking\n# api key: 2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1\n\nimport subprocess\nimport sys\n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install(\"stocksera\")\n\nimport stocksera\n\nclient = stocksera.Client(api_key=\"2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1\")\n\n\nstw = client.stocktwits(ticker = \"QQQ\")\nprint(stw)\n\n\n\n\nsbrs = client.wsb_mentions(days=500, ticker=\"QQQ\")\nprint(sbrs)\n``\n\n\nDataset 4: (In Progress) Weekly Investor Sentiment from AAII\n`` {r}\n# AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results\n# read in as csv \n\n#getwd()\n#aaWeekly <- read.csv(\"./data/00-raw-data/sentiment_aaii.csv\")\n#head(aaWeekly)\n\n\n``\n\n\n",
    "supporting": [
      "datagathering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}