{
  "hash": "4f5e0f15bfdece381ba261f12ec2deca",
  "result": {
    "markdown": "---\ntitle: \"Data Gathering\"\nengine: knitr\nexecute:\n    freeze: true\n    eval: false\n---\n\n\n## Introduction\n\n### Finding Meaningful Retail Investor Data\n\nWhile retail investor trading activity in individual companies has aroused great amounts of interest in the academic literature, practical methods for accessing this data are surprisingly limited. Even measuring which trades are truly by retail investors is a disputed matter that raises questions about the validity of any study into the subject. In this regard, the paper by Boehmer, Jones, Zhang, and Zhan [@boehmer_tracking_2020] is widely referenced and considered the \"gold standard\" in identifying retail trades. The authors make use of a strategy based on the tenths of a cent that are involved in a trade, alleging that trades with particular trailing digits can be flagged as made by individual investors. I pursued this strategy initially but found two problems: First, the accuracy of the method itself is disputed, with another study finding it only identified 35% of verified retail trades they placed on different accounts [@barber_subpenny_2023]. Second, the process of gathering the orimary data needed for the method from government disclosures is difficult in itself, and no third parties appeared to offer data generated with exactly this method.\n\nAfter reviewing many financial databases, I ultimately identified a new data product offered by Nasdaq's data link service, which was termed the \"Retail Trading Activity Tracker\" and provided by The Applied Research Company. This provided two essential metrics: the porportion by dollar value of the stock's daily volume that was traded by retail investors, and the relative ranking of each stock by this proportion. I believed this data to be a high standard due to its valuation as a purchasable product, and its promotion by a prominent stock market actor in Nasdaq. Unfortunately, the full dataset required paid access, but I was able to access the top-10 stocks which had the highest retail trader activity proportion each day. By looking at the available proportion of activity, and whether each stock remained in the top-10 day over day, I was able to construct a daily binary outcome from whether each ticker increased or decreased in proportionate retail trader activity from the day before.\n\n### A Shortage of Public Social Media Data\n\nThe next question was how to gather meaningful inputs to compare with retail activity. My research question was centered on social media data and sentiment towards financial instruments, and I intended to gather data from online communities know for discussing investments such as Reddit, Stocktwits, and Robinhood. While past studies had found a wealth of data on these subjects, I quickly realized that this data had recently become less accessible. Since roughly the start of 2020:\n\n- Robinhood had shut down its API completely, which also closed associated datasources such as Robintrack which were used in many research papers. \n- Reddit had begun charging for access to its API, which it had previously offered for free.\n- Stocktwits had also shut down its API completely, and and associated 3rd party websites called Stocksera which had collected the data appeared to go offline during the project\n\nI mitigated these shortages by finding two datasets, one tabular and one textual, that contained information about retail traders' sentiment and social media activity. For the tabular data, I found the American Association of Individual Investors conducts a weekly survey of its members' sentiment about the market in the upcoming week (the three options being bullish, bearish, or neutral). For the textual data, I was able to use the Reddit API to make a limited number of free queries, which I used to gather the most popular posts from the largest stock trading forum. While I had originally hoped to gather an exhaustive list of posts from a variety of investment-focused forums, the limited dataset of around ~900 posts contained at least 2000 references to top-10 stocks in the desired time period, enough coverage for a meaningful investigation.\n\n## Methods\n\nReviewing the methods nessecary for gathering the datasets mentioned above:\n1. Daily NASDAQ data on the top-10 retail investor held companies (Python API through Quandl Package, R code for the equivalent package also listed below)\n2. Text data from popular stock-trading subreddits (Reddit API through R and RedditExtractoR package)\n3. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV)\n\n### Dataset 1: Nasdaq Retail Investor Activity\n\n- [View raw data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/00-raw-data/sentiment_aaii.csv)\n- [View clean data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/joinedSentiment.csv)\n\nThe Nasdaq API had a lengthy signup process, and required I generate an API key. After receiving my key, I was able to use packages in either R or Python that made use of the Quandly api to query Nasdaq's data. I did this in both Python and R, for good measure, and got the same result. The R version is below.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(Quandl)\nlibrary(lubridate)\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n# Here I loaded in my API key and queried the data (API Key hidden in this version of the document)\nQuandl.api_key(\"NNNNNNNNNNNN\")\n#data <- Quandl.datatable(\"NDAQ/RTAT10\", paginate = TRUE)\n```\n:::\n\n\nThe Python version of the NASDAQ api ultimately yielded the data I used in my project. The code below has been commented out because it was initially run in a separate, Python notebook. But you can review the simplicity of requesting the retail activity data below, which only took a few lines. I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.\n\n> Python Code\n\n> import pandas as pd\n\n> import numpy as np\n\n> import quandl\n\n> import nasdaqdatalink\n\n> quandl.ApiConfig.api_key = 'NNNNNNNNNNNNNNN'\n\n> table1 = quandl.get_table('NDAQ/RTAT10', date='2023-09-28,2023-09-27,2023-09-26', ticker='TSLA,TQQQ,SQQQ')\n\n> End Python Code\n\n\n\n\n### Dataset 2: Text Data from Reddit\n\n- [View raw data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/00-raw-data/sentiment_aaii.csv)\n- [View clean data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/joinedSentiment.csv)\n\n\nInitially, I planned to get a particular stock's mentions in titles in each of the 7 major subreddits dedicated to investing. I had originally created this set of forums based on measuring their overlap with known investment subreddits through user overlap [See tool here](ttps://subredditstats.com/subreddit-user-overlaps/superstonk). However, due to API changes I was not able to loop through all posts from these subreddits, and instead was limited to taking popular posts from a single forum. For this purpose, I chose the page \"stocks\" as it is dedicated solely to the discussion of particular stocks and not other topics. I used an R package to query Reddit's API (in the small amounts that were allowed), until I had a reasonable amount of data to proceed in the project, although far less than I had hoped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"RedditExtractoR\", repos='https://cloud.r-project.org/')\n\n# The original list of 7 subreddits I planned to use\nstockSubreddits <- c(\n    \"wallstreetbets\",\n    \"stocks\",\n    \"options\",\n    \"investing\",\n    \"stockamrket\",\n    \"superstonk\",\n    \"wallstreetbetsnew\"\n)\n\n# The function which initially flags the URLS of posts that contain a given string in the title\nlinks <- find_thread_urls(keywords = \"QQQ\", subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n\n# Testing formatting with a single example\nlinks <- tibble(links)\nlinks$ticker = \"QQQ\"\n\n# Getting all unique stocks in the dataset of top retail-traded securities\ntempClean <- read.csv('../data/01-modified-data/cleanRTAT.csv')\n\n# looking for the top 100 most popular stocks by retail activity, to find their relevant reddit posts (limited from doing all stocks by API bandwidth)\ntopStocks <- sort(table(tempClean$ticker), decreasing = TRUE)[1:100]\n\nuqTickers <- rownames(topStocks)\n\nprint(uqTickers)\n\n# Looping through the common top-10 stocks and pulling any top threads which referenced them in the title\nfor(i in 1:length(uqTickers)) {\n    print(i / nrow(uqTickers))\n    \n    templinks <- find_thread_urls(keywords = uqTickers[i], subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n    templinks$ticker <- uqTickers[i]\n    links <- rbind(links, templinks)\n}\n\n#write.xlsx(links,\"../data/01-modified-data/sampleRedditText.xlsx\")\n```\n:::\n\n\n\n### Dataset 3: AAII Investor Sentiment\n\n- [View raw data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/00-raw-data/sentiment_aaii.csv)\n- [View clean data](https://github.com/anly501/dsan-5000-project-corwindark/blob/d8c22f7f12f2d5edd0c771870a249f1493aa15a1/dsan-website/5000-website/data/01-modified-data/joinedSentiment.csv)\n\n\nFinally, the dataset for individual investors' weekly sentiment was readily available as a CSV on the AAII's website. I downloaded this and read it into the file.\n\n::: {.cell}\n\n```{.r .cell-code}\n# AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results\n# read in as csv \n\naaWeekly <- read.csv(\"./data/00-raw-data/sentiment_aaii.csv\")\nhead(aaWeekly)\n```\n:::\n",
    "supporting": [
      "datagathering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}