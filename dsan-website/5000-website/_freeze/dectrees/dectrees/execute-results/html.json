{
  "hash": "3101c99fa67e28f8aba079a7d1cb6fd7",
  "result": {
    "markdown": "---\ntitle: \"Decision Trees\"\nexecute:\n    freeze: true\n---\n\n<h1> Methods </h1>\n\nRandom and boosted forests are some of the most pervasive machine learning methods today. For tabular data, these methods are extremely effective at identifying nonlinear relationships, as well as identifying which features in the input space are most important. I hope to use the excellent results promised by these methods to achieve real predictive power on my outcome variables (the direction and amount that retail activity changes on a given stock in a given day). To do this,  I will try and predict these outcome variables using all of my tabular predictors. I have tabular data for all observations, but only some have complete  data, and based on the selectiveness of the different models the number of viable observations range from ~8,000 to just 2000 in the dataset. For each of these  models I will attempt to correctly classify which stocks increased or decreased in activity, which is the outcome variable/\n\nI will start by creating a single decision tree as a baseline, and I will determine what accuracy this tree is able to achieve in classifying increases and decreases in activity. I will also fit a random classifier, so that I can compare my later models to a purely random result. After I have run this baseline both with and without the text data, I will run a random forest model. Based on the performance of this model, I will also try a boosted model, and then I will pick one of the two for which to optimize hyperparamters to get the best result. \n\nDecision trees can work for problems like mine because they are exceptionally good at identifying nonlinear relationships in data. To understand how they work, imagine you are deciding between 20 new car models to buy. A decision tree is similar to how a human being might approach such a choice. In finding the best car to purchase, you might set a standard or threshold such as: \"less than x amount of dollars to purchase.\" This would eliminate some number of the cars. Then you would further ask for features such as \"all wheel drive,\" or \"seatwarmers,\" until eventually you had a car which was the closest to your true requirements. Decision trees are essentially a similar method as this anecdote, except they attempt to find the most efficient number of requirements you could specify to match a car to a given person (classification) or it's most ideal pricepoint(regression). In mathematical terms, they try to reduce the entropy of the dataset by dividing it along meaningful and efficient boundaries (called nodes that divide the data into branches), until only small distinct groups are left (called leaves).\n\n<h1> Class Distribution </h1>\n\nRead in and clean data:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom numpy import random\nfrom sklearn.metrics import accuracy_score\n\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntabData = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\") \ntabData.shape\n\ntextData = pd.read_csv(\"../data/01-modified-data/vectorizedReddit.csv\") \ntabData.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(9560, 23)\n```\n:::\n:::\n\n\nMore cleaning code to create the datasets.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntextsIn = textData\ndataIn = tabData\n\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\ntextsIn.shape\n\nfrom datetime import datetime, timedelta\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\ndataIn.SPYCloseWK = dataIn.SPYCloseWK.apply(lambda x: float(str(x).replace(\",\", \"\") ))\ndataIn.SPYHighWk = dataIn.SPYHighWk.apply(lambda x: float(str(x).replace(\",\", \"\") ))\ndataIn.SPYLowWK = dataIn.SPYLowWK.apply(lambda x: float(str(x).replace(\",\", \"\") ))\n\n# Join\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\nprocessed['activityIncrease'] = processed['deltaActivity'].apply(lambda x: int(x > 0) )\n\nprocessed2 = processed.dropna(subset=['date_utc'])\nprint(processed2.shape)\n\n\nexclude = ['lagweek','date_utc','title', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x']\n\n\nprocessed3 = processed2.drop(columns = exclude, axis = 1)\n\nprocessed3.head()\nX = processed3.drop('activityIncrease', axis=1)\ny = processed3['activityIncrease']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(2018, 35428)\n```\n:::\n:::\n\n\nClass distribution for full (8,000) and reduced (2,000) datasets.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Code to compute the class distribution\nxlocs = [0,1]\nxlabs = [\"Decreased\", \"Increased or Stayed the Same\"]\n\ntemp = processed2.loc[:,'activityIncrease'].astype(str)\ntemp2 = processed.loc[:,'activityIncrease'].astype(str)\n\nvalues, bins, bars = plt.hist(temp, edgecolor='white')\nplt.xticks(xlocs, xlabs)\nplt.bar_label(bars, fontsize=20, color='navy')\nplt.title(\"Class Distribution for Tickers with Reddit Posts \")\nplt.show()\n\nvalues, bins, bars = plt.hist(temp2, edgecolor='white')\nplt.xticks(xlocs, xlabs)\nplt.bar_label(bars, fontsize=20, color='navy')\nplt.title(\"Class Distribution for all Tickers in Dataset\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-5-output-1.png){width=660 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-5-output-2.png){width=660 height=431}\n:::\n:::\n\n\nBased on the plots, we can see that 827 stocks in the daily top 10 decreased in retail trader activity, while 1191 increased, for the data which had relevant reddit posts available. As a proportion, this is 0.41 of all the stocks in the top 10 with reddit posts. In the second plots, for all the tickers in the dataset, 4373 had a decrease, while 5648 had an increase.  As a proportion, this is 0.44 out of all stocks in the top 10. It is worth noting that the outcome variable differs in proportion between the categories.\n\n\n\n\n<h1> Baseline Model / Random Classifier </h1>\n\nNow, let's train a random classifier and a baseline decision tree on our two datasets, and visualize the tree that we construct. Starting with the random classifier:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nally = temp2.astype(int)\ntexty = temp.astype(int)\n\nrand_ally = random.randint(2, size = ally.shape[0])\nrand_texty = random.randint(2, size = texty.shape[0])\n\nconfusion_matrix = metrics.confusion_matrix(ally, rand_ally)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title(\"Confusion Matrix for Random Classifier, Text and Tabular Data \")\nplt.show()\nprint(accuracy_score(ally, rand_ally))\n\n\nconfusion_matrix = metrics.confusion_matrix(texty, rand_texty)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title(\"Confusion Matrix for Random Classifier, Text and Tabular Data \")\nplt.show()\n\nprint(accuracy_score(texty, rand_texty))\n```\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-6-output-1.png){width=542 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5072348069054985\n0.4821605550049554\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-6-output-3.png){width=533 height=449}\n:::\n:::\n\n\nBased on both the confusion matrices and the accuracy scores, we can see that the random classifier performs close to a coinflip, with 51% accuracy for the tabular data and 48% accuracy for the textual and tabular data combined. Now let's evaluate a single decision tree:\n\nFirst, we make sure we have train and validation data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntreeTab = dataIn\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x > 0) )\n\n\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'ticker', 'date.x', 'date.y', 'weekyear', 'week.x', 'Total', 'X8.week', 'week.y', 'lagweek'])\ntreeTaby = treeTab['activityIncrease']\n\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nclass_1 = DecisionTreeClassifier(max_depth=4)\n\nclass_1.fit(tree_trainx, tree_trainy)\n\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(class_1)\n\n```\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-8-output-1.png){width=1879 height=1498}\n:::\n:::\n\n\nLet's look at the confusion matrix:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nc1tpred = class_1.predict(tree_trainx)\nc1vpred = class_1.predict(tree_testx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c1vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c1vpred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7267828843106181\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-9-output-2.png){width=537 height=429}\n:::\n:::\n\n\nOur baseline tree obtains an impressive accuracy of 73.2% on the test set! Let's see if it performs similarly well on the text data. First, we prepare the text train and test split:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntreeTxt = processed3\n\ntreeTxtx =  treeTxt.drop(columns = ['activityIncrease'])\ntreeTxty = treeTxt['activityIncrease']\n\ntree_train_txtx, tree_test_txtx, tree_train_txty, tree_test_txty = train_test_split(treeTxtx, treeTxty, test_size= 0.33)\n```\n:::\n\n\nNow, we can fit our depth 4 tree:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nclass_2 = DecisionTreeClassifier(max_depth=4)\n\nclass_2.fit(tree_train_txtx, tree_train_txty)\n\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(class_2)\n\n```\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-11-output-1.png){width=1879 height=1498}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nc2tpred = class_2.predict(tree_train_txtx)\nc2vpred = class_2.predict(tree_test_txtx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_test_txty, c2vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_test_txty, c2vpred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5735735735735735\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-12-output-2.png){width=529 height=434}\n:::\n:::\n\n\nInterestingly, on the tree with the text data added, we see much worse performance, around 63%. This could certainly be due to the lack of training data when limiting to only those stock posts with reddit texts available. Notably, however, several of the text columns did enter into the decision tree, suggesting the text data was not completely meaningless. Let's see if we can get better performance by implementing bagging and random forests.\n\n<h1> Model Implementation and Hyperparameter Tuning </h1>\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntreeTab = dataIn\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x > 0) )\n\ntreeTab = treeTab.dropna()\n\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'ticker', 'date.x', 'date.y', 'weekyear', 'week.x', 'Total', 'X8.week', 'week.y', 'lagweek'])\ntreeTaby = treeTab['activityIncrease']\n\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\nrf.fit(tree_trainx, tree_trainy)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=1000, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nNow that we've fit the random forest model, let's diagnose and create plots:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nc3tpred = rf.predict(tree_trainx)\nc3vpred = rf.predict(tree_testx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c3vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c3vpred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7454248366013072\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-15-output-2.png){width=537 height=434}\n:::\n:::\n\n\nOut random forest classifier has only produced an accuracy of 73.5%, which is barely better than our naive decision tree. Let's try a gradient boosted classifier and see if it performs better:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ngbc = GradientBoostingClassifier(n_estimators = 1000, random_state = 42)\ngbc.fit(tree_trainx, tree_trainy)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(n_estimators=1000, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nc4tpred = gbc.predict(tree_trainx)\nc4vpred = gbc.predict(tree_testx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c4vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c4vpred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.746078431372549\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-17-output-2.png){width=537 height=430}\n:::\n:::\n\n\nSurprisingly, the gradient boosted classifier did worse, if anything, than the random forests. Let's try to select the optimal n_estimators hyperparameter and see if that helps our performance (I will use acccuracy scores for the plot)\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ntest_accuracy = []\ntrain_accuracy = []\nnestimators = []\n\nfor i in range(10, 150, 10):\n    print(i)\n    rf = GradientBoostingClassifier(n_estimators = i, random_state = 42)\n    rf.fit(tree_trainx, tree_trainy)\n    \n    testpreds = rf.predict(tree_testx)\n    trainpreds = rf.predict(tree_trainx)\n\n    train_accuracy.append(accuracy_score(tree_trainy, trainpreds))\n    test_accuracy.append(accuracy_score(tree_testy, testpreds))\n    nestimators.append(i)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplt.plot(nestimators, test_accuracy,label='Test Accuracy' )\nplt.plot(nestimators, train_accuracy,label='Train Accuracy' )\nplt.xlabel(\"Number of Estimators\")\nplt.ylabel(\"Accuracy of the Model\")\nplt.title(\"Accuracy vs. Number of Estimators for Boosted Forests\")\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nText(0.5, 1.0, 'Accuracy vs. Number of Estimators for Boosted Forests')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-19-output-2.png){width=597 height=449}\n:::\n:::\n\n\nViewing the hyperparameter chart, it seems clear that a smaller number of estimators, close to 100, actually leads to the best performance on the test set. Likely this is the consequence of overfitting. Regardless, the ideal parameter choice is clear as the best model is also the simplest, and leads to an accuracy of almost 75%, the best of any model to date.\n\n\n\n<h1> Final Results for Boosted Forest </h1>\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ngbc = GradientBoostingClassifier(n_estimators = 100, random_state = 42)\ngbc.fit(tree_trainx, tree_trainy)\n    \nc5tpred = gbc.predict(tree_trainx)\nc5vpred = gbc.predict(tree_testx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c5vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c5vpred))\n\nsub_tree = gbc.estimators_[50, 0]\n\nplot_tree(sub_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7526143790849673\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-20-output-2.png){width=537 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-20-output-3.png){width=1879 height=1498}\n:::\n:::\n\n\n## Adding in Text Data to Boosted Forests\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ntreeTab = processed2\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x > 0) )\n\n\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'lagweek','date_utc','title', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x'])\n\n\ntreeTaby = treeTab['activityIncrease']\n\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\corwi\\AppData\\Local\\Temp\\ipykernel_35040\\2411035305.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\ntest_accuracy = []\ntrain_accuracy = []\nnestimators = []\n\nfor i in range(1, 2):\n    print(i)\n    rf = HistGradientBoostingClassifier(random_state = 42)\n    rf.fit(tree_trainx, tree_trainy)\n    \n    testpreds = rf.predict(tree_testx)\n    trainpreds = rf.predict(tree_trainx)\n\n    train_accuracy.append(accuracy_score(tree_trainy, trainpreds))\n    test_accuracy.append(accuracy_score(tree_testy, testpreds))\n    nestimators.append(i)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nplt.plot(nestimators, test_accuracy,label='Test Accuracy' )\nplt.plot(nestimators, train_accuracy,label='Train Accuracy' )\nplt.xlabel(\"Number of Estimators\")\nplt.ylabel(\"Accuracy of the Model\")\nplt.title(\"Accuracy vs. Number of Estimators for Boosted Forests\")\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nText(0.5, 1.0, 'Accuracy vs. Number of Estimators for Boosted Forests')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](dectrees_files/figure-html/cell-23-output-2.png){width=597 height=449}\n:::\n:::\n\n\n<h1> Conclusions </h1>\n\nOverall, the boosted forest model was the best performing out of the 1) simple decision tree 2) random forest and 3) boosted models. This also clearly outperformed a random baseline, and was most robust with 100 estimators included in the model. The boosted forest forest model performs very well (for a financial model) at 74.7% accuracy. This is much better than a 50% baseline and still well above a most-common-outcome classifier which would have 56.5% accuracy. This provides a remarkable picture of what drives investor activity in individual stocks, although it may not be the complete picture still. To understand what features the model uses to predict with accuracy, let's trace one sample pathway through one component tree of the boosted model:\n\nLooking at the final tree above (one of many in the classifier, but chosen to illustrate the internal relationships that power the mode). The features, going down the tree are: First a split based on current investor activity in the stock, if it is lower than 0.043 then the model check whether the spread in investor sentiment (bearish bullish) is wider than -0.33, which would indicate a bearish environment. Then, the model would predict based on investor activity in the stock. \n\nReviewing this example, it appears the model is very interested in the sentiment of investors in the survey, particularly bearish sentiment, as well as the activity of retail investors in a stock on the current day. \n\nConcerns: One concern I have is that the amount of data is somewhat small <3000 by the end for the random forests given excluded NA values. This is a concern because it allows models such as decision trees to overfit, and this is shown in the random forest having almost perfect accuracy in the training set, but no improvements in the test set with more estimators. I am also concerned that the CURRENT investor activity index is being used to predict the change for the next day, as this worries me that there is some conflation in the acitivty day over day. I would be curious to confirm how all of the data is defined, to ensure we are actually predicting the future without having more information than is realistic in real time.\n\nFuture directions: I would like to incorporate the text data as well, to make the predictions even more robust.\n\n",
    "supporting": [
      "dectrees_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}