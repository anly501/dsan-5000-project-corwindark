[
  {
    "objectID": "naivebayes/naivebayes.html",
    "href": "naivebayes/naivebayes.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Introduction to Naive Bayes\n\nA naive bayes classifier assigns labels to observations using a process based on Bayes’ theorem. The important element of the theorem is updating the probability of an event based on the known prior events. For instance, if all vehicles with 2 wheels are motorcycles, then we can update the probability that an unknown vehicle is a motorcycle given whether we know it has two wheels. In the case of a classification algorithm, the prosterior probabilities in question are the likelihoods that a given observation belongs to each class. Using the known prior events to update the probability of each class, the algorithm would take whichever class label had the highest likelihood and assign that to the data point in question.\nA naive Bayes classifier has several variants based on the types of features being used to predict class labels. - Gaussian: If the predictors are continuous, then we likely will not have exact probabilities for the given input point (as no other point may have the exact same value for all input features). As such, a different approach is required, where it is assumed that the continuous attributes are normally distributed. Then, for a given point that needs to be classified, we can express the probability of it belonging to a certain class as the likelihood that the point came from each class’s distribution. This is relatively easy, as we can simply find the likelihood that the point was generated by each class distribution, and assign the most likely one as the label. - Multinomial: This method is preferred when the predictors are generated by a multinomial distribution, meaning they are discrete, with more than 2 levels. Labels would be assigned to new observations based on the conditional probability of observing the point’s attributes given that it came from each class (which can be calculated again using Baye’s theorem since the attributes are discirete). - Bernoulli Naive Bayes: The Bernoulli method is preferred when the predictors are binary, meaning they are discrete but only have two values they can take. This is calculated similar to the multinomial variant, but is even more straightforwards as there are only two classes.\n\nNaive Bayes for Retail Investor Activity\n\nObjective of Naive Bayes for my Project: With my Naive Bayes classifier, I hope to identify stocks in the top 10 by retail investor activity that have increases in retail investor activity on a given day. To do this I will convert a continuous variable (the daily change in retail investor activity) into two classes based on whether a given stock increased or decreased in activity day-over-day.\nTo predict this outcome variable for each stock in the top 10, I will use two different sets of predictors in the Naive Bayes classifier. First, I will use my tabular data: - Individual investor sentiment from the proceding week, including the percent of respondents who were bearish, bullish, and neutral on the market. - Tabular data from reddit posts including the number of comments.\nSecond, I will use my textual data to predict the same increase/decrease outcome variable: - A corpus of 7.5k reddit posts mentioning top-10 tickers, which contain about 50,000 unique words.\n\nPrep Data for Naive Bayes\n\nWe are splitting the data into training, validation, and test sets to ensure we optimally fit the model and avoid over or under fitting. In particular, we can use the validation set to tune our model and improve its generalizability, and then use the test set to evaluate the performance of our model. I will do a 70-15-15 split between the train, evaluation, and test data, and this is done simply by randomly splitting the dataset.\nFor the text data: I will prepare the corpus in a few steps. 1. I have already used a stemmer to reduce the number of unique words by about 7,000. After this reduction we still have 50,000 words. 2. I will filter out words with &lt;5 total uses in the corpus. 3. Then, I will join my binary outcome variable with the reddit text data according to the date of each post. The outcomes will be lagged by 1 day, as we want to ensure that the reddit mentions preceeded the change in a stock’s trading activity. As an example of how this will work: the outcome variable for a post that mentioned ticker QQQ on 12/1 would be 1 or 0 depending on whether QQQ increased or decreased in activity on 12/2.\n\nfrom sklearn.model_selection import train_test_split\nimport csv\nimport pandas as pd\nimport os\n\nos.getcwd()\n\ndataIn = pd.read_csv('../data/01-modified-data/joinedSentiment.csv')\n\nprint(dataIn['deltaActivity'] == 0)\n\n0       False\n1       False\n2        True\n3       False\n4       False\n        ...  \n9555    False\n9556    False\n9557    False\n9558    False\n9559    False\nName: deltaActivity, Length: 9560, dtype: bool\n\n\n\n# Let's read in our vectorized data from reddit\ntextsIn = pd.read_csv('../data/01-modified-data/vectorizedReddit.csv')\n\n# Double check shape\nprint(textsIn.shape)\n\n(7436, 49692)\n\n\n\n# Check columns are sorted by occurence\nprint(textsIn.sum(axis=0))\n\n# Looks like it is sorted in the proper order\n\n# How many columns are for a word that is only used once?\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\nprint(textsIn.head())\n\n\n\ndate_utc    2020-07-302020-09-152022-01-032021-11-122020-0...\ntitle       Sample ETF portfolioPsycho Market Recap - Tue ...\nticker      QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ...\ncomments                                            1065358.0\n00                                                       4387\n                                  ...                        \nïif                                                         1\nïred                                                        1\nïthere                                                      1\nïyou                                                        1\nüü                                                          1\nLength: 49692, dtype: object\n     date_utc                                              title ticker  \\\n0  2020-07-30                               Sample ETF portfolio    QQQ   \n1  2020-09-15             Psycho Market Recap - Tue Sept 15 2020    QQQ   \n2  2022-01-03  Here is a Market Recap for today Monday, Janua...    QQQ   \n3  2021-11-12  Here is a Market Recap for today Friday, Novem...    QQQ   \n4  2020-09-30  Here is a Market Recap for today Wed, Sept 30....    QQQ   \n\n   comments  00  000  00000  000000000  000001  0000050863  ...  zyne  \\\n0      43.0   0    0      0          0       0           0  ...     0   \n1      17.0   0    0      0          0       0           0  ...     0   \n2      27.0   0    1      0          0       0           0  ...     0   \n3       6.0   0    0      0          0       0           0  ...     0   \n4       5.0   0    2      0          0       0           0  ...     0   \n\n   zynerba  zynex  zynga  zypot4g  zyq8gpy  zyxi  zzhq7gy  zzpcoa9  zzqa2sj  \n0        0      0      0        0        0     0        0        0        0  \n1        0      0      0        0        0     0        0        0        0  \n2        0      0      0        0        0     0        0        0        0  \n3        0      0      0        0        0     0        0        0        0  \n4        0      0      0        0        0     0        0        0        0  \n\n[5 rows x 35405 columns]\n\n\n\n# How big a change was removing single-use words?\ntextsIn.shape\n# After removing words only used once,  we have 35k words\nprint(dataIn.head)\n\n\n# Now, lets try and fit the GNB\n\n# We will need to merge in the activity change data, lagged by one day\nfrom datetime import datetime, timedelta\n\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n\n\n\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n\n&lt;bound method NDFrame.head of       Unnamed: 0     X      date.x ticker  activity  sentiment  deltaActivity  \\\n0              1  9551  2020-01-02   TSLA    0.0226          0        -0.0014   \n1              2  9552  2020-01-02    SPY    0.0147          3        -0.0341   \n2              3  9553  2020-01-02   ROKU    0.0119          0         0.0000   \n3              4  9554  2020-01-02    QQQ    0.0093          6        -0.0111   \n4              5  9555  2020-01-02   MSFT    0.0116          2        -0.0181   \n...          ...   ...         ...    ...       ...        ...            ...   \n9555        9556     6  2023-10-18   NVDA    0.0695          2         0.0428   \n9556        9557     7  2023-10-18   META    0.0124          1        -0.0064   \n9557        9558     8  2023-10-18   AMZN    0.0185          6        -0.0016   \n9558        9559     9  2023-10-18    AMD    0.0264          2         0.0025   \n9559        9560    10  2023-10-18   AAPL    0.0255          5        -0.0204   \n\n      deltaSentiment  newEntry  week.x  ... Neutral Bearish  Total  X8.week  \\\n0                  0     False       1  ...     NaN     NaN    NaN      NaN   \n1                 -7     False       1  ...     NaN     NaN    NaN      NaN   \n2                  0      True       1  ...     NaN     NaN    NaN      NaN   \n3                  1     False       1  ...     NaN     NaN    NaN      NaN   \n4                  1     False       1  ...     NaN     NaN    NaN      NaN   \n...              ...       ...     ...  ...     ...     ...    ...      ...   \n9555               0     False      42  ...     NaN     NaN    NaN      NaN   \n9556               2     False      42  ...     NaN     NaN    NaN      NaN   \n9557               4     False      42  ...     NaN     NaN    NaN      NaN   \n9558              -1     False      42  ...     NaN     NaN    NaN      NaN   \n9559               6     False      42  ...     NaN     NaN    NaN      NaN   \n\n      Spread SPYHighWk SPYLowWK  SPYCloseWK week.y lagweek  \n0        NaN       NaN      NaN         NaN    NaN     NaN  \n1        NaN       NaN      NaN         NaN    NaN     NaN  \n2        NaN       NaN      NaN         NaN    NaN     NaN  \n3        NaN       NaN      NaN         NaN    NaN     NaN  \n4        NaN       NaN      NaN         NaN    NaN     NaN  \n...      ...       ...      ...         ...    ...     ...  \n9555     NaN       NaN      NaN         NaN    NaN     NaN  \n9556     NaN       NaN      NaN         NaN    NaN     NaN  \n9557     NaN       NaN      NaN         NaN    NaN     NaN  \n9558     NaN       NaN      NaN         NaN    NaN     NaN  \n9559     NaN       NaN      NaN         NaN    NaN     NaN  \n\n[9560 rows x 23 columns]&gt;\n\n\n\n# Now we subtracted one day, let's add the categorical label variable we are trying to predict and then split the data\n\ndataIn['activityIncrease'] = dataIn['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n# Change percent signs to decimals\n#dataIn['Bearish'] = dataIn['Bearish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Bullish'] = dataIn['Bullish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Neutral'] = dataIn['Neutral'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Spread'] = dataIn['Spread'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n\n\n\nAttributeError: 'float' object has no attribute 'rstrip'\n\n\n\n# Now that we subtracted one day in the cell above, let's merge the text corpus with the daily activity label\n\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\n\n# now let's check that our merge worked and then split into test, training, and validation\n#print(processed.iloc[:,range(15,25)].head)\n\n#print(processed.isna().sum() &lt; processed.shape[0])\n\nprocessed2 = processed.dropna(subset=['date_utc'])\nprint(processed2.head)\n\ntrain, test = train_test_split(processed2, test_size=0.15)\ntrain, validate = train_test_split(train, test_size = 0.177)\n\n&lt;bound method NDFrame.head of       Unnamed: 0     X      date.x ticker  activity_x  sentiment_x  \\\n6              7  9557  2020-01-01   BABA      0.0088            0   \n45            46  9516  2020-01-07   BYND      0.0139            1   \n50            51  9501  2020-01-08   TSLA      0.0689            1   \n55            56  9506  2020-01-08   BYND      0.0144            2   \n56            56  9506  2020-01-08   BYND      0.0144            2   \n...          ...   ...         ...    ...         ...          ...   \n9816        9356   206  2023-09-19   NVDA      0.0445            1   \n9817        9357   207  2023-09-19   MSFT      0.0121            0   \n9819        9359   209  2023-09-19    AMD      0.0330           -6   \n9820        9360   210  2023-09-19   AAPL      0.0323            2   \n9999        9539    29  2023-10-15    AMD      0.0264           -1   \n\n      deltaActivity  deltaSentiment  newEntry  week.x  ... zyne zynerba  \\\n6           -0.0103              -2     False       1  ...  0.0     0.0   \n45           0.0027              -1     False       2  ...  0.0     0.0   \n50          -0.0110               0     False       2  ...  0.0     0.0   \n55           0.0005               1     False       2  ...  0.0     0.0   \n56           0.0005               1     False       2  ...  0.0     0.0   \n...             ...             ...       ...     ...  ...  ...     ...   \n9816         0.0152               1     False      38  ...  0.0     0.0   \n9817        -0.0038              -2     False      38  ...  0.0     0.0   \n9819         0.0033             -12     False      38  ...  0.0     0.0   \n9820         0.0018               3     False      38  ...  0.0     0.0   \n9999         0.0084              -4     False      42  ...  0.0     0.0   \n\n      zynex  zynga  zypot4g zyq8gpy zyxi  zzhq7gy zzpcoa9 zzqa2sj  \n6       0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n45      0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n50      0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n55      0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n56      0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n...     ...    ...      ...     ...  ...      ...     ...     ...  \n9816    0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n9817    0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n9819    0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n9820    0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n9999    0.0    0.0      0.0     0.0  0.0      0.0     0.0     0.0  \n\n[2018 rows x 35428 columns]&gt;\n\n\n\n# Now we have our filtered data, with 2000 observations. Train is 1411\n\nprint(train.shape)\n\n# Now we can try and fit our GNB classifier\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(Xtrn, Xtst, Ytrn, Ytst, i_print=False):\n\n    #if(i_print):\n        #print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=Xtrn\n    y_train=Ytrn\n\n    x_test=Xtst\n    y_test=Ytst\n\n    # INITIALIZE MODEL \n    model = GaussianNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n\n\n(1411, 35428)\n\n\n\n# Test function on training and validation set\nnonNumericColumns = ['X', 'activityIncrease', 'date.x', 'Total', 'X8.week', 'date.x', 'SPYHighWk', 'SPYLowWK', 'SPYCloseWK', 'date_utc', 'title', 'ticker', 'date.y', 'sentiment_x', 'deltaActivity', 'deltaSentiment', 'activity_x', 'week.x', 'weekyear', 'newEntry']\n\n\n# All of the below are for the text model\nxtrn = train.drop(columns = nonNumericColumns)\nxtst = validate.drop(columns = nonNumericColumns)\nxreal = test.drop(columns = nonNumericColumns)\n\n# replace remaining NAs\nxtrn = xtrn.fillna(0)\nxtst = xtst.fillna(0)\nxreal = xreal.fillna(0)\n\n# pull out the output feature (increase 1 or 0)\nytrn = train['activityIncrease']\nytst = validate['activityIncrease']\nyrl = test['activityIncrease']\n\n\n# These are for the non-text, tabular model\nxtabtrn = xtrn.iloc[:,0:8]\nxtabtst = xtst.iloc[:,0:8]\nxtabreal = xreal.iloc[:,0:8]\n\n\n#Same output features, no need to repeat, and NAs are already replaced\n\n\n\n\nprint(list(xtabtrn.columns.values) )\n\n\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtrn, xtst, ytrn, ytst, i_print=True)\n\n['Unnamed: 0', 'Bullish', 'Neutral', 'Bearish', 'Spread', 'week.y', 'lagweek', 'comments']\n74.4153082919915 55.92105263157895 1.046875 1.375\n\n\n\nFeature selection for Record data\n\n\ndef initialize_arrays_tab():\n    global num_features_tab,train_accuracies_tab\n    global test_accuracies_tab,train_time_tab,eval_time_tab\n    num_features_tab=[]\n    train_accuracies_tab=[]\n    test_accuracies_tab=[]\n    train_time_tab=[]\n    eval_time_tab=[]\n\n\n# start arrays\ninitialize_arrays_tab()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search_tab(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        \n        # temp data\n        xtraintemp =xtabtrn.iloc[:,0:upper_index]\n        xtesttemp =xtabtst.iloc[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtraintemp, xtesttemp, ytrn, ytst, i_print=False)\n\n        if(i%1==0):\n            print(i,upper_index,xtraintemp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features_tab.append(xtraintemp.shape[1])\n        train_accuracies_tab.append(acc_train)\n        test_accuracies_tab.append(acc_test)\n        train_time_tab.append(time_train)\n        eval_time_tab.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search_tab(num_runs=7, min_index=0, max_index= xtabtrn.shape[1])\n\n1 1 1 57.33522324592487 57.89473684210527\n2 2 2 59.74486180014175 59.86842105263158\n3 3 3 59.88660524450744 60.85526315789473\n4 4 4 61.374911410347266 59.210526315789465\n5 5 5 60.17009213323884 58.223684210526315\n6 6 6 61.09142452161588 57.56578947368421\n7 7 7 63.07583274273565 60.19736842105263\n\n\n\n# More helper functions\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef save_results_tab(path_root):\n    out= np.transpose(np.array([num_features_tab,train_accuracies_tab,test_accuracies_tab,train_time_tab,eval_time_tab])) \n    out= pd.DataFrame(out)\n    out.to_csv(path_root+\"_tab.csv\")\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results_tab(path_root):\n\n    #PLOT-1\n    plt.plot(num_features_tab,train_accuracies_tab,'-or')\n    plt.plot(num_features_tab,test_accuracies_tab,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'_tab-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features_tab,train_time_tab,'-or')\n    plt.plot(num_features_tab,eval_time_tab,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'_tab-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies_tab),train_time_tab,'-or')\n    plt.plot(np.array(test_accuracies_tab),eval_time_tab,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'_tab-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features_tab,np.array(train_accuracies_tab)-np.array(test_accuracies_tab),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'_tab-4.png')\n    plt.show()\n\n\noutput_dir = \"./\"\nsave_results_tab(output_dir+\"/partial_grid_search\")\nplot_results_tab(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature selection for Text data\n\n\n# now lets start removing features using code from the lab demo\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# start arrays\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        \n        # temp data\n        xtraintemp =xtrn.iloc[:,0:upper_index]\n        xtesttemp =xtst.iloc[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtraintemp, xtesttemp, ytrn, ytst, i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtraintemp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtraintemp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index= xtrn.shape[1])\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n#partial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n5 1770 1770 67.54075124025513 53.61842105263158\n10 3540 3540 68.67469879518072 53.289473684210535\n15 5310 5310 70.16300496102055 53.61842105263158\n20 7080 7080 71.58043940467753 53.94736842105263\n25 8850 8850 72.85613040396882 53.61842105263158\n30 10620 10620 73.49397590361446 51.64473684210527\n35 12390 12390 73.42310418143161 52.63157894736842\n40 14160 14160 73.5648476257973 52.30263157894737\n45 15930 15930 73.8483345145287 51.973684210526315\n50 17700 17700 74.20269312544295 53.94736842105263\n55 19470 19470 73.91920623671156 53.94736842105263\n60 21240 21240 73.8483345145287 52.30263157894737\n65 23010 23010 74.1318214032601 53.61842105263158\n70 24780 24780 74.06094968107725 52.960526315789465\n75 26550 26550 74.1318214032601 54.60526315789473\n80 28320 28320 74.20269312544295 53.61842105263158\n85 30090 30090 73.99007795889439 53.94736842105263\n90 31860 31860 74.1318214032601 53.94736842105263\n95 33630 33630 74.27356484762579 54.276315789473685\n100 35400 35400 74.4153082919915 55.92105263157895\n\n\n\n# More helper functions\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef save_results(path_root):\n    out= np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out= pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\noutput_dir = \"./\"\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes with Labeled Record Data\n\n\n# Based on previous section, using all 8 features is ideal\n#  fit model finally, report on overall accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nrealTabModel = GaussianNB()\nrealTabModel.fit(xtabtrn, ytrn)\n\nrealTabTestPreds = realTabModel.predict(xtabreal)\n\nconfusion_matrix = metrics.confusion_matrix(yrl.values, realTabTestPreds)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.show()\n\ntabOutcomeData = pd.DataFrame.from_dict( {'ticker': test['ticker'], 'comments': xtabreal['comments'], 'real value': yrl.values, 'predicted value': realTabTestPreds, \"miss\": yrl.values - realTabTestPreds} )\n\nsns.scatterplot(data=tabOutcomeData.groupby(['ticker'])['miss'].mean().to_frame(), x = 'miss', y = 'ticker')\nplt.xlabel('Average Prediction Miss (-1 = mistaken increase, 1 = mistaken decrease)')\nplt.ylabel('Stock Ticker')\nplt.title('Average Prediction Misses by Stock')\nplt.show()\n\nsns.boxplot(data=tabOutcomeData, x = 'predicted value', y = 'comments')\nplt.xlabel('Model Prediction (0 decrease, 1 increase)')\nplt.ylabel('Number of Comments on Related Posts')\nplt.title('Predictions vs. Comments on Reddit Posts')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe optimal model for the record data was the one that included all 7 predictors. I identified this in the feature selection plots using the elbow method, which showed that the increase in performance never leveled off as more tabular features were added.\nThen, I fit this model on the training data and used it to predict values in the test set, the results of which are pictured in the first image above, the confusion matrix. While I expected the model to perform worse on the test set, the overall accuracy score (~0.64) was similar to those observed in the training (~0.60) and validation (~0.63) sets. The model appears to have learned to be successful by predicting more 1’s (or increases) than 0’s (or decreases). This gave the model high sensitivity but low precision.\nIn terms of over and under fitting, I believe my model is currently underfit although it includes all available features, with poor performance over all. I do not believe the record data model has been over fit because it has similar performance in both the training and test sets, so it has low variance. Yet the model has high bias in both training and test sets, only capturing an accurate result 60% of the time, barely beating a coin toss.\nThe projects findings will be documented in a final slideshow, which will polish up and abridge the work contained in this page.\nIn conclusion: A Gaussian Naive Bayes classifier performs moderately well at predicting whether a given stock in the top-10 rankings of retail trader activity will see an increase or decrease in activity on the next trading day. With only 7 input features (such as the sentiment of individual investors and the number of reddit comments on posts mentioning the stock), the model identifies increases or decreases correctly about 64% of the time. While this is an improvement over random chance, the model could certainly be improved. The model also shows some relationships between variables: Certain stocks were almost always predicted over-optimisticly, such as SNAP (Snapchat), which was a famous stock that retail investors were interested in but lost money on. The number of comments also appears to be negatively correlated to stock increases, with tickers that had more comments on related posts being less likely to have an increase predicted the next day.\n\nNaive Bayes with Labeled Text Data\n\nUsing your optimal feature set from the previous section, fit a final “optimal” NB model for your Record data. Report and comment on the findings. It is required that you create code, appropriate visualizations, result summaries, confusion matrices, etc Describe how the trained model is tested on the testing dataset. Discuss the evaluation metrics used to assess the performance of the Naive Bayes classifier (e.g., accuracy, precision, recall, F1-score). Discuss the concepts of overfitting and under-fitting and whether your model is doing it. Discuss the model’s performance in terms of accuracy and other relevant metrics. Describe how the project findings will be documented and reported, including the format of reports or presentations. e.g. what is the output that you generate. What does the output mean? What does it tell you about your data? Does your model do a good job of predicting your test data? Include and discuss relevant visualizations, results, the confusion matrices, etc . Create and include a minimum of three visualizations for each case (text and record classification). Write a conclusion paragraph interpreting the results. Note, this is not the same as a write-up of technical methodological details.\n\n# fit model finally, report on overall accuracy\n# based on the previous charts, we can use the elbow method for \n\nrealModel = GaussianNB()\nrealModel.fit(xtrn.iloc[:,0:10000], ytrn)\n\n\nrealTestPreds = realModel.predict(xreal.iloc[:,0:10000])\nprint(realTestPreds)\nprint(yrl.values)\n\nconfusion_matrix = metrics.confusion_matrix(yrl.values, realTestPreds)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.show()\n\noutcomeData = pd.DataFrame.from_dict( {'ticker': test['ticker'], 'comments': xreal['comments'], 'real value': yrl.values, 'predicted value': realTestPreds, \"miss\": yrl.values - realTestPreds} )\n\nsns.scatterplot(data=outcomeData.groupby(['ticker'])['miss'].mean().to_frame(), x = 'miss', y = 'ticker')\nplt.xlabel('Average Prediction Miss (-1 = mistaken increase, 1 = mistaken decrease)')\nplt.ylabel('Stock Ticker')\nplt.title('Average Prediction Misses by Stock')\nplt.show()\n\nsns.boxplot(data=outcomeData, x = 'predicted value', y = 'comments')\nplt.xlabel('Model Prediction (0 decrease, 1 increase)')\nplt.ylabel('Number of Comments on Related Posts')\nplt.title('Predictions vs. Comments on Reddit Posts')\nplt.show()\n\n[1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0\n 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1\n 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n 0 0 1 1 1 1 1]\n[1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0\n 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1\n 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1\n 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0\n 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1\n 1 1 1 1 0 0 1]\n\n\n\n\n\n\n\n\n\n\n\nFor the textual data Gaussian Naive Bayes classifier, it required more deduction to pick the correct number of features. This is because I began with about 35,000 columns in the dataset, due to the large number of unique words used in the reddit text corpus. However, using the elbow method, it appeared that the model’s gains in performance peaked around the 10,000 feature mark. This number was also the point where the model reached its approximate peak in performance in the validation set, so I was curious to extend it to the test set and determine if it would continue to have low bias.\nAs such, I fit this model on the training data and used it to predict values in the test set, the results of which are pictured in the first image above, the confusion matrix. Surprisingly, the far more comlicated model performed worse on the test set in terms of overall accuracy than the simple, 7-feature model (~0.61 instead of ~0.64). This performance was similar to those observed in the training (~0.60) and validation (~0.63) sets. Once again, the model appears to have learned to be successful by predicting more 1’s (or increases) than 0’s (or decreases). This gave the model high sensitivity but low precision.\nIn terms of over and under fitting, I believe the text data model model is still underfit, because it continues to have high bias and low variance across the training and test set. This is supported by the model’s similar accuracy in both the training and test datasets. These findings will also be incorporated into a slideshow, along with, potentially, further work to tune the text data analysis.\nIn conclusion: A Gaussian Naive Bayes classifier trained on 1400 reddit posts performs moderately well at predicting whether a given stock in the top-10 rankings of retail trader activity will see an increase or decrease in activity on the next trading day. With 10,000 input features, the model identifies increases or decreases correctly about 61% of the time. While this is an improvement over random chance, the model could certainly be improved. As a point of interest, I included the same tabular data points in the text model as well, and found the relationships between the variables had changed. When accounting for textual data from reddit, the model begins to predict increases for stock tickers with a large number of comments in related posts. Further, the model’s performance changed for particular stocks, with SNAP (Snapchat) no longer being tied with SQQQ for the most optimistic performance. Based on these results, it does not seem that reddit post texts contain a large amount of useful information for predicting retail investor actvity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "dimreduction/dimreduction.html",
    "href": "dimreduction/dimreduction.html",
    "title": "\nDimensionality Reduction: Introduction\n",
    "section": "",
    "text": "Dimensionality Reduction: Introduction\n\n\nProject Proposal\n\nThe objective of this project is to reduce the tabular data’s dimensionality, breaking down the range of disparate values into vectors which combine the meaninfgul signal contained in the data sources I have aggregated. This can enable faster processing of the data in other analyses such as decision trees or regression models, and it will also enable better performance by excluding noise from the data where possible. In terms of tools: I will use Python’s scikit learn and its sub-package scikit learn.metrics, particularly the PCA and tSNE methods, to analyze the data and attempt dimensionality reduction.\nThe record dataset I will be analyzing for dimensionality reduction includes: 1 - The daily level of retail activity for the top 10 most active stock tickers, 2 - The daily change in the level of retail activity for the top 10 most active stock tickers, 3 - Weekly individual investor survey data (columns for the percent of respondents that were bearish, bullish, or neutral), 4 - Weekly change in major stock indices prices, 4 - weekly stocktwits rankings of the most active stock tickers.\nThe dataset described above was selected by using all of the available record data that I gathered for this project. This is because data pertaining to retail investor sentiment and trading is exceptionally scarce online, and so it makes sense to use all available data for topics such as dimensionality reduction. Text data was not used due to its fundamental differences from the record data, and computational requirements given the 10,000 columns.\n\nCode Implementation:\n\n\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport pandas as pd\nimport seaborn as sns\n\n\nimport os\nos.getcwd()\n\n\n'c:\\\\Users\\\\corwi\\\\Documents\\\\dsan-5000-project-corwindark\\\\dsan-website\\\\5000-website\\\\dimreduction'\n\n\n\n# Read data in:\ntabData = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\") \ntabData.shape\n\n(9560, 23)\n\n\n\n# Prep data for PCA and T-SME\nnonNumericColumns = ['X', \"Unnamed: 0\", 'week.y', 'lagweek', 'sentiment',  'date.x', 'Total', 'X8.week', 'date.x', 'ticker', 'date.y', 'deltaSentiment',  'week.x', 'weekyear', 'newEntry']\n\nx = tabData.drop(columns = nonNumericColumns)\nx = x.dropna()\n\nxsave = x\n\nx = x.drop(columns = ['deltaActivity'])\n\nx.SPYCloseWK = x.SPYCloseWK.apply(lambda x: float(x.replace(\",\", \"\") ))\nx.SPYHighWk = x.SPYHighWk.apply(lambda x: float(x.replace(\",\", \"\") ))\nx.SPYLowWK = x.SPYLowWK.apply(lambda x: float(x.replace(\",\", \"\") ))\n\n\n# Implement PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(x)\nprint('\\nPCA')\nprint(pca.components_)\n\nprint(pca.explained_variance_ratio_ * 100)\n\n\nPCA\n[[-1.68248660e-06 -9.38301381e-06 -2.92182977e-05  3.85961909e-05\n  -4.79948736e-05 -5.59590301e-01 -5.97065315e-01 -5.74779697e-01]\n [ 1.02733629e-05 -6.50141479e-04 -2.83986205e-04  9.34058655e-04\n  -1.58436440e-03  7.83508069e-01 -6.07178267e-01 -1.32082442e-01]\n [-3.33404703e-06 -4.95640335e-05 -6.83867804e-05  1.18117062e-04\n  -1.67786032e-04 -2.70132741e-01 -5.24257322e-01  8.07578178e-01]]\n[99.45726907  0.34201376  0.20071211]\n\n\n\n# Given that the first component explains so much of the variance, let's do PCA with only 2 components\npca = PCA(n_components=2)\npca.fit(x)\nprint('\\nPCA')\nprint(pca.components_)\n\nxpca = pca.fit_transform(x)\n\nprint(pca.explained_variance_ratio_ * 100)\n\n\nPCA\n[[-1.68248660e-06 -9.38301381e-06 -2.92182977e-05  3.85961909e-05\n  -4.79948736e-05 -5.59590301e-01 -5.97065315e-01 -5.74779697e-01]\n [ 1.02733629e-05 -6.50141479e-04 -2.83986205e-04  9.34058655e-04\n  -1.58436440e-03  7.83508069e-01 -6.07178267e-01 -1.32082442e-01]]\n[99.45726907  0.34201376]\n\n\n\n# Implement TSNE\nfrom sklearn.manifold import TSNE\n\n#xt = x.drop_duplicates()\ntsne = TSNE(n_components=3, random_state=21)\nX_tsne = tsne.fit_transform(x)\ntsne.kl_divergence_\n\n\n\n-4.008790016174316\n\n\n\n# Parameter tuning for TSNE\nkl_divergences =  []  \n# first val is perplexity, second is KL divergence\n\nfor i in range(1,10):\n    print(i)\n    tsne = TSNE(n_components=2, perplexity= i, random_state=21)\n    X_tsne = tsne.fit_transform(x)\n    kl = tsne.kl_divergence_\n    kl_divergences.append([ [i], [kl] ])\n\nprint(kl_divergences)\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n[[[1], [-0.40658706426620483]], [[2], [-0.4820026159286499]], [[3], [-0.505386471748352]], [[4], [-0.7875299453735352]], [[5], [-0.9650571346282959]], [[6], [-1.0686755180358887]], [[7], [-1.1880613565444946]], [[8], [-1.2203705310821533]], [[9], [-1.407416820526123]]]\n\n\n\nkls = pd.DataFrame(kl_divergences)\n#print(kls.shape)\nkls.columns = ['Perplexity', 'KL_Divergence']\n#print(kls.head())\nkls.Perplexity = kls.Perplexity.apply(lambda x: x[0])\nkls.KL_Divergence = kls.KL_Divergence.apply(lambda x: x[0])\n\n#print(kls.head())\n#kls = pd.DataFrame(kl_divergences, columns = ['Perplexity', 'KL_Divergence']) \nsns.lineplot(kls, x = 'Perplexity', y = 'KL_Divergence') \n\n&lt;Axes: xlabel='Perplexity', ylabel='KL_Divergence'&gt;\n\n\n\n\n\nFrom looking at the plot of perplexity versus KL divergence, it appears that the lower values of perplexity performed better and yielded a KL divergence closer to 0. As such, I would use perplexity = 1 for this model.\n\nProject Report\n\nLet’s start with the results of the principle component analysis:\n\nincreaseActivity = (xsave['deltaActivity'] &gt; 0)\nincreaseActivity.head()\n\n#plt.hist(xpca[:,0], hue=increaseActivity)\n#plt.show()\n\n\n\nplt.hist(xpca[increaseActivity,0],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(xpca[~(increaseActivity),0],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \nplt.title(\"Distribution of Increased/Decreased Activity Across the First Component\")\nplt.legend()\nplt.show()\n\n\nplt.hist(xpca[increaseActivity,1],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(xpca[~(increaseActivity),1],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \n\nplt.title(\"Distribution of Increased/Decreased Activity Across the Second Component\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nfrom scipy import stats\n\n\nstats.ttest_ind(xpca[increaseActivity,1], xpca[~(increaseActivity),1] )\n\nTtestResult(statistic=8.74005373734051, pvalue=2.7359772711526402e-18, df=9268.0)\n\n\nAs seen above, the PCA method produces principle components which are somewhat good at splitting our data into the classes we care about. There is a noticeable difference between the classes in the first principle component especially, where higher values had more decreases in activity, and lower values had less. This should be enough on its own to outperform a simple random classifier. Indeed, a T-Test where the two samples are the two possible values of the outcome variable (increase/decrease), and the means were of the values of the first principle component foe each ovservation, returned a p value close to zero. This demonstrates the statistically significant difference along this axes.\nInterestingly, of the 8 variables included in the tabular data, the first axis had negative coefficients for seven of them. This is particularly surprising because the first principle component also explained so much of the variation in the data at &gt;95%. However, I believe the reason for this became apparent on further investigation: All 7 features that shared negative coefficients were in some way related to positive sentiment or exciment around a stock’s value. The closing value of the S&P 500, the positive sentiment in investor surveys, and the number of reddit comments all relate to investors being excited about the stock in particular or the market in general. In contrast, the one positive coefficient was for the investors who responded with negative sentiment in the survey, and predictably a higher value in the first component was tied with a greater likelihood of retail investor activity decreases.\nIn contrast, TSNE’s reduced dimensions had little interpretable meaning.\n\nincreaseActivity = (xsave['deltaActivity'] &gt; 0)\nincreaseActivity.head()\n\nincreaseActivity = increaseActivity[0:X_tsne.shape[0]]\n#plt.hist(xpca[:,0], hue=increaseActivity)\n#plt.show()\n\n\nplt.hist(X_tsne[increaseActivity,0],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(X_tsne[~(increaseActivity),0],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \nplt.title(\"Distribution of Increased/Decreased Activity Across the First Component\")\nplt.legend()\nplt.show()\n\n\nplt.hist(X_tsne[increaseActivity,1],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(X_tsne[~(increaseActivity),1],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \n\nplt.title(\"Distribution of Increased/Decreased Activity Across the Second Component\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nWhile the TSNE result did not produce a dimension that separated the the outcome variable as cleanly as PCA, it did create distributions along the new dimensions that were far more normal and less skewed in a particular dimension. Compared to the PCA dimensions that had clear skew, the TSNE distributions were symmetrical.\n\nplt.scatter( x= X_tsne[increaseActivity,0], y = X_tsne[increaseActivity,1],\n         alpha=0.5, # the transaparency parameter\n         color = \"Orange\", \n         label='Increased Activity') \n  \nplt.scatter( x= X_tsne[~(increaseActivity),0], y = X_tsne[~(increaseActivity),1],\n         alpha=0.5, # the transaparency parameter\n         color = \"blue\", \n         label='Decreased Activity') \n  \n\n  \nplt.title(\"Distribution of Increased/Decreased Activity Across Components\")\nplt.legend()\nplt.show()\n\n\n\n\nWith regard to the components themselves, a visualization shows how the selected perplexity of one has created a cluster with a dense center and sparse exterior. Coloring the points by whether there was an increase in retail trader activity, we do not see much patter, as the two distributions are intertwined. However, we can also see that the distribution of both new dimensions is clearly normal, as identified earlier when reviewing the two dimensions individually.\n\nComparing and Contrasting PCA and TSNE\n\nBetween the two methods, I believe that PCA has produced a more useful result for my research question. Because PCA expresses the new dimension as a linear combination of the input features, I was able to identify a novel relationship in the data. In particular, this relationship was that the negative sentiment aspect of the investor survey was an oppositely-valenced feature from the others in my tabular dataset. The PCA model was easy to fit overall, and the elbow plot clearly showed that the majority of variance in the data was explained by just one principle component (~95%), with a second component being the maximum I would consider. This made it a relatively easy choice to chose 2 components for PCA, but after visualizing both of them I believe the second could perhaps be considered spurious in the future.\nFor TSNE, I found the method useful for visualizing my data in only two dimensions, after having 8 dimensions to begin with. The resulting distributions were not skewed and were easy to cross-reference with my outcome variable. Plotting the perplexity values vs. the KL Divergence score of the models showed that as perplexity increased, KL Divergence got further from 0. This also made hyperparameter selection a relatively easy choice, as I was able to conclude that the lowest perplexity score was also the best one in terms of KL divergence performance. As visualized above, this did not lead to the most meaninfgul conclusions, but the approach was an interesting contrast with PCA none the less.\nIn conclusion, I think that when visualization is required T-SNE is a better choice, but when you need to understand linear relationships between your variables, PCA is a better choice, as evidenced by what each method produced for my dataset."
  },
  {
    "objectID": "datagathering/datagathering.html",
    "href": "datagathering/datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "For this project, I need to gather a range of data on retail investor sentiment. Reviewing the literature, it does not appear that a consesus has been reached on which mediums are most useful for measuring retail investor trades, let alone their sentiment. After an extensive review, I decided to try and construct a set of trading signals from the following: 1. Daily NASDAQ data on the top-10 retail investor held companies (API through R Quandl Package) 2. Text data from popular stock-trading subreddits (Reddit API through R) 3. Stocktwits mentions of popular companies through (Stocksera API in Python) 4. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV) 5. Text data from major financial new companies\nStarting with dataset 1: Nasdaq retail investor holdings ``{r} library(tidyverse) library(Quandl) library(lubridate) library(RedditExtractoR) library(reticulate) library(xlsx)\nQuandl.api_key(“psq4nx69HDimf2kQcxZJ”) data &lt;- Quandl.datatable(“NDAQ/RTAT10”, paginate = TRUE)\ncleanData &lt;- data %&gt;% mutate(date = ymd(date)) %&gt;% filter(year(date) &gt; 2019)\ncleanData &lt;- cleanData[order(cleanData\\(date),] cleanData\\)deltaActivity &lt;- vector(mode = “numeric”, length = nrow(cleanData)) cleanData\\(deltaSentiment &lt;- vector(mode = \"numeric\", length = nrow(cleanData)) cleanData\\)newEntry &lt;- vector(mode = ‘logical’, length = nrow(cleanData))\nfor(i in 1:nrow(cleanData)) { if(i %% 1000 == 0) { print(i) } prevDay &lt;- cleanData %&gt;% filter(day(date) == day(cleanData\\(date[i]) - 1) %&gt;%  filter(ticker == cleanData\\)ticker[i])\nif(nrow(prevDay) &gt; 0) {\n\n    cleanData$deltaActivity[i] = cleanData$activity[i] - prevDay$activity[1]\n    cleanData$deltaSentiment[i] = cleanData$sentiment[i] - prevDay$sentiment[1]\n    cleanData$newEntry[i] = FALSE\n\n}else {\n    cleanData$newEntry[i] = TRUE\n}\n}\ncolnames(cleanData) hist(cleanData$deltaActivity)\nhead(cleanData, 100) getwd() #write.csv(cleanData, “./data/01-modified-data/cleanRTAT.csv”) ``\nHere I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.\nDataset 2: Text data from reddit\n``{r}\n\nget a particular stock’s mentions in titles in each of the 7 subreddits\n\n\nfound subreddits by measuring overlap from https://subredditstats.com/subreddit-user-overlaps/superstonk\n\n\nwhich is calculated based on how likely users are to post on one or the other subs\n\n\ntargeting investment related subs with &gt;400k users\n\n\nloop through subreddits and get titles from past month\n#install.packages(“RedditExtractoR”, repos=‘https://cloud.r-project.org/’)\nstockSubreddits &lt;- c( “wallstreetbets”, “stocks”, “options”, “investing”, “stockamrket”, “superstonk”, “wallstreetbetsnew” )\nlinks &lt;- find_thread_urls(keywords = “QQQ”, subreddit = “stocks”, sort_by = “top”, period = “all”)\nlinks &lt;- tibble(links)\nlinks$ticker = “QQQ”\n\n\nget all unique stocks in the dataset of top retail-traded securities\ntempClean &lt;- read.csv(‘../data/01-modified-data/cleanRTAT.csv’)\ntopStocks &lt;- sort(table(tempClean$ticker), decreasing = TRUE)[1:100]\nuqTickers &lt;- rownames(topStocks)\nprint(uqTickers)\nfor(i in 1:length(uqTickers)) { print(i / nrow(uqTickers))\ntemplinks &lt;- find_thread_urls(keywords = uqTickers[i], subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\ntemplinks$ticker &lt;- uqTickers[i]\nlinks &lt;- rbind(links, templinks)\n}\nwrite.xlsx(links,“../data/01-modified-data/sampleRedditText.xlsx”)\n``\nHere I have gathered a list of 7 stock trading subreddits, which I created by searching for subreddits which mentioned individual stocks and investing, which also had more than ~400k subscribers. Then, I can use the find_thread_urls command to get a list of threads which mention particular keywords. For this initial gathering step, I simply gather posts with they keyword “SPY,” a ticker for the S&P 500, from r/stocks, a major investing subreddit.\nUpdated Method: using r/stocks only, get posts for the 100 most common tickers in the most-held datatable.\n\nStocktwits rankings ``{python} # stocksera # https://pypi.org/project/stocksera/ # use this for news ratings, stocktwits website mentions and ranking # api key: 2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1\n\nimport subprocess import sys\ndef install(name): subprocess.call([sys.executable, ‘-m’, ‘pip’, ‘install’, name])\n#install(“stocksera”)\nimport stocksera\nclient = stocksera.Client(api_key=“2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1”)\nstw = client.stocktwits(ticker = “QQQ”) print(stw)\nsbrs = client.wsb_mentions(days=500, ticker=“QQQ”) print(sbrs) ``\nDataset 4: (In Progress) Weekly Investor Sentiment from AAII `` {r} # AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results # read in as csv\n#getwd() #aaWeekly &lt;- read.csv(“./data/00-raw-data/sentiment_aaii.csv”) #head(aaWeekly)\n``"
  },
  {
    "objectID": "datacleaning/datacleaning.html",
    "href": "datacleaning/datacleaning.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Clean Record Data in R\n\n\nlibrary(reticulate)\n\n\nCleaning Reddit Text Corpus in Python\n\nMost of my work on preparing the text data is on the Naive Bayes tab. Below I take the reddit data and\n\nimport os\n\nprint\n\n&lt;built-in function print&gt;\n\nos.getcwd()\n\n'C:\\\\Users\\\\corwi\\\\Documents\\\\dsan-5000-project-corwindark\\\\dsan-website\\\\5000-website\\\\datacleaning'\n\n\nI ran the following code to clean my data:\nfrom sklearn.feature_extraction.text import CountVectorizer import numpy as np import pandas as pd import os import subprocess import sys from nltk.stem import PorterStemmer\n#subprocess.call([sys.executable, ‘-m’, ‘pip’, ‘install’, ‘nltk’])\nos.getcwd()\nredditTexts = pd.read_excel(“./data/01-modified-data/sampleRedditText.xlsx”, sheet_name=“Sheet1”)\nredditTexts.head() #positiveLingo &lt;- c(“call”, “calls”, “long”, “hold”, “buy”, “bull”, “bullish”, “bulls”, “support”, “strong”) #negativeLingo &lt;- c(“put”, “puts”, “short”, “shorters”, “short-sellers”, “sell”, “sellers”, “bear”, “bears”, “bearish”, “weakness”, “weak”)\nremoveNAtexts = redditTexts.dropna(subset = [‘text’])\nstemmer = PorterStemmer()\nremoveNAtexts[“text”] = removeNAtexts[“text”].apply(lambda x: x.split())\n#df[‘stemmed’] = df[‘unstemmed’].apply(lambda x: [stemmer.stem(y) for y in x])\nremoveNAtexts[“text”] = removeNAtexts[“text”].apply(lambda x: [stemmer.stem(y) for y in x])\nremoveNAtexts[“text”] = removeNAtexts[“text”].apply(lambda x: ’ ’.join(x))\nvectorizer = CountVectorizer() Xs = vectorizer.fit_transform(removeNAtexts[“text”])\n#print(type(Xs))\ncol_names=vectorizer.get_feature_names_out()\nsmallReddit = removeNAtexts[[“date_utc”, “title”, “ticker”, “comments”]]\nsmallReddit.index\nb = pd.DataFrame(Xs.toarray(), columns=col_names, index= smallReddit.index)\nredditCV = pd.concat([smallReddit, b], axis=1)\nredditCV.head()\nredditCV.to_csv(‘./data/01-modified-data/vectorizedReddit.csv’, index=False) #print(df)\n\n\n# VOCABULARY DICTIONARY\n#print(\"vocabulary = \",vectorizer.vocabulary_)   \n# STOP WORDS \n#print(\"stop words =\", vectorizer.stop_words)\n#print(\"col_names=\",col_names)\n#print(\"CORPUS WIDE WORD COUNTS:\",np.sum(Xs,axis=0))\n#print(\"WORDS PER DOCUMENT:\\n\",np.sum(Xs,axis=1))\n\n~51700 words without stemming ~49692 words after stemming\n\nNow, let’s"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "\nIntroduction\n",
    "section": "",
    "text": "Introduction\n\nMy feature data is based on 2,000+ Reddit posts, which pertained to stocks that were in the top-10 by daily retail trader activity on the day after they were posted. The dataset has 35,403 columns, which is because they represent a vectorized corpus of text data. The texts originally contained about 50,000 plus columns, but the number of columns was filtered down by first lowercasing and stemming the words (removing about 7,000) and then removing words which only appeared one time (removing a further ~9,000 columns).\nIn my analysis, I am hoping to use clustering to study whether there are interesting structures and relationships between the language used in these 2,000 posts. I am hoping that clusters might reflect certain kinds of posts (as there are several archetypes visible on the forum pages), or, further, that the clusters might reveal some previously unseen groupings in the data. These groupings could be findings in themselves, or they could be avenues for future analysis. In addition, I will compare the clusters returned by the various algorithms with my outcome variable (whether the given stock being written about increased or decreased in activity the next day), to understand if the clustering has inadvertently classified the outcome variable in some form.\n\nTheory\n\n\nK Means Clustering\n\nK means clustering uses repeated placement of centroids and assignment of nearby points to try and minimize the within-cluster variance (squared distance from the centroid to all points). To begin K means clustering, the algorithm would randomly initialize a given number of centroids, which we refer to as K centroids. Then, the algorithm would assign all points in the space to clusters based on their closest centroid. A real-life analogy to this would be drawing a property line midway between two houses, so that the line sat halfway in the distance between them, and then all points which fell on either side of the property line would be considered in that “houses” cluster. Where the houses are the centroids.\nNext, the algorithm would move the centroids, this time placing them not randomly, but rather at the point which minimizes the squared distance to all of the points in their cluster. Then, the points are reassigned to the nearest centroid, and the process repeats. This process can converge quickly, or fail to converge in some rare cases, but on average it performs reasonably well at generating clusters that minimize the within-cluster squared distances to the centroids. In the property line example, it would be similar to how many houses are on rectangular lots, because the house behind them is further away than the house next door. But after drawing the initial property rectangle, the house would be relocated to the “center” of their property, and then the property lines would be moved so as to remain at the halfway point between each house. This would guarantee that the yards were as close to their respective houses (in squared distance) as possible.\n\nHierarchical Clustering\n\nHierarchical clustering works very differently from K means clustering. In hierarchical clustering, clusters can exist on multiple layers. Rather than a piece of property which must fall squarely in one house’s lot, a datapoint in a hierarchical cluster is more like a person in a family tree, where they could have many layers of other people above them. In this type of clustering, the algorithm can either proceed from the bottom up (forming small groups and then tying them together) or the top down (starting with the broadest categories and then dividing them further). Pretend we are at a family BBQ, and we want to identify the family tree that maps the relationships of all individuals there. We could start with the oldest person, and then trace their descendants, which would be top down hierarchical clustering. But we could also start with the children, identify their parents, and then their parents’ parents, and so on. Both options can be used in practice.\nOne advantage of hierachical clustering is that it does not require the user to input a value for K. Based on the way that the model tries to build the tree, it will ultimately decide upon the number of appropriate clusters itself. Hierarchical clustering can be a very good fit for some kinds of datasets where the desired clusters could be subsets of eachother, like trying to identify species and families from datasets of fossils, or trying to identifying familial groupings in a reunion. Hierarchical clustering can be evaluated by a Silouhette plot, which measures how separated the clusters in the dataset are.\n\nDBSCAN\n\nDBSCAN is a popular clustering method that workds based on density. In other words, it assigns clusters to dense areas with sparse areas in between them. This fundamental principle allows DBSCAN to work on nonlinear clusters, because all that matters is that the algorithm can distinguish an area of low density separating the higher density clusters. DBSCAN clustering is able to find high and low density areas based on the distances between points and their closest neighbors. If you imagine a sparse area of a plot, a given point will have higher average distances to its nearest neighbors than in a tight cluster.\nDBSCAN is useful for many applications, because many types of data exhibit nonlinear behavior. In my application for this dataset, DBSCAN is appealing because there may not be linear patterns in the text data I am investigating, but density in the usage of certain words could still form meaningful clusters. In terms of hyperparameters, DBSCAN also does not need to know the number of clusters ahead of time, which makes it an even more flexible method for a variety of situations. DBSCAN can also be evaluated using Silhouette plots, as the ideal algorithm will have clearly distinct clusters with meaningful distances between them.\n\nMethods\n\nFirst, I will read in and clean my stock ticker and reddit post text data. This will be done via Jupyter notebook, and common packages such as pandas, matplotlib, and sklearn.\nSecond, I will merge the two datasets together, so that the reddit posts are associated with particular stocks on given days.\nThird, I will filter to only keep the days for which reddit posts were present (the 2000+ observations mentioned above).\nFourth, I remove any columns other than the text data in order to feed purely textual data into the clustering algorithms.\nFifth, I will declare the functions that will fit the various clustering methods and also iterate over possible hyperparameters.\nSixth, I will feed my data into the functions, and review the outputs for each possible hyperparameter value (by reviewing the elbow and silhouette plots).\nSeventh, I will run the final model and view the clustered data along various axes until I ultimately pick one that is a good representation of the clustering results. (You will only see the final selected plots below.)\n\nCode Implementation\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.cluster\nimport pandas as pd\n\n\n\n# read in data, merge using code from naive bayes\ndataIn = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\")\ndataIn.shape\n\ntextsIn = pd.read_csv(\"../data/01-modified-data/vectorizedReddit.csv\")\ntextsIn.shape\n\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\ntextsIn.shape\n\nfrom datetime import datetime, timedelta\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n\n# Join\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\nprocessed2 = processed.dropna(subset=['date_utc'])\nprint(processed2.shape)\n\n\nexclude = ['Bullish','Spread','SPYHighWk','SPYLowWK','SPYCloseWK','lagweek','date_utc','title','Bearish','Neutral', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', '' 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x']\n\n\nprocessed2['activityIncrease'] = processed2['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\nprocessed3 = processed2.drop(columns = exclude, axis = 1)\n\nprocessed3.head()\nX = processed3.drop('activityIncrease', axis=1)\ny = processed3['activityIncrease']\n\n(2018, 35427)\n\n\nC:\\Users\\corwi\\AppData\\Local\\Temp\\ipykernel_34132\\1696908631.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  processed2['activityIncrease'] = processed2['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\n\n\n# Import necessary libraries and functions if needed, like numpy.\n\n# Define a function to generate 2D clusters with normal distributions.\ndef generate_2D_normal_clusters(points_per_cluster=200, num_cluster=3, correlated=True):\n    # Initialize an empty dictionary to store the generated clusters.\n    sets = {}\n\n    # Loop to generate multiple clusters.\n    for cluster in range(0, num_cluster):\n        # Randomly select the number of points for this cluster within a range.\n\n        # Define parameters for a 2D normal distribution.\n        L = 10  # A scaling factor\n        # Randomly select the mean (center) of the distribution in X and Y.\n        ux = np.random.uniform(-L, L, size=1)[0]\n        uy = np.random.uniform(-L, L, size=1)[0]\n        # Randomly select the standard deviation in X and Y.\n        sx = np.random.uniform(0.1 * L, 0.25 * L, size=1)[0]\n        sy = np.random.uniform(0.1 * L, 0.25 * L, size=1)[0]\n        # Randomly select the correlation between X and Y (Pearson correlation).\n        rho = np.random.uniform(0.0, 0.99, size=1)[0]\n\n        # Create the mean vector and covariance matrix for the distribution.\n        u = np.array([ux, uy])\n        S = np.array([[sx**2.0, rho * sy * sx], [rho * sy * sx, sy**2.0]])\n\n        # If 'correlated' is False, set the off-diagonal elements of the covariance matrix to 0.\n        if correlated == False:\n            S[0, 1] = 0\n            S[1, 0] = 0\n\n        # Generate points from the multivariate normal distribution defined by 'u' and 'S'.\n        x1, x2 = np.random.multivariate_normal(u, S, points_per_cluster).T\n\n        # Create or concatenate the data and labels for the clusters.\n        if cluster == 0:\n            X = np.array([x1, x2]).T  # Data\n            y = cluster * np.ones(points_per_cluster)  # Labels\n        else:\n            X = np.concatenate((X, np.array([x1, x2]).T), axis=0)\n            y = np.concatenate((y, cluster * np.ones(points_per_cluster)), axis=0)\n\n    # Return the generated data (X) and labels (y).\n    return X, y\n\n# Generate the data for machine learning.\nnum_clusters_real = 1 + int(7 * np.random.uniform(0, 1))  # Determine the number of clusters.\nprint(num_clusters_real)\n\n\n# Print the shape of the generated data and labels.\nprint(X.shape, y.shape)\n\n# Make sure that the data is stored as a contiguous array for efficient memory access.\nX = np.ascontiguousarray(X)\n\n3\n(2018, 35402) (2018,)\n\n\n\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(processed2.loc[:,'deltaActivity'], processed2.loc[:,'date_utc'],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    plt.xlabel('Daily Change in Retail Investor Activity')\n    plt.ylabel('Date')\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n\nplot(X,y)\n\n\n\n\n\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH)\n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nopt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=10, i_plot=True)\n\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\nplot(X,opt_labels)\n\n\n\n\n\nopt_labels=maximize_silhouette(X,algo=\"ag\",nmax=4, i_plot=True)\nplot(X,opt_labels)\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\nopt_labels=maximize_silhouette(X,algo=\"dbscan\",nmax=10, i_plot=True)\nplot(X,opt_labels)\n\nOPTIMAL PARAMETER = 0.25\n\n\n\n\n\n\n\n\n\nHyper Parameter Tuning and Results\n\nAcross the models, hyper parameter tuning was accomplished with silhouette plots, where I looked for the maximum silhouette value for each model. Hierarchical clustering struggled both computationally and in its performance. While the ideal hyperparameter in the silhouette plot was 2 for the maximum value of N, the agglomerative clustering did not produce meaningful clusters no matter what axes I viewed them on. I believe this may be because the structure of the data prevented the hierarchical approach from working well, given that I have many points which could not be related to any other points given that they might have had totally unique language and thus total uniqueness in the dataset. While the other methods simply grouped most points together and identified ones which stood out from the pack, perhaps hierarchical clustering could not accomplish the same feat and struggled to accomodate the many virtually patternless points.\nDBSAN did not change in its performance regardless of the hyperparameter selected. This is expected as the hyperparameter is not the same as the other two models, because DBSAN does not require us to input the number of clusters. However, DBSAN did reveal interesting features in the data. Plotting the clusters vs. the change in retail activity on the X axis and time on the Y axis (chart above) showed that DBSAN had identified small clusters that were very closely related in time. I believe the reason for this is that DBSAN creates clusters based on density, and given that our feature space is the language used in reddit posts, this means that DBSAN has identified clusters of similar posts throughout the data. These posts actually make intuitive sense, because reddits sometimes work like echo chambers, where particular stocks such as GAMESTOP or AMC become very popular to discuss and trade for a small period of time. This would lead posts from this period to become more similar, as they are discussing the same point, which then makes the clustering algorithm identify them together. When viewed over time, we then see that the clusters have selected small points in time, due to the way the data was generated. DBSAN revealed this pattern in the data, which I found quite interesting.\nThe silhouette plot for K-means was very clear: The value decreased for each additional mean that was added to the model. As such, the smallest value, with only 2 clusters, appeared to be the ideal choice for the data. But upon reviewing the clusters that were returned along many axes, there did not appear to be a meaningful interpretation of the data they represented, even with the ideal K value. This was mainly because one cluster contained almost all of the data in the dataset, which meant that the secondary cluster did not yield many interesting results, and was also largely impossible to pick out in any of the charts. I believe this shows that the data did not lend itself to linear separability, as DBSAN performed much better than Kmeans at identifying interesting features.\n\nConclusions\n\nOut of the three clustering methods, DBSCAN provided the most interesting results for my study. In particular, the DBSCAN plot showed stratified clusters of points within the timeline of the data. I believe these could correspond to events or moments of excitement about particular stocks, which led to repeated posts using similar language, which in turn made those posts seem more similar to the clustering algorithm. Because DBSCAN focuses on areas of relative density and sparseness, it was able to identify these small moments within the larger dataset. In contrast, K means might have been confused by nonlinearity, or the relative rarity of these datapoints within the corpus.\n\nReferences\n\nNo outside works used that need to be cited. Lab code was used for the clustering functions."
  },
  {
    "objectID": "aboutme/aboutme.html",
    "href": "aboutme/aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Coming from an undergraduate background in social science, I am interested in data science because it is useful for such a wide array of problems, and I feel working in data science let’s you focus on interesting subjects throughout your career. I am a Seattle native but came to DC to attend American University undergrad, where I majored in International Studies with a minor in Data Science. During undergrad, I focused on international relations research, but through a quantitative lens, writing papers on political bargaining in Somalia and modeling protest violence in the United States. After graduating during COVID, I worked for a small international development consulting firm, providing data science adjacent skills on a myriad of different development projects for USAID, Australia’s DFAT, the World Bank, and others. I really enjoyed my time working in that field, but I wanted to learn more advanced ML methods, and so I am excited to be at Georgetown with the opportunity to dig into the concepts that interest me! After working on blended finance programs for several years\nResearch interests: - Metalearning and ensembling for time series prediction - Bet sizing and portfolio management - Tying diverse datasets together\nOutside school, I live in Arlington. Having lived around the DMV for the past 5 years off and on, I have found there are many exciting activities in the area. In particular, I enjoy getting outdoors however I can, such as hiking or just walking around different neighborhoods. I also love to cook, travel to other countries or just new US cities, develop video games, and try out as many coffee shops as possible! (Although the East Coast really hasn’t approached Seattle’s standards yet in my, totally biased, opinion.)"
  },
  {
    "objectID": "arm/arm.html",
    "href": "arm/arm.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "conclusions/conclusions.html",
    "href": "conclusions/conclusions.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "dataexploration/dataexploration.html",
    "href": "dataexploration/dataexploration.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "EDA Overview\n\nAt the broadest level, there is a temporal component to the research question that will shape how we conduct the analysis. Namely, the outcome variable is retail trading activity in stocks, as measured by daily holding data on the top 10 stocks sourced from NASDAQ’s API. As such, we are interested in data from the other datasets (reddit text information, investor sentiment survey, stocktwits ranking) as of one day before. I will merge the datasets in R, and then use R (ggplot) to create visuals and carry out EDA.\n\nJoining the Data\n\n(Will be moved to data cleaning eventually)\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(Quandl)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n\nLet’s join the data together into one dataset, with properly lagged predictor variables.\n\nredditIn &lt;- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn &lt;- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn &lt;- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] &lt;- \"date\"\n\nsentimentIn &lt;- sentimentIn |&gt;\n    mutate(date = mdy(date)) |&gt;\n    mutate(week = week(date)) |&gt;\n    mutate(lagweek = week + 1) |&gt;\n    mutate(weekyear = paste(year(date), lagweek )) |&gt;\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |&gt;\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |&gt;\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |&gt;\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100`.\nCaused by warning:\n! NAs introduced by coercion\n\nrtatIn &lt;- rtatIn %&gt;%\n    mutate(date = ymd(date)) %&gt;%\n    mutate(week = week(date)) %&gt;%\n    mutate(weekyear = paste(year(date), week))\n\nhead(rtatIn)\n\n     X       date ticker activity sentiment deltaActivity deltaSentiment\n1 9551 2020-01-02   TSLA   0.0226         0       -0.0014              0\n2 9552 2020-01-02    SPY   0.0147         3       -0.0341             -7\n3 9553 2020-01-02   ROKU   0.0119         0        0.0000              0\n4 9554 2020-01-02    QQQ   0.0093         6       -0.0111              1\n5 9555 2020-01-02   MSFT   0.0116         2       -0.0181              1\n6 9556 2020-01-02     FB   0.0093        -2       -0.0078             -2\n  newEntry week weekyear\n1    FALSE    1   2020 1\n2    FALSE    1   2020 1\n3     TRUE    1   2020 1\n4    FALSE    1   2020 1\n5    FALSE    1   2020 1\n6    FALSE    1   2020 1\n\njoinedSentiment &lt;- left_join(rtatIn, sentimentIn, by = \"weekyear\")\n\nhead(joinedSentiment)\n\n     X     date.x ticker activity sentiment deltaActivity deltaSentiment\n1 9551 2020-01-02   TSLA   0.0226         0       -0.0014              0\n2 9552 2020-01-02    SPY   0.0147         3       -0.0341             -7\n3 9553 2020-01-02   ROKU   0.0119         0        0.0000              0\n4 9554 2020-01-02    QQQ   0.0093         6       -0.0111              1\n5 9555 2020-01-02   MSFT   0.0116         2       -0.0181              1\n6 9556 2020-01-02     FB   0.0093        -2       -0.0078             -2\n  newEntry week.x weekyear date.y Bullish Neutral Bearish Total X8.week Spread\n1    FALSE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n2    FALSE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n3     TRUE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n4    FALSE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n5    FALSE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n6    FALSE      1   2020 1   &lt;NA&gt;      NA      NA      NA  &lt;NA&gt;    &lt;NA&gt;     NA\n  SPYHighWk SPYLowWK SPYCloseWK week.y lagweek\n1      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n2      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n3      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n4      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n5      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n6      &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;     NA      NA\n\n\n\nNumeric Summaries\n\n\nsummary(joinedSentiment$activity)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00760 0.01720 0.02460 0.03103 0.03590 0.26150 \n\nsummary(joinedSentiment$sentiment)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.0000  -1.0000   1.0000   0.9117   3.0000  19.0000 \n\nsummary(joinedSentiment$deltaActivity)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.194000 -0.003700  0.002100  0.003997  0.011600  0.232000 \n\nsummary(joinedSentiment$deltaSentiment)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.000  -3.000   0.000  -1.259   1.000  20.000 \n\nsummary(joinedSentiment$Bullish)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1584  0.2483  0.3188  0.3262  0.3956  0.5691     290 \n\nsummary(joinedSentiment$Bearish)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1976  0.2751  0.3805  0.3732  0.4490  0.6087     290 \n\nsummary(joinedSentiment$Neutral)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1450  0.2715  0.3043  0.3005  0.3345  0.4091     290 \n\nsummary(joinedSentiment$Spread)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-0.43100 -0.19200 -0.07500 -0.04695  0.11700  0.36500      290 \n\n\nBarplots for discrete variables:\n\nggplot(joinedSentiment, aes(x = newEntry)) + geom_bar() + labs(x= \"Current Stock is in Top 10\", y = \"Count\", title = \"Distribution of New Stocks Amongst Top-10 Stocks by Retail Activity\") + theme_grey(base_size = 20)\n\n\n\nsort(table(joinedSentiment$ticker), decreasing = TRUE)[1:10]\n\n\nAAPL TSLA  SPY  AMD  QQQ TQQQ NVDA AMZN SQQQ MSFT \n 956  956  926  876  760  694  665  531  408  389 \n\n\nIn addition to the porportion of new stocks in the top 10, the table shows up the top 10 stocks by number of days spent in the top-10 ranking of retail investor actvity. We can see that the distribution has a heavy right tail, as even amongst the top 10 stocks there is a large divergence, with the top stock (AAPL) having more than two times as many days in the top 10 as the 10th place stock (MSFT).\n\nOutcome Variable Exploration\n\nFirst, lets explore the outcome variable table, of the top 10 retail-investor held stocks each day. Each stock has an ‘activity’ number which measures the percent of traded shares held by retail investors. Let’s see how those scores are distributed:\n\nggplot(joinedSentiment, aes(x = activity)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\") + theme_grey(base_size = 20)\n\n\n\n\nActivity scores seem to be skewed to the right, with a thin tail. I wonder which stocks compose the highest activity values. Let’s look at which of these scores were from stocks that had just appeared in the top 10 that day:\n\nggplot(joinedSentiment, aes(x = activity, fill = newEntry)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\nFrom this chart, we can see that most of the high-activity scores are from stocks which previously entered the top 10. This makes sense, as we would expects stock to increase in activity over multiple days, before reaching the top of the list. Similar to songs on top music charts.\nMy next question is how the outcome variable has changed over time. Let’s look at attention scores over time:\n\njoinedSentimentAvgs &lt;- joinedSentiment %&gt;%\n    group_by(date.x) %&gt;%\n    summarize(dailyactivitytotal = sum(activity), dailyactivityavg = mean(activity),\n    dailynewtickers = sum(as.numeric(newEntry))) \n\nggplot(joinedSentimentAvgs, aes(x = date.x, y = dailyactivitytotal)) + geom_line() + labs(x= \"Date\", y = \"Total Retail Investor Activity\", title = \"Total of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\nThis chart is interesting because it shows that our outcome variable has a clear upward trend over time. Retail investors appear to be a greater proportion of the total activity in the stock market today than they were in 2020.\nOne last visual I wanted to create to evaluate the outcome variables was the number of new top-10 stocks that entered the list each week\n\nlibrary(ggbeeswarm)\n\nggplot(joinedSentimentAvgs, aes(x = as.character(year(date.x)), y = dailynewtickers)) + geom_boxplot() + labs(x= \"Year\", y = \"New Stocks in the Top 10\", title = \"Number of New Stocks in the Top 10 of Retail Activity by Year\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\nHere we can see there is not a huge difference in the distributions of new stocks between the years, except for 2021, when there was a lot more days with at least 1 new stock in the top 10. But far and away, at least 50% of days have no new stocks in the top 10 of retail trader activity.\n\nBivariate Analysis\n\nNow, lets look at some of the predictor variables in relation to the outcome variables.\n\nlibrary(\"corrplot\")\n\ncorrplot 0.92 loaded\n\nnoNAsentiment &lt;- na.omit(joinedSentiment[,c(4,5,6,7,12,13,14,17)])\n\nsave &lt;- cor(noNAsentiment)\ncorrplot(save)\n\n\n\nsave \n\n                  activity    sentiment deltaActivity deltaSentiment\nactivity        1.00000000 -0.068425337   0.620189979     0.06955678\nsentiment      -0.06842534  1.000000000   0.009215771     0.51408474\ndeltaActivity   0.62018998  0.009215771   1.000000000    -0.04522242\ndeltaSentiment  0.06955678  0.514084741  -0.045222415     1.00000000\nBullish        -0.07234842  0.111433821  -0.095796815     0.10398764\nNeutral         0.04440169  0.019445339   0.038646679    -0.01661078\nBearish         0.04285201 -0.111207408   0.067113719    -0.08588212\nSpread         -0.05902646  0.115454510  -0.083795941     0.09801129\n                   Bullish     Neutral     Bearish      Spread\nactivity       -0.07234842  0.04440169  0.04285201 -0.05902646\nsentiment       0.11143382  0.01944534 -0.11120741  0.11545451\ndeltaActivity  -0.09579681  0.03864668  0.06711372 -0.08379594\ndeltaSentiment  0.10398764 -0.01661078 -0.08588212  0.09801129\nBullish         1.00000000 -0.09615506 -0.85871354  0.96041621\nNeutral        -0.09615506  1.00000000 -0.42751147  0.18492522\nBearish        -0.85871354 -0.42751147  1.00000000 -0.96747469\nSpread          0.96041621  0.18492522 -0.96747469  1.00000000\n\n\nThe correlations between weekly investor sentiment polling and investor activity in the following week are displayed in the plot. While many crosstabs are highly correlated, these are only because the variables have been calculated together (i.e. the sentiment polling includes % with positive sentiment and % with negative sentiment, which are perfectly correlated since they are two sides of the same measure). The highest correlation value recorded outside these collinear predictors was ~0.115 for the spread in bullish and bearish opinion vs. sentiment towards top10 stocks.\nThis correlation makes sense, because both measures are capturing the same underlying phenomena: investor sentiment. If retail investor sentiment is bearish about the market in general, then it would likely also be negative about particular stocks.\n\nOutliers and Segmentation\n\nWe already saw in the overtime chart that perhaps 2021 was a different type of year than the other 3, as it had a high percentage of days with new stocks in the top 10.\nLets look at the key outcome variables and see if any values violate Tukey’s rule\n\nboxplot(joinedSentiment$activity)\n\ncutoff &lt;- mean(joinedSentiment$activity) + (1.5 * IQR(joinedSentiment$activity))\n\nabline(h = cutoff, col = \"Red\")\n\n\n\n\nWith the red line above denoting the 1.5*IQR + mean threshold, we can see that many values lie above this line, but they do not appear to be separated from the main distribution. Rather, it looks like the main distribution has a fat right tail. Perhaps it would be well approximated by a Student’s distribution.\n\nHypothesis Testing"
  },
  {
    "objectID": "dectrees/dectrees.html",
    "href": "dectrees/dectrees.html",
    "title": "dectrees",
    "section": "",
    "text": "Methods\n\n\nClass Distribution\n\n\n# Code to compute the class distribution\n\n\nBaseline Model / Random Classifier\n\n\n\n# Random classifier code\n\n\nModel Implementation\n\n\nHyperparameter Tuning\n\n\nFinal Results\n\n\nConclusions"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "Introduction/introduction.html",
    "href": "Introduction/introduction.html",
    "title": "Introduction - Retail Investor Sentiment and Market Outcomes",
    "section": "",
    "text": "Financial markets have become substantially more available to a wider class of investors in recent years. Brokers have developed mobile apps that can run on anyone’s personal device, trading fees have been reduced to zero in many cases, and new fintech companies have emerged that seek to make investing more available to everyday people, such as Robinhood, Acorns, Wealthfront and others. These developments came to a head in 2020, when so-called retail investing (investing by individuals who buy stocks directly and have smaller balances) exploded during the pandemic. (“New Research: Global Pandemic Brings Surge of New and Experienced Retail Investors Into the Stock Market  FINRA.org” n.d.)\nWhile broader access to financial markets could distribute the gains from investing more fairly, it also increases the risk that everyday investors will lose money through bad investments. Without the backing of institutions that is often found in retirement accounts, nothing protects indvididual investors from taking on extreme risk or buying into companies without sufficient information. Further, retail investors have a spillover effect on markets as a whole, because the sums of money involved can affect companies’ valuations and financial conditions overall. As such, understanding retail investors’ impact has become crucial to staying informed on the forces guiding market dynamics in today’s world.\nUnsurprisingly, researchers have begun to study this phenomena from several angles. Academics have found that including sentiment from retail investor forums helps their models predict prices more accurately (Jing, Wu, and Wang 2021), relying on social media data sources that have become increasingly important in financial analysis (Khan et al. 2022). Khan et. al also found that the impact of social media hype and retail investors varied in importance based on the particular stock market exchange or security being studied. Dividing up the market into smaller segments is certainly a promising area of analysis, based on this work. Overall, data science methods are well suited to address this question, and some areas such as deep learning have become embedded across topics in finance and banking (Huang, Chai, and Cho 2020).\nIn this project, I am seeking to investigate the following research questions:\n\nTo what extent do common measures of retail investor interest correspond to movement in stock prices?\nIs retail investor interest positively or negatively correlated with short and long term outcomes?\nWhich social media sites show promise for measuring retail investor interest?\nDoes the impact of retail investor interest vary based on the financial exchange in question?\nDoes the impact of retail investor interest vary based on the particular security in question?\nHas the importance of retail investor interest diminished since 2020?\nTo what extent is it possible to predict how long retail investor interest will last in a particular stock?\nWhich types of stocks are most likely to attract retail investor interest?\nDo media datasets include signs that news coverage spawns retail investor interest?\nDo media datasets include signs that retail investor interest spawns news coverage?\n\nTo address these questions, I will look at common measures of retail investor interest, including forum posts, social media messages and sentiment, and trading patterns for particular financial instruments.\n\n\n\n\nReferences\n\nHuang, Jian, Junyi Chai, and Stella Cho. 2020. “Deep Learning in Finance and Banking: A Literature Review and Classification.” Frontiers of Business Research in China 14 (1): 13. https://doi.org/10.1186/s11782-020-00082-6.\n\n\nJing, Nan, Zhao Wu, and Hefei Wang. 2021. “A Hybrid Model Integrating Deep Learning with Investor Sentiment Analysis for Stock Price Prediction.” Expert Systems with Applications 178 (September): 115019. https://doi.org/10.1016/j.eswa.2021.115019.\n\n\nKhan, Wasiat, Mustansar Ali Ghazanfar, Muhammad Awais Azam, Amin Karami, Khaled H. Alyoubi, and Ahmed S. Alfakeeh. 2022. “Stock Market Prediction Using Machine Learning Classifiers and Social Media, News.” Journal of Ambient Intelligence and Humanized Computing 13 (7): 3433–56. https://doi.org/10.1007/s12652-020-01839-w.\n\n\n“New Research: Global Pandemic Brings Surge of New and Experienced Retail Investors Into the Stock Market  FINRA.org.” n.d. FINRA. Accessed September 21, 2023. https://www.finra.org/media-center/newsreleases/2021/new-research-global-pandemic-brings-surge-new-and-experienced-retail."
  }
]