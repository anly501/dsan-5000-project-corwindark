[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "dectrees/dectrees.html",
    "href": "dectrees/dectrees.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "dataexploration/dataexploration.html",
    "href": "dataexploration/dataexploration.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "arm/arm.html",
    "href": "arm/arm.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "aboutme/aboutme.html",
    "href": "aboutme/aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Coming from an undergraduate background in social science, I am interested in data science because it is useful for such a wide array of problems, and I feel working in data science let’s you focus on interesting subjects throughout your career. I am a Seattle native but came to DC to attend American University undergrad, where I majored in International Studies with a minor in Data Science. During undergrad, I focused on international relations research, but through a quantitative lens, writing papers on political bargaining in Somalia and modeling protest violence in the United States. After graduating during COVID, I worked for a small international development consulting firm, providing data science adjacent skills on a myriad of different development projects for USAID, Australia’s DFAT, the World Bank, and others. I really enjoyed my time working in that field, but I wanted to learn more advanced ML methods, and so I am excited to be at Georgetown with the opportunity to dig into the concepts that interest me! After working on blended finance programs for several years\nResearch interests: - Metalearning and ensembling for time series prediction - Bet sizing and portfolio management - Tying diverse datasets together\nOutside school, I live in Arlington. Having lived around the DMV for the past 5 years off and on, I have found there are many exciting activities in the area. In particular, I enjoy getting outdoors however I can, such as hiking or just walking around different neighborhoods. I also love to cook, travel to other countries or just new US cities, develop video games, and try out as many coffee shops as possible! (Although the East Coast really hasn’t approached Seattle’s standards yet in my, totally biased, opinion.)"
  },
  {
    "objectID": "conclusions/conclusions.html",
    "href": "conclusions/conclusions.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "datacleaning/datacleaning.html",
    "href": "datacleaning/datacleaning.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "datagathering/datagathering.html",
    "href": "datagathering/datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "For this project, I need to gather a range of data on retail investor sentiment. Reviewing the literature, it does not appear that a consesus has been reached on which mediums are most useful for measuring retail investor trades, let alone their sentiment. After an extensive review, I decided to try and construct a set of trading signals from the following: 1. Daily NASDAQ data on the top-10 retail investor held companies (API through R Quandl Package) 2. Text data from popular stock-trading subreddits (Reddit API through R) 3. Stocktwits mentions of popular companies through (Stocksera API in Python) 4. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV) 5. Text data from major financial new companies\nStarting with dataset 1: Nasdaq retail investor holdings\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.2.1     v dplyr   1.1.2\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(Quandl)\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\n\n\nAttaching package: 'xts'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\nQuandl.api_key(\"psq4nx69HDimf2kQcxZJ\")\ndata &lt;- Quandl.datatable(\"NDAQ/RTAT10\", paginate = TRUE)\nhead(data)\n\n        date ticker activity sentiment\n1 2023-10-02   TSLA   0.1352        -1\n2 2023-10-02   TQQQ   0.0215        -4\n3 2023-10-02   SQQQ   0.0147        -1\n4 2023-10-02    SPY   0.0359        10\n5 2023-10-02    QQQ   0.0284         3\n6 2023-10-02   NVDA   0.0478         2\n\n\nHere I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.\nDataset 2: Text data from reddit\n\n# get a particular stock's mentions in titles in each of the 7 subreddits\n# found subreddits by measuring overlap from https://subredditstats.com/subreddit-user-overlaps/superstonk\n# which is calculated based on how likely users are to post on one or the other subs\n# targeting investment related subs with &gt;400k users\n# loop through subreddits and get titles from past month\n\n\n#install.packages(\"RedditExtractoR\", repos='https://cloud.r-project.org/')\nlibrary(RedditExtractoR)\n#install.packages(\"reticulate\")\nlibrary(reticulate)\n\nstockSubreddits &lt;- c(\n    \"wallstreetbets\",\n    \"stocks\",\n    \"options\",\n    \"investing\",\n    \"stockamrket\",\n    \"superstonk\",\n    \"wallstreetbetsnew\"\n)\n\npositiveLingo &lt;- c(\"call\", \"calls\", \"long\", \"hold\", \"buy\", \"bull\", \"bullish\", \"bulls\", \"support\", \"strong\")\nnegativeLingo &lt;- c(\"put\", \"puts\", \"short\", \"shorters\", \"short-sellers\", \"sell\", \"sellers\", \"bear\", \"bears\", \"bearish\", \"weakness\", \"weak\")\n\n\nlinks &lt;- find_thread_urls(keywords = \"SPY\", subreddit = \"stocks\", sort_by = \"hot\", period = \"day\")\n\nparsing URLs on page 1...\n\nhead(links)\n\n                                                                    date_utc\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023 2023-10-03\n                                                                   timestamp\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023 1696325412\n                                                                                                                              title\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023 r/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023 This is the daily discussion, so anything stocks related is fine, but the theme for today is on technical analysis (TA), but if TA is not your thing then just ignore the theme and/or [post your arguments against TA here](https://www.reddit.com/r/stocks/search?q=author%3Aautomoderator+%22r%2Fstocks+semiannual+arguments+against+TA%22&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=new&amp;t=all) and not in the current post.\\n\\nSome helpful day to day links, including news:\\n\\n* [Finviz](https://finviz.com/quote.ashx?t=spy) for charts, fundamentals, and aggregated news on individual stocks\\n* [Bloomberg market news](https://www.bloomberg.com/markets)\\n* StreetInsider news:\\n  * [Market Check](https://www.streetinsider.com/Market+Check) - Possibly why the market is doing what it's doing including sudden spikes/dips\\n  * [Reuters aggregated](https://www.streetinsider.com/Reuters) - Global news\\n\\n-----\\n\\n**Technical analysis (TA)** uses historical price movements, real time data, indicators based on math and/or statistics, and charts; all of which help measure the trajectory of a security.  TA can also be used to interpret the actions of other market participants and predict their actions.\\n        \\nThe main benefit to TA is that everything shows up in the price (commonly known as **\"priced in\"**):  All news, investor sentiment, and changes to fundamentals are reflected in a security's price.\\n\\nTA can be useful on any timeframe, both short and long term.\\n\\nIntro to technical analysis by [Stockcharts chartschool](https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:introduction_to_technical_indicators_and_oscillators#benefits_and_drawbacks_of_leading_indicators) and their [article on candlesticks](https://stockcharts.com/school/doku.php?id=chart_school:chart_analysis:introduction_to_candlesticks)\\n\\nIf you have questions, please see the following word cloud and click through for the wiki:\\n\\n[Indicator - Trade Signals - Lagging Indicator - Leading Indicator - Oversold - Overbought - Divergence - Whipsaw - Resistance - Support - Breakout/Breakdown - Alerts - Trend line - Market Participants - Moving average - RSI - VWAP - MACD - ATR - Bollinger Bands - Ichimoku clouds - Methods - Trend Following - Fading - Channels - Patterns - Pivots](https://www.reddit.com/r/stocks/wiki/ta-themed-post)\\n\\nSee our past [daily discussions here.](https://www.reddit.com/r/stocks/search?q=author%3Aautomoderator+%22r%2Fstocks+daily+discussion%22&amp;restrict_sr=on&amp;sort=new&amp;t=all)  Also links for:  [Technicals](https://www.reddit.com/r/stocks/search?q=author%3Aautomoderator+title%3Atechnicals&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=new&amp;t=all) Tuesday, [Options Trading](https://www.reddit.com/r/stocks/search?q=author%3Aautomoderator+title%3Aoptions&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=new&amp;t=all) Thursday, and [Fundamentals](https://www.reddit.com/r/stocks/search?q=author%3Aautomoderator+title%3Afundamentals&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=new&amp;t=all) Friday.\n                                                                  subreddit\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023    stocks\n                                                                  comments\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023      122\n                                                                                                                                                                url\nr/Stocks Daily Discussion &amp; Technicals Tuesday - Oct 03, 2023 https://www.reddit.com/r/stocks/comments/16yn71b/rstocks_daily_discussion_technicals_tuesday_oct/\n\n#threadContent &lt;- get_thread_content(links)\n#threadContent[1]\n\nHere I have gathered a list of 7 stock trading subreddits, which I created by searching for subreddits which mentioned individual stocks and investing, which also had more than ~400k subscribers. Then, I can use the find_thread_urls command to get a list of threads which mention particular keywords. For this initial gathering step, I simply gather posts with they keyword “SPY,” a ticker for the S&P 500, from r/stocks, a major investing subreddit.\n\nStocktwits rankings\n\n\n# stocksera\n# https://pypi.org/project/stocksera/\n# use this for news ratings, stocktwits website mentions and ranking\n# api key: 2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1\n\nimport subprocess\nimport sys\n\ndef install(name):\n    subprocess.call([sys.executable, '-m', 'pip', 'install', name])\n\n#install(\"stocksera\")\n\nimport stocksera\n\nclient = stocksera.Client(api_key=\"2qs0tTwf.8sC7wUQ1Pf5zroD3IHx3XVTLwpHUifZ1\")\ndata = client.stocktwits(ticker=\"AAPL\")\nprint(data[0])\n\n{'rank': 22, 'watchlist': 760696, 'date_updated': '2021-11-29 03:00'}\n\n\nDataset 4: (In Progress) Weekly Investor Sentiment from AAII\n\n# AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results\n# read in as csv \n\n#getwd()\n#aaWeekly &lt;- read.csv(\"./data/00-raw-data/sentiment_aaii.csv\")\n#head(aaWeekly)\n\nDataset 5: (In Progress) Investing Media Sentiment\n\n# text data from: fortune, FT, Bloomberg, BBC, economist, wsj\n# https://github.com/je-suis-tm/web-scraping#available-scrapers\n\n\n# google trends search data\n\n\n\n#"
  },
  {
    "objectID": "dimreduction/dimreduction.html",
    "href": "dimreduction/dimreduction.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "print(\"a\")"
  },
  {
    "objectID": "Introduction/introduction.html",
    "href": "Introduction/introduction.html",
    "title": "Introduction - Retail Investor Sentiment and Market Outcomes",
    "section": "",
    "text": "Financial markets have become substantially more available to a wider class of investors in recent years. Brokers have developed mobile apps that can run on anyone’s personal device, trading fees have been reduced to zero in many cases, and new fintech companies have emerged that seek to make investing more available to everyday people, such as Robinhood, Acorns, Wealthfront and others. These developments came to a head in 2020, when so-called retail investing (investing by individuals who buy stocks directly and have smaller balances) exploded during the pandemic. (“New Research: Global Pandemic Brings Surge of New and Experienced Retail Investors Into the Stock Market  FINRA.org” n.d.)\nWhile broader access to financial markets could distribute the gains from investing more fairly, it also increases the risk that everyday investors will lose money through bad investments. Without the backing of institutions that is often found in retirement accounts, nothing protects indvididual investors from taking on extreme risk or buying into companies without sufficient information. Further, retail investors have a spillover effect on markets as a whole, because the sums of money involved can affect companies’ valuations and financial conditions overall. As such, understanding retail investors’ impact has become crucial to staying informed on the forces guiding market dynamics in today’s world.\nUnsurprisingly, researchers have begun to study this phenomena from several angles. Academics have found that including sentiment from retail investor forums helps their models predict prices more accurately (Jing, Wu, and Wang 2021), relying on social media data sources that have become increasingly important in financial analysis (Khan et al. 2022). Khan et. al also found that the impact of social media hype and retail investors varied in importance based on the particular stock market exchange or security being studied. Dividing up the market into smaller segments is certainly a promising area of analysis, based on this work. Overall, data science methods are well suited to address this question, and some areas such as deep learning have become embedded across topics in finance and banking (Huang, Chai, and Cho 2020).\nIn this project, I am seeking to investigate the following research questions:\n\nTo what extent do common measures of retail investor interest correspond to movement in stock prices?\nIs retail investor interest positively or negatively correlated with short and long term outcomes?\nWhich social media sites show promise for measuring retail investor interest?\nDoes the impact of retail investor interest vary based on the financial exchange in question?\nDoes the impact of retail investor interest vary based on the particular security in question?\nHas the importance of retail investor interest diminished since 2020?\nTo what extent is it possible to predict how long retail investor interest will last in a particular stock?\nWhich types of stocks are most likely to attract retail investor interest?\nDo media datasets include signs that news coverage spawns retail investor interest?\nDo media datasets include signs that retail investor interest spawns news coverage?\n\nTo address these questions, I will look at common measures of retail investor interest, including forum posts, social media messages and sentiment, and trading patterns for particular financial instruments.\n\n\n\n\nReferences\n\nHuang, Jian, Junyi Chai, and Stella Cho. 2020. “Deep Learning in Finance and Banking: A Literature Review and Classification.” Frontiers of Business Research in China 14 (1): 13. https://doi.org/10.1186/s11782-020-00082-6.\n\n\nJing, Nan, Zhao Wu, and Hefei Wang. 2021. “A Hybrid Model Integrating Deep Learning with Investor Sentiment Analysis for Stock Price Prediction.” Expert Systems with Applications 178 (September): 115019. https://doi.org/10.1016/j.eswa.2021.115019.\n\n\nKhan, Wasiat, Mustansar Ali Ghazanfar, Muhammad Awais Azam, Amin Karami, Khaled H. Alyoubi, and Ahmed S. Alfakeeh. 2022. “Stock Market Prediction Using Machine Learning Classifiers and Social Media, News.” Journal of Ambient Intelligence and Humanized Computing 13 (7): 3433–56. https://doi.org/10.1007/s12652-020-01839-w.\n\n\n“New Research: Global Pandemic Brings Surge of New and Experienced Retail Investors Into the Stock Market  FINRA.org.” n.d. FINRA. Accessed September 21, 2023. https://www.finra.org/media-center/newsreleases/2021/new-research-global-pandemic-brings-surge-new-and-experienced-retail."
  }
]