[
  {
    "objectID": "naivebayes/naivebayes.html",
    "href": "naivebayes/naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "A naive bayes classifier assigns labels to observations using a process based on Bayes’ theorem. The important element of the theorem is updating the probability of an event based on the known prior events. For instance, if all vehicles with 2 wheels are motorcycles, then we can update the probability that an unknown vehicle is a motorcycle given whether we know it has two wheels. In the case of a classification algorithm, the prosterior probabilities in question are the likelihoods that a given observation belongs to each class. Using the known prior events to update the probability of each class, the algorithm would take whichever class label had the highest likelihood and assign that to the data point in question.\nA naive Bayes classifier has several variants based on the types of features being used to predict class labels. - Gaussian: If the predictors are continuous, then we likely will not have exact probabilities for the given input point (as no other point may have the exact same value for all input features). As such, a different approach is required, where it is assumed that the continuous attributes are normally distributed. Then, for a given point that needs to be classified, we can express the probability of it belonging to a certain class as the likelihood that the point came from each class’s distribution. This is relatively easy, as we can simply find the likelihood that the point was generated by each class distribution, and assign the most likely one as the label. - Multinomial: This method is preferred when the predictors are generated by a multinomial distribution, meaning they are discrete, with more than 2 levels. Labels would be assigned to new observations based on the conditional probability of observing the point’s attributes given that it came from each class (which can be calculated again using Baye’s theorem since the attributes are discirete). - Bernoulli Naive Bayes: The Bernoulli method is preferred when the predictors are binary, meaning they are discrete but only have two values they can take. This is calculated similar to the multinomial variant, but is even more straightforwards as there are only two classes.\n\n\nObjective of Naive Bayes for my Project: With my Naive Bayes classifier, I hope to identify stocks in the top 10 by retail investor activity that have increases in retail investor activity on a given day. To do this I will convert a continuous variable (the daily change in retail investor activity) into two classes based on whether a given stock increased or decreased in activity day-over-day.\nTo predict this outcome variable for each stock in the top 10, I will use two different sets of predictors in the Naive Bayes classifier. First, I will use my tabular data: - Individual investor sentiment from the proceding week, including the percent of respondents who were bearish, bullish, and neutral on the market. - Tabular data from reddit posts including the number of comments.\nSecond, I will use my textual data to predict the same increase/decrease outcome variable: - A corpus of 7.5k reddit posts mentioning top-10 tickers, which contain about 50,000 unique words.\n\n\n\nWe are splitting the data into training, validation, and test sets to ensure we optimally fit the model and avoid over or under fitting. In particular, we can use the validation set to tune our model and improve its generalizability, and then use the test set to evaluate the performance of our model. I will do a 70-15-15 split between the train, evaluation, and test data, and this is done simply by randomly splitting the dataset.\nFor the text data: I will prepare the corpus in a few steps. 1. I have already used a stemmer to reduce the number of unique words by about 7,000. After this reduction we still have 50,000 words. 2. I will filter out words with &lt;5 total uses in the corpus. 3. Then, I will join my binary outcome variable with the reddit text data according to the date of each post. The outcomes will be lagged by 1 day, as we want to ensure that the reddit mentions preceeded the change in a stock’s trading activity. As an example of how this will work: the outcome variable for a post that mentioned ticker QQQ on 12/1 would be 1 or 0 depending on whether QQQ increased or decreased in activity on 12/2.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport csv\nimport pandas as pd\nimport os\n\nos.getcwd()\n\n# reading in data\ndataIn = pd.read_csv('../data/01-modified-data/joinedSentiment.csv')\n\n#print(dataIn['deltaActivity'] == 0)\n\n\n\n\nCode\n# Let's read in our vectorized data from reddit\ntextsIn = pd.read_csv('../data/01-modified-data/vectorizedReddit.csv')\n\n# Double check shape\nprint(textsIn.shape)\n\n\n(7436, 49692)\n\n\n\n\nCode\n# Check columns are sorted by occurence\n#print(textsIn.sum(axis=0))\n\n# Looks like it is sorted in the proper order\n\n# How many columns are for a word that is only used once?\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\n#print(textsIn.head())\n\n\n\n\n\n\nCode\n# How big a change was removing single-use words?\ntextsIn.shape\n# After removing words only used once,  we have 35k words\n#print(dataIn.head)\n\n\n# Now, lets try and fit the GNB\n\n# We will need to merge in the activity change data, lagged by one day\nfrom datetime import datetime, timedelta\n\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n\n\n\n\nCode\n# Now we subtracted one day, let's add the categorical label variable we are trying to predict and then split the data\n\ndataIn['activityIncrease'] = dataIn['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n# Change percent signs to decimals\n#dataIn['Bearish'] = dataIn['Bearish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Bullish'] = dataIn['Bullish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Neutral'] = dataIn['Neutral'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Spread'] = dataIn['Spread'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n\n\n\n\n\n\nCode\n# Now that we subtracted one day in the cell above, let's merge the text corpus with the daily activity label\n\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\n\n\n\nCode\n# now let's check that our merge worked and then split into test, training, and validation\n#print(processed.iloc[:,range(15,25)].head)\n\n#print(processed.isna().sum() &lt; processed.shape[0])\n\nprocessed2 = processed.dropna(subset=['date_utc'])\n#print(processed2.head)\n\ntrain, test = train_test_split(processed2, test_size=0.15)\ntrain, validate = train_test_split(train, test_size = 0.177)\n\n\n\n\nCode\n# Now we have our filtered data, with 2000 observations. Train is 1411\n\n#print(train.shape)\n\n# Now we can try and fit our GNB classifier\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(Xtrn, Xtst, Ytrn, Ytst, i_print=False):\n\n    #if(i_print):\n        #print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=Xtrn\n    y_train=Ytrn\n\n    x_test=Xtst\n    y_test=Ytst\n\n    # INITIALIZE MODEL \n    model = GaussianNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n\n\n\n\n\nCode\n# Test function on training and validation set\nnonNumericColumns = ['X', 'activityIncrease', 'date.x', 'Total', 'X8.week', 'date.x', 'SPYHighWk', 'SPYLowWK', 'SPYCloseWK', 'date_utc', 'title', 'ticker', 'date.y', 'sentiment_x', 'deltaActivity', 'deltaSentiment', 'activity_x', 'week.x', 'weekyear', 'newEntry']\n\n\n# All of the below are for the text model\nxtrn = train.drop(columns = nonNumericColumns)\nxtst = validate.drop(columns = nonNumericColumns)\nxreal = test.drop(columns = nonNumericColumns)\n\n# replace remaining NAs\nxtrn = xtrn.fillna(0)\nxtst = xtst.fillna(0)\nxreal = xreal.fillna(0)\n\n# pull out the output feature (increase 1 or 0)\nytrn = train['activityIncrease']\nytst = validate['activityIncrease']\nyrl = test['activityIncrease']\n\n\n# These are for the non-text, tabular model\nxtabtrn = xtrn.iloc[:,0:8]\nxtabtst = xtst.iloc[:,0:8]\nxtabreal = xreal.iloc[:,0:8]\n\n\n#Same output features, no need to repeat, and NAs are already replaced\n#print(list(xtabtrn.columns.values) )\n\n# Train model\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtrn, xtst, ytrn, ytst, i_print=True)\n\n\n77.53366406803686 55.92105263157895 1.328125 1.28125"
  },
  {
    "objectID": "naivebayes/naivebayes.html#introduction-to-naive-bayes",
    "href": "naivebayes/naivebayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "A naive bayes classifier assigns labels to observations using a process based on Bayes’ theorem. The important element of the theorem is updating the probability of an event based on the known prior events. For instance, if all vehicles with 2 wheels are motorcycles, then we can update the probability that an unknown vehicle is a motorcycle given whether we know it has two wheels. In the case of a classification algorithm, the prosterior probabilities in question are the likelihoods that a given observation belongs to each class. Using the known prior events to update the probability of each class, the algorithm would take whichever class label had the highest likelihood and assign that to the data point in question.\nA naive Bayes classifier has several variants based on the types of features being used to predict class labels. - Gaussian: If the predictors are continuous, then we likely will not have exact probabilities for the given input point (as no other point may have the exact same value for all input features). As such, a different approach is required, where it is assumed that the continuous attributes are normally distributed. Then, for a given point that needs to be classified, we can express the probability of it belonging to a certain class as the likelihood that the point came from each class’s distribution. This is relatively easy, as we can simply find the likelihood that the point was generated by each class distribution, and assign the most likely one as the label. - Multinomial: This method is preferred when the predictors are generated by a multinomial distribution, meaning they are discrete, with more than 2 levels. Labels would be assigned to new observations based on the conditional probability of observing the point’s attributes given that it came from each class (which can be calculated again using Baye’s theorem since the attributes are discirete). - Bernoulli Naive Bayes: The Bernoulli method is preferred when the predictors are binary, meaning they are discrete but only have two values they can take. This is calculated similar to the multinomial variant, but is even more straightforwards as there are only two classes.\n\n\nObjective of Naive Bayes for my Project: With my Naive Bayes classifier, I hope to identify stocks in the top 10 by retail investor activity that have increases in retail investor activity on a given day. To do this I will convert a continuous variable (the daily change in retail investor activity) into two classes based on whether a given stock increased or decreased in activity day-over-day.\nTo predict this outcome variable for each stock in the top 10, I will use two different sets of predictors in the Naive Bayes classifier. First, I will use my tabular data: - Individual investor sentiment from the proceding week, including the percent of respondents who were bearish, bullish, and neutral on the market. - Tabular data from reddit posts including the number of comments.\nSecond, I will use my textual data to predict the same increase/decrease outcome variable: - A corpus of 7.5k reddit posts mentioning top-10 tickers, which contain about 50,000 unique words.\n\n\n\nWe are splitting the data into training, validation, and test sets to ensure we optimally fit the model and avoid over or under fitting. In particular, we can use the validation set to tune our model and improve its generalizability, and then use the test set to evaluate the performance of our model. I will do a 70-15-15 split between the train, evaluation, and test data, and this is done simply by randomly splitting the dataset.\nFor the text data: I will prepare the corpus in a few steps. 1. I have already used a stemmer to reduce the number of unique words by about 7,000. After this reduction we still have 50,000 words. 2. I will filter out words with &lt;5 total uses in the corpus. 3. Then, I will join my binary outcome variable with the reddit text data according to the date of each post. The outcomes will be lagged by 1 day, as we want to ensure that the reddit mentions preceeded the change in a stock’s trading activity. As an example of how this will work: the outcome variable for a post that mentioned ticker QQQ on 12/1 would be 1 or 0 depending on whether QQQ increased or decreased in activity on 12/2.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport csv\nimport pandas as pd\nimport os\n\nos.getcwd()\n\n# reading in data\ndataIn = pd.read_csv('../data/01-modified-data/joinedSentiment.csv')\n\n#print(dataIn['deltaActivity'] == 0)\n\n\n\n\nCode\n# Let's read in our vectorized data from reddit\ntextsIn = pd.read_csv('../data/01-modified-data/vectorizedReddit.csv')\n\n# Double check shape\nprint(textsIn.shape)\n\n\n(7436, 49692)\n\n\n\n\nCode\n# Check columns are sorted by occurence\n#print(textsIn.sum(axis=0))\n\n# Looks like it is sorted in the proper order\n\n# How many columns are for a word that is only used once?\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\n#print(textsIn.head())\n\n\n\n\n\n\nCode\n# How big a change was removing single-use words?\ntextsIn.shape\n# After removing words only used once,  we have 35k words\n#print(dataIn.head)\n\n\n# Now, lets try and fit the GNB\n\n# We will need to merge in the activity change data, lagged by one day\nfrom datetime import datetime, timedelta\n\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n\n\n\n\nCode\n# Now we subtracted one day, let's add the categorical label variable we are trying to predict and then split the data\n\ndataIn['activityIncrease'] = dataIn['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n# Change percent signs to decimals\n#dataIn['Bearish'] = dataIn['Bearish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Bullish'] = dataIn['Bullish'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Neutral'] = dataIn['Neutral'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n#dataIn['Spread'] = dataIn['Spread'].apply(lambda x: x.rstrip(\"%\").astype(float)/100)\n\n\n\n\n\n\nCode\n# Now that we subtracted one day in the cell above, let's merge the text corpus with the daily activity label\n\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\n\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\n\n\n\nCode\n# now let's check that our merge worked and then split into test, training, and validation\n#print(processed.iloc[:,range(15,25)].head)\n\n#print(processed.isna().sum() &lt; processed.shape[0])\n\nprocessed2 = processed.dropna(subset=['date_utc'])\n#print(processed2.head)\n\ntrain, test = train_test_split(processed2, test_size=0.15)\ntrain, validate = train_test_split(train, test_size = 0.177)\n\n\n\n\nCode\n# Now we have our filtered data, with 2000 observations. Train is 1411\n\n#print(train.shape)\n\n# Now we can try and fit our GNB classifier\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(Xtrn, Xtst, Ytrn, Ytst, i_print=False):\n\n    #if(i_print):\n        #print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=Xtrn\n    y_train=Ytrn\n\n    x_test=Xtst\n    y_test=Ytst\n\n    # INITIALIZE MODEL \n    model = GaussianNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n\n\n\n\n\nCode\n# Test function on training and validation set\nnonNumericColumns = ['X', 'activityIncrease', 'date.x', 'Total', 'X8.week', 'date.x', 'SPYHighWk', 'SPYLowWK', 'SPYCloseWK', 'date_utc', 'title', 'ticker', 'date.y', 'sentiment_x', 'deltaActivity', 'deltaSentiment', 'activity_x', 'week.x', 'weekyear', 'newEntry']\n\n\n# All of the below are for the text model\nxtrn = train.drop(columns = nonNumericColumns)\nxtst = validate.drop(columns = nonNumericColumns)\nxreal = test.drop(columns = nonNumericColumns)\n\n# replace remaining NAs\nxtrn = xtrn.fillna(0)\nxtst = xtst.fillna(0)\nxreal = xreal.fillna(0)\n\n# pull out the output feature (increase 1 or 0)\nytrn = train['activityIncrease']\nytst = validate['activityIncrease']\nyrl = test['activityIncrease']\n\n\n# These are for the non-text, tabular model\nxtabtrn = xtrn.iloc[:,0:8]\nxtabtst = xtst.iloc[:,0:8]\nxtabreal = xreal.iloc[:,0:8]\n\n\n#Same output features, no need to repeat, and NAs are already replaced\n#print(list(xtabtrn.columns.values) )\n\n# Train model\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtrn, xtst, ytrn, ytst, i_print=True)\n\n\n77.53366406803686 55.92105263157895 1.328125 1.28125"
  },
  {
    "objectID": "naivebayes/naivebayes.html#feature-selection-for-record-data",
    "href": "naivebayes/naivebayes.html#feature-selection-for-record-data",
    "title": "Naive Bayes",
    "section": "Feature selection for Record data",
    "text": "Feature selection for Record data\n\n\nCode\ndef initialize_arrays_tab():\n    global num_features_tab,train_accuracies_tab\n    global test_accuracies_tab,train_time_tab,eval_time_tab\n    num_features_tab=[]\n    train_accuracies_tab=[]\n    test_accuracies_tab=[]\n    train_time_tab=[]\n    eval_time_tab=[]\n\n\n# start arrays\ninitialize_arrays_tab()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search_tab(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        \n        # temp data\n        xtraintemp =xtabtrn.iloc[:,0:upper_index]\n        xtesttemp =xtabtst.iloc[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtraintemp, xtesttemp, ytrn, ytst, i_print=False)\n\n        if(i%1==0):\n            print(i,upper_index,xtraintemp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features_tab.append(xtraintemp.shape[1])\n        train_accuracies_tab.append(acc_train)\n        test_accuracies_tab.append(acc_test)\n        train_time_tab.append(time_train)\n        eval_time_tab.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search_tab(num_runs=7, min_index=0, max_index= xtabtrn.shape[1])\n\n\n1 1 1 58.39829907866761 55.5921052631579\n2 2 2 60.31183557760453 59.539473684210535\n3 3 3 60.31183557760453 59.539473684210535\n4 4 4 61.65839829907866 56.57894736842105\n5 5 5 59.957476966690294 56.9078947368421\n6 6 6 62.296243798724305 57.89473684210527\n7 7 7 62.296243798724305 60.526315789473685\n\n\n\n\nCode\n# More helper functions\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef save_results_tab(path_root):\n    out= np.transpose(np.array([num_features_tab,train_accuracies_tab,test_accuracies_tab,train_time_tab,eval_time_tab])) \n    out= pd.DataFrame(out)\n    out.to_csv(path_root+\"_tab.csv\")\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results_tab(path_root):\n\n    #PLOT-1\n    plt.plot(num_features_tab,train_accuracies_tab,'-or')\n    plt.plot(num_features_tab,test_accuracies_tab,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'_tab-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features_tab,train_time_tab,'-or')\n    plt.plot(num_features_tab,eval_time_tab,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'_tab-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies_tab),train_time_tab,'-or')\n    plt.plot(np.array(test_accuracies_tab),eval_time_tab,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'_tab-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features_tab,np.array(train_accuracies_tab)-np.array(test_accuracies_tab),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'_tab-4.png')\n    plt.show()\n\n\noutput_dir = \"./\"\nsave_results_tab(output_dir+\"/partial_grid_search\")\nplot_results_tab(output_dir+\"/partial_grid_search\")"
  },
  {
    "objectID": "naivebayes/naivebayes.html#feature-selection-for-text-data",
    "href": "naivebayes/naivebayes.html#feature-selection-for-text-data",
    "title": "Naive Bayes",
    "section": "Feature selection for Text data",
    "text": "Feature selection for Text data\n\n\nCode\n# now lets start removing features using code from the lab demo\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# start arrays\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        \n        # temp data\n        xtraintemp =xtrn.iloc[:,0:upper_index]\n        xtesttemp =xtst.iloc[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtraintemp, xtesttemp, ytrn, ytst, i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtraintemp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtraintemp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index= xtrn.shape[1])\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n#partial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 1770 1770 66.05244507441532 56.25\n10 3540 3540 69.31254429482637 55.5921052631579\n15 5310 5310 70.30474840538625 54.93421052631579\n20 7080 7080 72.14741318214033 55.26315789473685\n25 8850 8850 73.5648476257973 57.23684210526315\n30 10620 10620 74.98228206945429 56.57894736842105\n35 12390 12390 75.69099929128278 56.57894736842105\n40 14160 14160 75.90361445783132 55.92105263157895\n45 15930 15930 76.96669029057406 54.93421052631579\n50 17700 17700 77.25017717930545 55.26315789473685\n55 19470 19470 77.39192062367115 56.9078947368421\n60 21240 21240 76.96669029057406 56.25\n65 23010 23010 77.67540751240254 55.92105263157895\n70 24780 24780 77.32104890148831 56.9078947368421\n75 26550 26550 77.6045357902197 56.57894736842105\n80 28320 28320 77.10843373493977 55.92105263157895\n85 30090 30090 77.39192062367115 55.26315789473685\n90 31860 31860 77.03756201275691 55.26315789473685\n95 33630 33630 77.32104890148831 55.5921052631579\n100 35400 35400 77.53366406803686 55.92105263157895\n\n\n\n\nCode\n# More helper functions\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef save_results(path_root):\n    out= np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out= pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\noutput_dir = \"./\"\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")"
  },
  {
    "objectID": "naivebayes/naivebayes.html#naive-bayes-with-labeled-record-data",
    "href": "naivebayes/naivebayes.html#naive-bayes-with-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes with Labeled Record Data",
    "text": "Naive Bayes with Labeled Record Data\n\n\nCode\n# Based on previous section, using all 8 features is ideal\n#  fit model finally, report on overall accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n\nrealTabModel = GaussianNB()\nrealTabModel.fit(xtabtrn, ytrn)\n\n# fit model and get predictions\nrealTabTestPreds = realTabModel.predict(xtabreal)\n\n# use a confusion matrix from SKLearn to visualize the accuracy \nconfusion_matrix = metrics.confusion_matrix(yrl.values, realTabTestPreds)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title('Confusion Matrix for Naive Bayes Classifier on Record Data')\nplt.show()\n\ntabOutcomeData = pd.DataFrame.from_dict( {'ticker': test['ticker'], 'comments': xtabreal['comments'], 'real value': yrl.values, 'predicted value': realTabTestPreds, \"miss\": yrl.values - realTabTestPreds} )\n\nsns.scatterplot(data=tabOutcomeData.groupby(['ticker'])['miss'].mean().to_frame(), x = 'miss', y = 'ticker')\nplt.xlabel('Average Prediction Miss (-1 = mistaken increase, 1 = mistaken decrease)')\nplt.ylabel('Stock Ticker')\nplt.title('Average Prediction Misses by Stock')\nplt.show()\n\nsns.boxplot(data=tabOutcomeData, x = 'predicted value', y = 'comments')\nplt.xlabel('Model Prediction (0 decrease, 1 increase)')\nplt.ylabel('Number of Comments on Related Posts')\nplt.title('Predictions vs. Comments on Reddit Posts')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe optimal model for the record data was the one that included all 7 predictors. I identified this in the feature selection plots using the elbow method, which showed that the increase in performance never leveled off as more tabular features were added.\nThen, I fit this model on the training data and used it to predict values in the test set, the results of which are pictured in the first image above, the confusion matrix. While I expected the model to perform worse on the test set, the overall accuracy score (~0.64) was similar to those observed in the training (~0.60) and validation (~0.63) sets. The model appears to have learned to be successful by predicting more 1’s (or increases) than 0’s (or decreases). This gave the model high sensitivity but low precision.\nIn terms of over and under fitting, I believe my model is currently underfit although it includes all available features, with poor performance over all. I do not believe the record data model has been over fit because it has similar performance in both the training and test sets, so it has low variance. Yet the model has high bias in both training and test sets, only capturing an accurate result 60% of the time, barely beating a coin toss.\nThe projects findings will be documented in a final slideshow, which will polish up and abridge the work contained in this page.\nIn conclusion: A Gaussian Naive Bayes classifier performs moderately well at predicting whether a given stock in the top-10 rankings of retail trader activity will see an increase or decrease in activity on the next trading day. With only 7 input features (such as the sentiment of individual investors and the number of reddit comments on posts mentioning the stock), the model identifies increases or decreases correctly about 64% of the time. While this is an improvement over random chance, the model could certainly be improved. The model also shows some relationships between variables: Certain stocks were almost always predicted over-optimisticly, such as SNAP (Snapchat), which was a famous stock that retail investors were interested in but lost money on. The number of comments also appears to be negatively correlated to stock increases, with tickers that had more comments on related posts being less likely to have an increase predicted the next day."
  },
  {
    "objectID": "naivebayes/naivebayes.html#naive-bayes-with-labeled-text-data",
    "href": "naivebayes/naivebayes.html#naive-bayes-with-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes with Labeled Text Data",
    "text": "Naive Bayes with Labeled Text Data\n\n\nCode\n# fit model finally, report on overall accuracy\n# based on the previous charts, we can use the elbow method for \nrealModel = GaussianNB()\nrealModel.fit(xtrn.iloc[:,0:10000], ytrn)\n\n# Get predictions\nrealTestPreds = realModel.predict(xreal.iloc[:,0:10000])\n#print(realTestPreds)\n#print(yrl.values)\n\n# Make another confusion matrix to compare the accuracy of this classifier\n\nconfusion_matrix = metrics.confusion_matrix(yrl.values, realTestPreds)\n\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title(\"Confusion Matrix of Naive Bayes with Optimal Hyperparameter\")\nplt.show()\n\noutcomeData = pd.DataFrame.from_dict( {'ticker': test['ticker'], 'comments': xreal['comments'], 'real value': yrl.values, 'predicted value': realTestPreds, \"miss\": yrl.values - realTestPreds} )\n\n# Create a view exploratory charts to look at which observations are being classified in which direction, and how accurately\nsns.scatterplot(data=outcomeData.groupby(['ticker'])['miss'].mean().to_frame(), x = 'miss', y = 'ticker')\nplt.xlabel('Average Prediction Miss (-1 = mistaken increase, 1 = mistaken decrease)')\nplt.ylabel('Stock Ticker')\nplt.title('Average Prediction Misses by Stock')\nplt.show()\n\n# Looking at the number of comments vs. predicted value\nsns.boxplot(data=outcomeData, x = 'predicted value', y = 'comments')\nplt.xlabel('Model Prediction (0 decrease, 1 increase)')\nplt.ylabel('Number of Comments on Related Posts')\nplt.title('Predictions vs. Comments on Reddit Posts')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFor the textual data Gaussian Naive Bayes classifier, it required more deduction to pick the correct number of features. This is because I began with about 35,000 columns in the dataset, due to the large number of unique words used in the reddit text corpus. However, using the elbow method, it appeared that the model’s gains in performance peaked around the 10,000 feature mark. This number was also the point where the model reached its approximate peak in performance in the validation set, so I was curious to extend it to the test set and determine if it would continue to have low bias.\nAs such, I fit this model on the training data and used it to predict values in the test set, the results of which are pictured in the first image above, the confusion matrix. Surprisingly, the far more complicated model performed worse on the test set in terms of overall accuracy than the simple, 7-feature model (~0.61 instead of ~0.64). This performance was similar to those observed in the training (~0.60) and validation (~0.63) sets. Once again, the model appears to have learned to be successful by predicting more 1’s (or increases) than 0’s (or decreases). This gave the model high sensitivity but low precision.\nIn terms of over and under fitting, I believe the text data model model is still underfit, because it continues to have high bias and low variance across the training and test set. This is supported by the model’s similar accuracy in both the training and test datasets. These findings will also be incorporated into a slideshow, along with, potentially, further work to tune the text data analysis.\nIn conclusion: A Gaussian Naive Bayes classifier trained on 1400 reddit posts performs moderately well at predicting whether a given stock in the top-10 rankings of retail trader activity will see an increase or decrease in activity on the next trading day. With 10,000 input features, the model identifies increases or decreases correctly about 61% of the time. While this is an improvement over random chance, the model could certainly be improved. As a point of interest, I included the same tabular data points in the text model as well, and found the relationships between the variables had changed. When accounting for textual data from reddit, the model begins to predict increases for stock tickers with a large number of comments in related posts. Further, the model’s performance changed for particular stocks, with SNAP (Snapchat) no longer being tied with SQQQ for the most optimistic performance. Based on these results, it does not seem that reddit post texts contain a large amount of useful information for predicting retail investor actvity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Coming from an undergraduate background in social science, I am interested in data science because it is useful for such a wide array of problems, and I feel working in data science let’s you focus on interesting subjects throughout your career. I am a Seattle native but came to DC to attend American University for my undergraduate degree, where I majored in International Studies with a minor in Data Science. During my time in undergrad, I focused on international relations research, but through a quantitative lens, writing papers on political bargaining in Somalia and modeling protest violence in the United States. After graduating during COVID, I worked for a small international development consulting firm, providing data science adjacent skills on a myriad of different development projects for USAID, Australia’s DFAT, the World Bank, and others. I really enjoyed my time working in that field, but I wanted to learn more advanced ML methods, and so I am excited to be at Georgetown with the opportunity to dig into the concepts that interest me! After working on blended finance programs for several years, I am excited to be back in school with the opportunity to learn about powerful data science methods.\nResearch interests:\n\nMetalearning and ensembling for time series prediction\nBet sizing and portfolio management\nTying diverse datasets together\n\nOutside school, I live in Arlington. Having lived around the DMV for the past 5 years off and on, I have found there are many exciting activities in the area. In particular, I enjoy getting outdoors however I can, such as hiking or just walking around different neighborhoods. I also love to cook, travel to other countries or just new US cities, develop video games, and try out as many coffee shops as possible! (Although the East Coast really hasn’t approached Seattle’s standards yet in my, totally biased, opinion.)"
  },
  {
    "objectID": "dectrees/dectrees.html",
    "href": "dectrees/dectrees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Random and boosted forests are some of the most pervasive machine learning methods today. For tabular data, these methods are extremely effective at identifying nonlinear relationships, as well as identifying which features in the input space are most important. I hope to use the excellent results promised by these methods to achieve real predictive power on my outcome variables (the direction and amount that retail activity changes on a given stock in a given day). To do this, I will try and predict these outcome variables using all of my tabular predictors. I have tabular data for all observations, but only some have complete data, and based on the selectiveness of the different models the number of viable observations range from ~8,000 to just 2000 in the dataset. For each of these models I will attempt to correctly classify which stocks increased or decreased in activity, which is the outcome variable.\nI will start by creating a single decision tree as a baseline, and I will determine what accuracy this tree is able to achieve in classifying increases and decreases in activity. I will also fit a random classifier, so that I can compare my later models to a purely random result. After I have run this baseline both with and without the text data, I will run a random forest model. Based on the performance of this model, I will also try a boosted model, and then I will pick one of the two for which to optimize hyperparamters to get the best result.\nDecision trees can work for problems like mine because they are exceptionally good at identifying nonlinear relationships in data. To understand how they work, imagine you are deciding between 20 new car models to buy. A decision tree is similar to how a human being might approach such a choice. In finding the best car to purchase, you might set a standard or threshold such as: “less than x amount of dollars to purchase.” This would eliminate some number of the cars. Then you would further ask for features such as “all wheel drive,” or “seatwarmers,” until eventually you had a car which was the closest to your true requirements. Decision trees are essentially a similar method as this anecdote, except they attempt to find the most efficient number of requirements you could specify to match a car to a given person (classification) or it’s most ideal pricepoint(regression). In mathematical terms, they try to reduce the entropy of the dataset by dividing it along meaningful and efficient boundaries (called nodes that divide the data into branches), until only small distinct groups are left (called leaves)."
  },
  {
    "objectID": "dectrees/dectrees.html#methods",
    "href": "dectrees/dectrees.html#methods",
    "title": "Decision Trees",
    "section": "",
    "text": "Random and boosted forests are some of the most pervasive machine learning methods today. For tabular data, these methods are extremely effective at identifying nonlinear relationships, as well as identifying which features in the input space are most important. I hope to use the excellent results promised by these methods to achieve real predictive power on my outcome variables (the direction and amount that retail activity changes on a given stock in a given day). To do this, I will try and predict these outcome variables using all of my tabular predictors. I have tabular data for all observations, but only some have complete data, and based on the selectiveness of the different models the number of viable observations range from ~8,000 to just 2000 in the dataset. For each of these models I will attempt to correctly classify which stocks increased or decreased in activity, which is the outcome variable.\nI will start by creating a single decision tree as a baseline, and I will determine what accuracy this tree is able to achieve in classifying increases and decreases in activity. I will also fit a random classifier, so that I can compare my later models to a purely random result. After I have run this baseline both with and without the text data, I will run a random forest model. Based on the performance of this model, I will also try a boosted model, and then I will pick one of the two for which to optimize hyperparamters to get the best result.\nDecision trees can work for problems like mine because they are exceptionally good at identifying nonlinear relationships in data. To understand how they work, imagine you are deciding between 20 new car models to buy. A decision tree is similar to how a human being might approach such a choice. In finding the best car to purchase, you might set a standard or threshold such as: “less than x amount of dollars to purchase.” This would eliminate some number of the cars. Then you would further ask for features such as “all wheel drive,” or “seatwarmers,” until eventually you had a car which was the closest to your true requirements. Decision trees are essentially a similar method as this anecdote, except they attempt to find the most efficient number of requirements you could specify to match a car to a given person (classification) or it’s most ideal pricepoint(regression). In mathematical terms, they try to reduce the entropy of the dataset by dividing it along meaningful and efficient boundaries (called nodes that divide the data into branches), until only small distinct groups are left (called leaves)."
  },
  {
    "objectID": "dectrees/dectrees.html#class-distribution",
    "href": "dectrees/dectrees.html#class-distribution",
    "title": "Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\nRead in and clean data:\n\n\nCode\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom numpy import random\nfrom sklearn.metrics import accuracy_score\n\n\n\n\nCode\n# Read in Data\ntabData = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\") \ntabData.shape\n\ntextData = pd.read_csv(\"../data/01-modified-data/vectorizedReddit.csv\") \ntabData.shape\n\n\n(9560, 23)\n\n\nMore cleaning code to create the datasets.\n\n\nCode\n# Clean data, same code as used in other tabs with slight tweaks\ntextsIn = textData\ndataIn = tabData\n\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\ntextsIn.shape\n\n# Lag outcome data by 1 day, to make sure we are predicting the future and not the present\nfrom datetime import datetime, timedelta\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n# Fix formatting of price values\ndataIn.SPYCloseWK = dataIn.SPYCloseWK.apply(lambda x: float(str(x).replace(\",\", \"\") ))\ndataIn.SPYHighWk = dataIn.SPYHighWk.apply(lambda x: float(str(x).replace(\",\", \"\") ))\ndataIn.SPYLowWK = dataIn.SPYLowWK.apply(lambda x: float(str(x).replace(\",\", \"\") ))\n\n# Join datasets\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n# Create binary outcome variable\nprocessed['activityIncrease'] = processed['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\nprocessed2 = processed.dropna(subset=['date_utc'])\nprint(processed2.shape)\n\n# Exclude non numeric columns\nexclude = ['lagweek','date_utc','title', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x']\n\n\nprocessed3 = processed2.drop(columns = exclude, axis = 1)\n\nprocessed3.head()\nX = processed3.drop('activityIncrease', axis=1)\ny = processed3['activityIncrease']\n\n\n(2018, 35428)\n\n\nClass distribution for full (8,000) and reduced (2,000) datasets.\n\n\nCode\n# Code to compute the class distribution\nxlocs = [0,1]\nxlabs = [\"Decreased\", \"Increased or Stayed the Same\"]\n\ntemp = processed2.loc[:,'activityIncrease'].astype(str)\ntemp2 = processed.loc[:,'activityIncrease'].astype(str)\n\n\n# Create plots of class distribution\nvalues, bins, bars = plt.hist(temp, edgecolor='white')\nplt.xticks(xlocs, xlabs)\nplt.bar_label(bars, fontsize=20, color='navy')\nplt.title(\"Class Distribution for Tickers with Reddit Posts \")\nplt.show()\n\nvalues, bins, bars = plt.hist(temp2, edgecolor='white')\nplt.xticks(xlocs, xlabs)\nplt.bar_label(bars, fontsize=20, color='navy')\nplt.title(\"Class Distribution for all Tickers in Dataset\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the plots, we can see that 827 stocks in the daily top 10 decreased in retail trader activity, while 1191 increased, for the data which had relevant reddit posts available. As a proportion, this is 0.41 of all the stocks in the top 10 with reddit posts. In the second plots, for all the tickers in the dataset, 4373 had a decrease, while 5648 had an increase. As a proportion, this is 0.44 out of all stocks in the top 10. It is worth noting that the outcome variable differs in proportion between the categories."
  },
  {
    "objectID": "dectrees/dectrees.html#baseline-model-random-classifier",
    "href": "dectrees/dectrees.html#baseline-model-random-classifier",
    "title": "Decision Trees",
    "section": "Baseline Model / Random Classifier",
    "text": "Baseline Model / Random Classifier\nNow, let’s train a random classifier and a baseline decision tree on our two datasets, and visualize the tree that we construct. Starting with the random classifier:\n\n\nCode\n# create both the textual, and textual + tabular datasets\nally = temp2.astype(int)\ntexty = temp.astype(int)\n\nrand_ally = random.randint(2, size = ally.shape[0])\nrand_texty = random.randint(2, size = texty.shape[0])\n\n# Run a confusion matrix for both datasets, with a random classifier\nconfusion_matrix = metrics.confusion_matrix(ally, rand_ally)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title(\"Confusion Matrix for Random Classifier, Text and Tabular Data \")\nplt.show()\nprint(accuracy_score(ally, rand_ally))\n\n\nconfusion_matrix = metrics.confusion_matrix(texty, rand_texty)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nplt.title(\"Confusion Matrix for Random Classifier, Text Data Only\")\nplt.show()\n\nprint(accuracy_score(texty, rand_texty))\n\n\n\n\n\n0.49915178125935533\n0.4886025768087215\n\n\n\n\n\nBased on both the confusion matrices and the accuracy scores, we can see that the random classifier performs close to a coinflip, with 51% accuracy for the tabular data and 48% accuracy for the textual and tabular data combined. Now let’s evaluate a single decision tree:\nFirst, we make sure we have train and validation data:\n\n\nCode\ntreeTab = dataIn\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\n# Drop columns which wont work for the decision tree classifier\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'ticker', 'date.x', 'date.y', 'weekyear', 'week.x', 'Total', 'X8.week', 'week.y', 'lagweek'])\ntreeTaby = treeTab['activityIncrease']\n\n# Train test split\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n\n\n\n\nCode\n# Define the classifier (max depth 4 performed best)\nclass_1 = DecisionTreeClassifier(max_depth=4)\n\n# Fit it to the data\nclass_1.fit(tree_trainx, tree_trainy)\n\n# Helper function from the lab to plot decision trees\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(15,10))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(class_1)\n\n\n\n\n\nLet’s look at the confusion matrix:\n\n\nCode\n# Predict the data for test and training sets\nc1tpred = class_1.predict(tree_trainx)\nc1vpred = class_1.predict(tree_testx)\n\n# Generate confusion matrix\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c1vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c1vpred))\n\n\n0.7201267828843106\n\n\n\n\n\nOur baseline tree obtains an impressive accuracy of 73.2% on the test set! Let’s see if it performs similarly well on the text data. First, we prepare the text train and test split:\n\n\nCode\ntreeTxt = processed3\n\ntreeTxtx =  treeTxt.drop(columns = ['activityIncrease'])\ntreeTxty = treeTxt['activityIncrease']\n\n# Train test split for the textual data\ntree_train_txtx, tree_test_txtx, tree_train_txty, tree_test_txty = train_test_split(treeTxtx, treeTxty, test_size= 0.33)\n\n\nNow, we can fit our depth 4 tree:\n\n\nCode\n# Rub a classifier on text data\nclass_2 = DecisionTreeClassifier(max_depth=4)\n\nclass_2.fit(tree_train_txtx, tree_train_txty)\n\nplot_tree(class_2)\n\n\n\n\n\n\n\nCode\nc2tpred = class_2.predict(tree_train_txtx)\nc2vpred = class_2.predict(tree_test_txtx)\n# Once again, predict and create confusion matrix, but for text data\nconfusion_matrix = metrics.confusion_matrix(tree_test_txty, c2vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_test_txty, c2vpred))\n\n\n0.5975975975975976\n\n\n\n\n\nInterestingly, on the tree with the text data added, we see much worse performance, around 63%. This could certainly be due to the lack of training data when limiting to only those stock posts with reddit texts available. Notably, however, several of the text columns did enter into the decision tree, suggesting the text data was not completely meaningless. Let’s see if we can get better performance by implementing bagging and random forests.\n\nModel Implementation and Hyperparameter Tuning\n\n\n\nCode\ntreeTab = dataIn\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\ntreeTab = treeTab.dropna()\n\n# drop columns which will not work for random forest classifier\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'ticker', 'date.x', 'date.y', 'weekyear', 'week.x', 'Total', 'X8.week', 'week.y', 'lagweek'])\ntreeTaby = treeTab['activityIncrease']\n# train test split\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n\n\n\n\nCode\n# run classifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\nrf.fit(tree_trainx, tree_trainy)\n\n\nRandomForestClassifier(n_estimators=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=1000, random_state=42)\n\n\nNow that we’ve fit the random forest model, let’s diagnose and create plots:\n\n\nCode\nc3tpred = rf.predict(tree_trainx)\nc3vpred = rf.predict(tree_testx)\n# calculate predictions and generate confusion matrix\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c3vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c3vpred))\n\n\n0.7428104575163399\n\n\n\n\n\nOut random forest classifier has only produced an accuracy of 73.5%, which is barely better than our naive decision tree. Let’s try a gradient boosted classifier and see if it performs better:\n\n\nCode\n# run gradient boosting classifier (same dataset as random forest this time)\ngbc = GradientBoostingClassifier(n_estimators = 1000, random_state = 42)\ngbc.fit(tree_trainx, tree_trainy)\n\n\nGradientBoostingClassifier(n_estimators=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier(n_estimators=1000, random_state=42)\n\n\n\n\nCode\nc4tpred = gbc.predict(tree_trainx)\nc4vpred = gbc.predict(tree_testx)\n# get predictions and create confusion matrix for boosted classifier\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c4vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c4vpred))\n\n\n0.7428104575163399\n\n\n\n\n\nSurprisingly, the gradient boosted classifier did worse, if anything, than the random forests. Let’s try to select the optimal n_estimators hyperparameter and see if that helps our performance (I will use acccuracy scores for the plot)\n\n\nCode\ntest_accuracy = []\ntrain_accuracy = []\nnestimators = []\n\n# append the accuracy of the gradient boosted classifier at different hyperparameter values (n estimators)\nfor i in range(10, 150, 10):\n    print(i)\n    rf = GradientBoostingClassifier(n_estimators = i, random_state = 42)\n    rf.fit(tree_trainx, tree_trainy)\n    \n    testpreds = rf.predict(tree_testx)\n    trainpreds = rf.predict(tree_trainx)\n\n    train_accuracy.append(accuracy_score(tree_trainy, trainpreds))\n    test_accuracy.append(accuracy_score(tree_testy, testpreds))\n    nestimators.append(i)\n\n\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n\n\n\n\nCode\n# plot the accuracy of models with different hyperparameters\nplt.plot(nestimators, test_accuracy,label='Test Accuracy' )\nplt.plot(nestimators, train_accuracy,label='Train Accuracy' )\nplt.xlabel(\"Number of Estimators\")\nplt.ylabel(\"Accuracy of the Model\")\nplt.title(\"Accuracy vs. Number of Estimators for Boosted Forests\")\n\n\nText(0.5, 1.0, 'Accuracy vs. Number of Estimators for Boosted Forests')\n\n\n\n\n\nViewing the hyperparameter chart, it seems clear that a smaller number of estimators, close to 100, actually leads to the best performance on the test set. Likely this is the consequence of overfitting. Regardless, the ideal parameter choice is clear as the best model is also the simplest, and leads to an accuracy of almost 75%, the best of any model to date."
  },
  {
    "objectID": "dectrees/dectrees.html#final-results-for-boosted-forest",
    "href": "dectrees/dectrees.html#final-results-for-boosted-forest",
    "title": "Decision Trees",
    "section": "Final Results for Boosted Forest",
    "text": "Final Results for Boosted Forest\n\n\nCode\ngbc = GradientBoostingClassifier(n_estimators = 100, random_state = 42)\ngbc.fit(tree_trainx, tree_trainy)\n    \nc5tpred = gbc.predict(tree_trainx)\nc5vpred = gbc.predict(tree_testx)\n\nconfusion_matrix = metrics.confusion_matrix(tree_testy, c5vpred)\ncm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\ncm_display.plot()\nprint(accuracy_score(tree_testy, c5vpred))\n\nsub_tree = gbc.estimators_[50, 0]\n\nplot_tree(sub_tree)\n\n\n0.7565359477124183"
  },
  {
    "objectID": "dectrees/dectrees.html#adding-in-text-data-to-boosted-forests",
    "href": "dectrees/dectrees.html#adding-in-text-data-to-boosted-forests",
    "title": "Decision Trees",
    "section": "Adding in Text Data to Boosted Forests",
    "text": "Adding in Text Data to Boosted Forests\n\n\nCode\ntreeTab = processed2\ntreeTab['activityIncrease'] = treeTab['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\n\ntreeTabx =  treeTab.drop(columns = ['activityIncrease', 'deltaActivity', 'lagweek','date_utc','title', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x'])\n\n\ntreeTaby = treeTab['activityIncrease']\n\ntree_trainx, tree_testx, tree_trainy, tree_testy = train_test_split(treeTabx, treeTaby, test_size= 0.33)\n\n\nC:\\Users\\corwi\\AppData\\Local\\Temp\\ipykernel_14544\\2411035305.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nCode\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\ntest_accuracy = []\ntrain_accuracy = []\nnestimators = []\n\nfor i in range(1, 2):\n    print(i)\n    rf = HistGradientBoostingClassifier(random_state = 42)\n    rf.fit(tree_trainx, tree_trainy)\n    \n    testpreds = rf.predict(tree_testx)\n    trainpreds = rf.predict(tree_trainx)\n\n    train_accuracy.append(accuracy_score(tree_trainy, trainpreds))\n    test_accuracy.append(accuracy_score(tree_testy, testpreds))\n    nestimators.append(i)\n\n\n1\n\n\n\n\nCode\nplt.plot(nestimators, test_accuracy,label='Test Accuracy' )\nplt.plot(nestimators, train_accuracy,label='Train Accuracy' )\nplt.xlabel(\"Number of Estimators\")\nplt.ylabel(\"Accuracy of the Model\")\nplt.title(\"Accuracy vs. Number of Estimators for Boosted Forests\")\n\n\nText(0.5, 1.0, 'Accuracy vs. Number of Estimators for Boosted Forests')"
  },
  {
    "objectID": "dectrees/dectrees.html#conclusions",
    "href": "dectrees/dectrees.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions",
    "text": "Conclusions\nOverall, the boosted forest model was the best performing out of the 1) simple decision tree 2) random forest and 3) boosted models. This also clearly outperformed a random baseline, and was most robust with 100 estimators included in the model. The boosted forest forest model performs very well (for a financial model) at 74.7% accuracy. This is much better than a 50% baseline and still well above a most-common-outcome classifier which would have 56.5% accuracy. This provides a remarkable picture of what drives investor activity in individual stocks, although it may not be the complete picture still. To understand what features the model uses to predict with accuracy, let’s trace one sample pathway through one component tree of the boosted model:\nLooking at the final tree above (one of many in the classifier, but chosen to illustrate the internal relationships that power the mode). The features, going down the tree are: First a split based on current investor activity in the stock, if it is lower than 0.043 then the model check whether the spread in investor sentiment (bearish bullish) is wider than -0.33, which would indicate a bearish environment. Then, the model would predict based on investor activity in the stock.\nReviewing this example, it appears the model is very interested in the sentiment of investors in the survey, particularly bearish sentiment, as well as the activity of retail investors in a stock on the current day.\nConcerns: One concern I have is that the amount of data is somewhat small &lt;3000 by the end for the random forests given excluded NA values. This is a concern because it allows models such as decision trees to overfit, and this is shown in the random forest having almost perfect accuracy in the training set, but no improvements in the test set with more estimators. I am also concerned that the CURRENT investor activity index is being used to predict the change for the next day, as this worries me that there is some conflation in the activity day over day. I would be curious to confirm how all of the data is defined, to ensure we are actually predicting the future without having more information than is realistic in real time.\nFuture directions: I would like to incorporate the text data as well, to make the predictions even more robust."
  },
  {
    "objectID": "dataexploration/dataexploration.html",
    "href": "dataexploration/dataexploration.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "At the broadest level, there is a temporal component to the research question that will shape how we conduct the analysis. Namely, the outcome variable is retail trading activity in stocks, as measured by daily holding data on the top 10 stocks sourced from NASDAQ’s API. As such, we are interested in data from the other datasets (reddit text information, investor sentiment survey, stocktwits ranking) as of one day before. I will merge the datasets in R, and then use R (ggplot) to create visuals and carry out EDA."
  },
  {
    "objectID": "dataexploration/dataexploration.html#eda-overview",
    "href": "dataexploration/dataexploration.html#eda-overview",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "At the broadest level, there is a temporal component to the research question that will shape how we conduct the analysis. Namely, the outcome variable is retail trading activity in stocks, as measured by daily holding data on the top 10 stocks sourced from NASDAQ’s API. As such, we are interested in data from the other datasets (reddit text information, investor sentiment survey, stocktwits ranking) as of one day before. I will merge the datasets in R, and then use R (ggplot) to create visuals and carry out EDA."
  },
  {
    "objectID": "dataexploration/dataexploration.html#exploring-the-data",
    "href": "dataexploration/dataexploration.html#exploring-the-data",
    "title": "Exploratory Data Analysis",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(Quandl)\nlibrary(lubridate)\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n\n\nLet’s join the data together into one dataset, with properly lagged predictor variables.\n\n\nCode\nredditIn &lt;- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn &lt;- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn &lt;- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] &lt;- \"date\"\n\nsentimentIn &lt;- sentimentIn |&gt;\n    mutate(date = mdy(date)) |&gt;\n    mutate(week = week(date)) |&gt;\n    mutate(lagweek = week + 1) |&gt;\n    mutate(weekyear = paste(year(date), lagweek )) |&gt;\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |&gt;\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |&gt;\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |&gt;\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n\n\nrtatIn &lt;- rtatIn %&gt;%\n    mutate(date = ymd(date)) %&gt;%\n    mutate(week = week(date)) %&gt;%\n    mutate(weekyear = paste(year(date), week))\n\n#head(rtatIn)\n\n\njoinedSentiment &lt;- left_join(rtatIn, sentimentIn, by = \"weekyear\")\n\n#head(joinedSentiment)\n\n\n\nNumeric Summaries\n\n\nCode\nprint(\"Activity Summary:\")\n\n\n[1] \"Activity Summary:\"\n\n\nCode\nsummary(joinedSentiment$activity)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00760 0.01720 0.02460 0.03103 0.03590 0.26150 \n\n\nCode\nprint(\"Sentiment Summary:\")\n\n\n[1] \"Sentiment Summary:\"\n\n\nCode\nsummary(joinedSentiment$sentiment)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-23.0000  -1.0000   1.0000   0.9117   3.0000  19.0000 \n\n\nCode\nprint(\"Change in Activity Summary:\")\n\n\n[1] \"Change in Activity Summary:\"\n\n\nCode\nsummary(joinedSentiment$deltaActivity)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.194000 -0.003700  0.002100  0.003997  0.011600  0.232000 \n\n\nCode\nprint(\"Change in Sentiment Summary:\")\n\n\n[1] \"Change in Sentiment Summary:\"\n\n\nCode\nsummary(joinedSentiment$deltaSentiment)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-24.000  -3.000   0.000  -1.259   1.000  20.000 \n\n\nCode\nprint(\"Positive Sentiment Proportion Summary:\")\n\n\n[1] \"Positive Sentiment Proportion Summary:\"\n\n\nCode\nsummary(joinedSentiment$Bullish)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1584  0.2483  0.3188  0.3262  0.3956  0.5691     290 \n\n\nCode\nprint(\"Negative Sentiment Proportion Summary:\")\n\n\n[1] \"Negative Sentiment Proportion Summary:\"\n\n\nCode\nsummary(joinedSentiment$Bearish)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1976  0.2751  0.3805  0.3732  0.4490  0.6087     290 \n\n\nCode\nprint(\"Neutral Sentiment Proportion Summary:\")\n\n\n[1] \"Neutral Sentiment Proportion Summary:\"\n\n\nCode\nsummary(joinedSentiment$Neutral)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1450  0.2715  0.3043  0.3005  0.3345  0.4091     290 \n\n\nCode\nprint(\"Spread in Sentiment Summary:\")\n\n\n[1] \"Spread in Sentiment Summary:\"\n\n\nCode\nsummary(joinedSentiment$Spread)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-0.43100 -0.19200 -0.07500 -0.04695  0.11700  0.36500      290 \n\n\n\n\nBarplots for discrete variables:\n\n\nCode\nggplot(joinedSentiment, aes(x = newEntry)) + geom_bar() + labs(x= \"Current Stock is in Top 10\", y = \"Count\", title = \"Distribution of New Stocks\\n Amongst Top-10 by Retail Activity\") + theme_grey(base_size = 20)\n\n\n\n\n\nCode\nprint(\"Most Popular Stocks by Days in Top 10:\")\n\n\n[1] \"Most Popular Stocks by Days in Top 10:\"\n\n\nCode\nsort(table(joinedSentiment$ticker), decreasing = TRUE)[1:10]\n\n\n\nAAPL TSLA  SPY  AMD  QQQ TQQQ NVDA AMZN SQQQ MSFT \n 956  956  926  876  760  694  665  531  408  389 \n\n\nIn addition to the porportion of new stocks in the top 10, the table shows up the top 10 stocks by number of days spent in the top-10 ranking of retail investor actvity. We can see that the distribution has a heavy right tail, as even amongst the top 10 stocks there is a large divergence, with the top stock (AAPL) having more than two times as many days in the top 10 as the 10th place stock (MSFT).\n\n\nOutcome Variable Exploration\nFirst, lets explore the outcome variable table, of the top 10 retail-investor held stocks each day. Each stock has an ‘activity’ number which measures the percent of traded shares held by retail investors. Let’s see how those scores are distributed:\n\n\nCode\nggplot(joinedSentiment, aes(x = activity)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily \\n Retail Activity Scores Amongst Top-10 Stocks\") + theme_grey(base_size = 20)\n\n\n\n\n\nActivity scores seem to be skewed to the right, with a thin tail. I wonder which stocks compose the highest activity values. Let’s look at which of these scores were from stocks that had just appeared in the top 10 that day:\n\n\nCode\nggplot(joinedSentiment, aes(x = activity, fill = newEntry)) + geom_bar(stat = 'bin', binwidth = 0.01) + labs(x= \"Daily Retail Activity Score\", y = \"Count\", title = \"Distribution of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\n\nFrom this chart, we can see that most of the high-activity scores are from stocks which previously entered the top 10. This makes sense, as we would expects stock to increase in activity over multiple days, before reaching the top of the list. Similar to songs on top music charts.\nMy next question is how the outcome variable has changed over time. Let’s look at attention scores over time:\n\n\nCode\njoinedSentimentAvgs &lt;- joinedSentiment %&gt;%\n    group_by(date.x) %&gt;%\n    summarize(dailyactivitytotal = sum(activity), dailyactivityavg = mean(activity),\n    dailynewtickers = sum(as.numeric(newEntry))) \n\nggplot(joinedSentimentAvgs, aes(x = date.x, y = dailyactivitytotal)) + geom_line() + labs(x= \"Date\", y = \"Total Retail Investor Activity\", title = \"Total of Daily Retail Activity Scores Amongst Top-10 Stocks\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\n\nThis chart is interesting because it shows that our outcome variable has a clear upward trend over time. Retail investors appear to be a greater proportion of the total activity in the stock market today than they were in 2020.\nOne last visual I wanted to create to evaluate the outcome variables was the number of new top-10 stocks that entered the list each week\n\n\nCode\nlibrary(ggbeeswarm)\n\nggplot(joinedSentimentAvgs, aes(x = as.character(year(date.x)), y = dailynewtickers)) + geom_boxplot() + labs(x= \"Year\", y = \"New Stocks in the Top 10\", title = \"Number of New Stocks in the Top 10 of Retail Activity by Year\", fill = \"New to Top 10\") + theme_grey(base_size = 20)\n\n\n\n\n\nHere we can see there is not a huge difference in the distributions of new stocks between the years, except for 2021, when there was a lot more days with at least 1 new stock in the top 10. But far and away, at least 50% of days have no new stocks in the top 10 of retail trader activity.\n\n\nBivariate Analysis\nNow, lets look at some of the predictor variables in relation to the outcome variables.\n\n\nCode\nlibrary(\"corrplot\")\n\nnoNAsentiment &lt;- na.omit(joinedSentiment[,c(4,5,6,7,12,13,14,17)])\n\nsave &lt;- cor(noNAsentiment)\ncorrplot(save)\n\n\n\n\n\nCode\nsave \n\n\n                  activity    sentiment deltaActivity deltaSentiment\nactivity        1.00000000 -0.068425337   0.620189979     0.06955678\nsentiment      -0.06842534  1.000000000   0.009215771     0.51408474\ndeltaActivity   0.62018998  0.009215771   1.000000000    -0.04522242\ndeltaSentiment  0.06955678  0.514084741  -0.045222415     1.00000000\nBullish        -0.07234842  0.111433821  -0.095796815     0.10398764\nNeutral         0.04440169  0.019445339   0.038646679    -0.01661078\nBearish         0.04285201 -0.111207408   0.067113719    -0.08588212\nSpread         -0.05902646  0.115454510  -0.083795941     0.09801129\n                   Bullish     Neutral     Bearish      Spread\nactivity       -0.07234842  0.04440169  0.04285201 -0.05902646\nsentiment       0.11143382  0.01944534 -0.11120741  0.11545451\ndeltaActivity  -0.09579681  0.03864668  0.06711372 -0.08379594\ndeltaSentiment  0.10398764 -0.01661078 -0.08588212  0.09801129\nBullish         1.00000000 -0.09615506 -0.85871354  0.96041621\nNeutral        -0.09615506  1.00000000 -0.42751147  0.18492522\nBearish        -0.85871354 -0.42751147  1.00000000 -0.96747469\nSpread          0.96041621  0.18492522 -0.96747469  1.00000000\n\n\nThe correlations between weekly investor sentiment polling and investor activity in the following week are displayed in the plot. While many crosstabs are highly correlated, these are only because the variables have been calculated together (i.e. the sentiment polling includes % with positive sentiment and % with negative sentiment, which are perfectly correlated since they are two sides of the same measure). The highest correlation value recorded outside these collinear predictors was ~0.115 for the spread in bullish and bearish opinion vs. sentiment towards top10 stocks.\nThis correlation makes sense, because both measures are capturing the same underlying phenomena: investor sentiment. If retail investor sentiment is bearish about the market in general, then it would likely also be negative about particular stocks.\n\n\nOutliers and Segmentation\nWe already saw in the overtime chart that perhaps 2021 was a different type of year than the other 3, as it had a high percentage of days with new stocks in the top 10.\nLets look at the key outcome variables and see if any values violate Tukey’s rule\n\n\nCode\nboxplot(joinedSentiment$activity)\n\ncutoff &lt;- mean(joinedSentiment$activity) + (1.5 * IQR(joinedSentiment$activity))\n\nabline(h = cutoff, col = \"Red\")\n\n\n\n\n\nWith the red line above denoting the 1.5*IQR + mean threshold, we can see that many values lie above this line, but they do not appear to be separated from the main distribution. Rather, it looks like the main distribution has a fat right tail. Perhaps it would be well approximated by a Student’s distribution.\n\n\nText Data\nLet’s use the TM package to make a cloud of the most popular words in the Reddit posts:\n\n\nCode\n#install.packages(\"tm\")\nlibrary(tm)\nlibrary(wordcloud)\n#Create a vector containing only the text\ntext &lt;- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\ntext &lt;- text$text\n\n# Create a corpus  \ndocs &lt;- Corpus(VectorSource(text))\ndocs &lt;- docs %&gt;%\n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace)\ndocs &lt;- tm_map(docs, content_transformer(tolower))\ndocs &lt;- tm_map(docs, removeWords, stopwords(\"english\"))\n\n\ndtm &lt;- TermDocumentMatrix(docs) \nmatrix &lt;- as.matrix(dtm) \nwords &lt;- sort(rowSums(matrix),decreasing=TRUE) \ndf &lt;- data.frame(word = names(words),freq=words)\n\nset.seed(1234) # for reproducibility \nwordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\nThe wordcloud is interesting: it contains far more formal language than I would have predicted. Stock trading forums online usually discuss a variety of issues with a varying degree of informalism. I believe the narrow and professional vocabulary in the posts may be due to the particular forum I chose, as the r/stocks subreddit is hyperfocused on discussions of particular stocks. I would hypothesize that the vocabulary and most used words would differ greatly between internet forums, even amongst those that are focused only on investing.\nA few notable points from the words displayed: earnings are by far the most common point of discussion, and seem to play an outsize role in why individual investors on the forum are interested in a particular stock. This could indicate than comparing retail investor conversations with earnings events could be a fruitful point of investigation in the future."
  },
  {
    "objectID": "dataexploration/dataexploration.html#findings-from-eda",
    "href": "dataexploration/dataexploration.html#findings-from-eda",
    "title": "Exploratory Data Analysis",
    "section": "Findings from EDA",
    "text": "Findings from EDA\nOverall, we can see that activity scores are somewhat similar to a normal distribution which is censored at 0 and centered around 0.02. Albeit with a taller mean and wider right tail than expected from a normal distribution. Activity scores have steadily risen over time, although 2021 was a particularly substantial year for increases in retail trading activity. We also didn’t see much obvious correlation between our tabular data and outcome variables, leaving it up to question whether we will be able to predict increases and decreases in retail activity with later models. Finally, the textual data does appear to be of good quality, and we notice substantive vocabulary centering on investments."
  },
  {
    "objectID": "conclusions/conclusions.html",
    "href": "conclusions/conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "From the beggining, it proved difficult to gather a breadth of high quality data concerning retail investor trading activity. Even after finding a workable dataset to provide the studied outcome variable, in the Nasdaq data, social media data was elusive. After gathering a passable subset of Reddit posts and a backlog of investor sentiment surveys, it was unclear at the onset of the project if enough meaningful information would be contained in these variables to predict as complicated a phenomena as individual investors’ trading patters. While no one model provided the whole picture, across the four methods studied this project was able to piece together a picture of how investor sentiment online transfers into differences in observed activity and trading patters. Naive Bayes models revealed that information was contained in the Reddit posts’ texts, especially in the 10,000 most popular words in use in the corpus. While the accuracy of the models did not surpass the baseline, the Naive Bayes models which used only textual data were able to obtain a similar accuracy to the models that used only the more traditional, tabular data. Conversely, the dimensionality reduction methods showed that the tabular data contained some extremely significant information. In particular, Principle Component Analysis created a new dimension, which was a combination of the tabular dataset, that valenced each predictor based on whether it corresponded to positive or negative sentiment around a stock. In this case, the negative sentiment was particularly powerful, but across both positive and negative sentiment the new dimension provided a noticeable separation between stocks that would go on to increase or decrease in activity. This means that survey data was indeed a relevant predictor of activity, as were overall stock prices, tabular variables extracted from Reddit such as the comment number, and Nasdaq recorded investor sentiment.\nSynthesizing dimensionality reduction and naive bayes, decision trees demonstrated that meaningful accuracy in predicting increasing and decreasing activity was possible when drawing on both datasets. By using both textual and tabular data, the random forests models showed that enough information was contained in the online discourse of retail investors to make substantive predictions about their future trading activity. All of the decision tree models performed strongly here, indicating the data itself may be of good quality for such analysis regardless of the particular structure of the decision tree in question.\nFinally, outside of these two mdoels, clustering illuminated new structure within the dataset itself. While traditional clustering methods such as K means did not bear fruit, density based clustering was able to identify dense periods in the usuage of certain words over time. These clusters might correspond to the percieved “bandwagoning” effect that is attributed to individual investor forums online, where discussion of particular stocks grows sharply and leads to massive influxes of activity. Future work might start with attempting to better delineate and define these language clusters, such that they could be compared to outcome variables other than activity itself (which did not appear related in this project).\nIn the end, I believe these findings lend support to the idea that retail investors can be influenced by posts and sentiment online, and that this influence is refelcted in their trading behaviors. In conjuction with the finding of clustering in online posts, this project intensifies the notion that retail trading is tempestuous and volatile. Given its importance to financial markets and everyday lives, this makes retail trading worthy of future study, so as to better explain the volatility it introduces into the economy."
  },
  {
    "objectID": "conclusions/conclusions.html#overview-of-project-conclusions",
    "href": "conclusions/conclusions.html#overview-of-project-conclusions",
    "title": "Conclusions",
    "section": "",
    "text": "From the beggining, it proved difficult to gather a breadth of high quality data concerning retail investor trading activity. Even after finding a workable dataset to provide the studied outcome variable, in the Nasdaq data, social media data was elusive. After gathering a passable subset of Reddit posts and a backlog of investor sentiment surveys, it was unclear at the onset of the project if enough meaningful information would be contained in these variables to predict as complicated a phenomena as individual investors’ trading patters. While no one model provided the whole picture, across the four methods studied this project was able to piece together a picture of how investor sentiment online transfers into differences in observed activity and trading patters. Naive Bayes models revealed that information was contained in the Reddit posts’ texts, especially in the 10,000 most popular words in use in the corpus. While the accuracy of the models did not surpass the baseline, the Naive Bayes models which used only textual data were able to obtain a similar accuracy to the models that used only the more traditional, tabular data. Conversely, the dimensionality reduction methods showed that the tabular data contained some extremely significant information. In particular, Principle Component Analysis created a new dimension, which was a combination of the tabular dataset, that valenced each predictor based on whether it corresponded to positive or negative sentiment around a stock. In this case, the negative sentiment was particularly powerful, but across both positive and negative sentiment the new dimension provided a noticeable separation between stocks that would go on to increase or decrease in activity. This means that survey data was indeed a relevant predictor of activity, as were overall stock prices, tabular variables extracted from Reddit such as the comment number, and Nasdaq recorded investor sentiment.\nSynthesizing dimensionality reduction and naive bayes, decision trees demonstrated that meaningful accuracy in predicting increasing and decreasing activity was possible when drawing on both datasets. By using both textual and tabular data, the random forests models showed that enough information was contained in the online discourse of retail investors to make substantive predictions about their future trading activity. All of the decision tree models performed strongly here, indicating the data itself may be of good quality for such analysis regardless of the particular structure of the decision tree in question.\nFinally, outside of these two mdoels, clustering illuminated new structure within the dataset itself. While traditional clustering methods such as K means did not bear fruit, density based clustering was able to identify dense periods in the usuage of certain words over time. These clusters might correspond to the percieved “bandwagoning” effect that is attributed to individual investor forums online, where discussion of particular stocks grows sharply and leads to massive influxes of activity. Future work might start with attempting to better delineate and define these language clusters, such that they could be compared to outcome variables other than activity itself (which did not appear related in this project).\nIn the end, I believe these findings lend support to the idea that retail investors can be influenced by posts and sentiment online, and that this influence is refelcted in their trading behaviors. In conjuction with the finding of clustering in online posts, this project intensifies the notion that retail trading is tempestuous and volatile. Given its importance to financial markets and everyday lives, this makes retail trading worthy of future study, so as to better explain the volatility it introduces into the economy."
  },
  {
    "objectID": "conclusions/conclusions.html#answering-the-research-questions",
    "href": "conclusions/conclusions.html#answering-the-research-questions",
    "title": "Conclusions",
    "section": "Answering the Research Questions",
    "text": "Answering the Research Questions\n\n\n\n\n\n\n\nQuestion\nAnswer\n\n\n\n\n1. To what extent do individual investors’ sentiments and conversations around stock tickers affect changes in trading activity in the post COVID era?\nTo a substantial extent, as we found individual investors’ future activity was related to both sentiment in surveys, as well as reddit textual data.\n\n\n2. Has the impact of online retail investor sentiment changed over time?\nThis project didn’t find differences in the ability of decision trees or naive bayes model to identify increasing and decreasing activity based on sentiment according to the time period. As such, it appears the impact of retail investor sentiment did not change.\n\n\n3. Which social media sites show the most promise for measuring retail investor interest?\nReddit appears to have a wealth of information on retail investors, as this project was only able to use a small subset of posts from a single subreddit, and still was able to draw meaningful inference with regard to retail investors.\n\n\n4. Which types of stocks are most likely to attract retail investor interest?\nSome stocks were always higher in the top 10 by daily activity, such as AAPL (Apple) which was almost always in the top 10 during the period studied. Others tended to have higher retail interest than predicted by models, such as F (Ford).\n\n\n5. To what extent is it possible to predict how long retail investor interest will last in a particular stock?\nThis repord ultimately didn’t have the data to address this question adequately, but it did look tangentially at stocks which were new to the top 10 and found it was not significant in predicting future activity.\n\n\n6. Is positive or negative sentiment more consequential in impacting retail investor activity?\nNegative sentiment was more consistently related to changes in retail activity across the models studied.\n\n\n7. What characteristics of social media posts determine if they will have an impact on retail activity?\nThe major impact discovered in this project would likely be timing, as it studied the impact of posts on stock returns the day after.\n\n\n8. Do surveys of individual investors’ sentiment accurately reflect the sentiment of all retail investors?\nWhile I suspected this would not be the case, survey data was related to changes in retail activity in a variety of models.\n\n\n9. Has the total amount of retail investor activity increased or decreased overtime?\nIncreased, as evidenced in the exploratory data analysis of the retail investor activity dataset.\n\n\n10. Does the language used in social media posts matter for their affect on retail trader activity?\nSpecific words were sometimes found to be relevant predictors in decision tree models, but overall the most specific impact was when language was used repeatedly."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Introduction",
    "section": "",
    "text": "My feature data is based on 2,000+ Reddit posts, which pertained to stocks that were in the top-10 by daily retail trader activity on the day after they were posted. The dataset has 35,403 columns, which is because they represent a vectorized corpus of text data. The texts originally contained about 50,000 plus columns, but the number of columns was filtered down by first lowercasing and stemming the words (removing about 7,000) and then removing words which only appeared one time (removing a further ~9,000 columns).\nIn my analysis, I am hoping to use clustering to study whether there are interesting structures and relationships between the language used in these 2,000 posts. I am hoping that clusters might reflect certain kinds of posts (as there are several archetypes visible on the forum pages), or, further, that the clusters might reveal some previously unseen groupings in the data. These groupings could be findings in themselves, or they could be avenues for future analysis. In addition, I will compare the clusters returned by the various algorithms with my outcome variable (whether the given stock being written about increased or decreased in activity the next day), to understand if the clustering has inadvertently classified the outcome variable in some form."
  },
  {
    "objectID": "clustering/clustering.html#theory",
    "href": "clustering/clustering.html#theory",
    "title": "Introduction",
    "section": "Theory",
    "text": "Theory\n\nK Means Clustering\nK means clustering uses repeated placement of centroids and assignment of nearby points to try and minimize the within-cluster variance (squared distance from the centroid to all points). To begin K means clustering, the algorithm would randomly initialize a given number of centroids, which we refer to as K centroids. Then, the algorithm would assign all points in the space to clusters based on their closest centroid. A real-life analogy to this would be drawing a property line midway between two houses, so that the line sat halfway in the distance between them, and then all points which fell on either side of the property line would be considered in that “houses” cluster. Where the houses are the centroids.\nNext, the algorithm would move the centroids, this time placing them not randomly, but rather at the point which minimizes the squared distance to all of the points in their cluster. Then, the points are reassigned to the nearest centroid, and the process repeats. This process can converge quickly, or fail to converge in some rare cases, but on average it performs reasonably well at generating clusters that minimize the within-cluster squared distances to the centroids. In the property line example, it would be similar to how many houses are on rectangular lots, because the house behind them is further away than the house next door. But after drawing the initial property rectangle, the house would be relocated to the “center” of their property, and then the property lines would be moved so as to remain at the halfway point between each house. This would guarantee that the yards were as close to their respective houses (in squared distance) as possible.\n\n\nHierarchical Clustering\nHierarchical clustering works very differently from K means clustering. In hierarchical clustering, clusters can exist on multiple layers. Rather than a piece of property which must fall squarely in one house’s lot, a datapoint in a hierarchical cluster is more like a person in a family tree, where they could have many layers of other people above them. In this type of clustering, the algorithm can either proceed from the bottom up (forming small groups and then tying them together) or the top down (starting with the broadest categories and then dividing them further). Pretend we are at a family BBQ, and we want to identify the family tree that maps the relationships of all individuals there. We could start with the oldest person, and then trace their descendants, which would be top down hierarchical clustering. But we could also start with the children, identify their parents, and then their parents’ parents, and so on. Both options can be used in practice.\nOne advantage of hierachical clustering is that it does not require the user to input a value for K. Based on the way that the model tries to build the tree, it will ultimately decide upon the number of appropriate clusters itself. Hierarchical clustering can be a very good fit for some kinds of datasets where the desired clusters could be subsets of eachother, like trying to identify species and families from datasets of fossils, or trying to identifying familial groupings in a reunion. Hierarchical clustering can be evaluated by a Silouhette plot, which measures how separated the clusters in the dataset are.\n\n\nDBSCAN\nDBSCAN is a popular clustering method that workds based on density. In other words, it assigns clusters to dense areas with sparse areas in between them. This fundamental principle allows DBSCAN to work on nonlinear clusters, because all that matters is that the algorithm can distinguish an area of low density separating the higher density clusters. DBSCAN clustering is able to find high and low density areas based on the distances between points and their closest neighbors. If you imagine a sparse area of a plot, a given point will have higher average distances to its nearest neighbors than in a tight cluster.\nDBSCAN is useful for many applications, because many types of data exhibit nonlinear behavior. In my application for this dataset, DBSCAN is appealing because there may not be linear patterns in the text data I am investigating, but density in the usage of certain words could still form meaningful clusters. In terms of hyperparameters, DBSCAN also does not need to know the number of clusters ahead of time, which makes it an even more flexible method for a variety of situations. DBSCAN can also be evaluated using Silhouette plots, as the ideal algorithm will have clearly distinct clusters with meaningful distances between them."
  },
  {
    "objectID": "clustering/clustering.html#methods",
    "href": "clustering/clustering.html#methods",
    "title": "Introduction",
    "section": "Methods",
    "text": "Methods\nFirst, I will read in and clean my stock ticker and reddit post text data. This will be done via Jupyter notebook, and common packages such as pandas, matplotlib, and sklearn.\nSecond, I will merge the two datasets together, so that the reddit posts are associated with particular stocks on given days.\nThird, I will filter to only keep the days for which reddit posts were present (the 2000+ observations mentioned above).\nFourth, I remove any columns other than the text data in order to feed purely textual data into the clustering algorithms.\nFifth, I will declare the functions that will fit the various clustering methods and also iterate over possible hyperparameters.\nSixth, I will feed my data into the functions, and review the outputs for each possible hyperparameter value (by reviewing the elbow and silhouette plots).\nSeventh, I will run the final model and view the clustered data along various axes until I ultimately pick one that is a good representation of the clustering results. (You will only see the final selected plots below.)"
  },
  {
    "objectID": "clustering/clustering.html#code-implementation",
    "href": "clustering/clustering.html#code-implementation",
    "title": "Introduction",
    "section": "Code Implementation",
    "text": "Code Implementation\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.cluster\nimport pandas as pd\n\n\n\n\n\nCode\n# read in data, merge using code from naive bayes\ndataIn = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\")\ndataIn.shape\n\ntextsIn = pd.read_csv(\"../data/01-modified-data/vectorizedReddit.csv\")\ntextsIn.shape\n\ns = textsIn.sum(axis=0)\ntextsIn=textsIn[ s.index[s != 1]   ]\ntextsIn.shape\n\nfrom datetime import datetime, timedelta\ndataIn['date.x'] = dataIn['date.x'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\ndataIn['date.x'] = dataIn['date.x'] - timedelta(days = 1)\n\n\n# Join\ntextsIn = pd.DataFrame(textsIn)\ntextsIn['date_utc'] = textsIn['date_utc'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date())\nprocessed = pd.merge(dataIn, textsIn, how = 'left', left_on = ['date.x', 'ticker'], right_on = ['date_utc', 'ticker'] )\n\n\n\n\nCode\nprocessed2 = processed.dropna(subset=['date_utc'])\nprint(processed2.shape)\n\n# exclude nonnumeric data\nexclude = ['Bullish','Spread','SPYHighWk','SPYLowWK','SPYCloseWK','lagweek','date_utc','title','Bearish','Neutral', 'Unnamed: 0', 'X', 'date.x', 'X8.week', 'Total', 'week.y', 'date.y', 'deltaActivity', 'weekyear', '' 'ticker', 'activity_x', 'sentiment_x', 'deltaSentiment', 'newEntry', 'week.x']\n\n\nprocessed2['activityIncrease'] = processed2['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\nprocessed3 = processed2.drop(columns = exclude, axis = 1)\n\nprocessed3.head()\nX = processed3.drop('activityIncrease', axis=1)\ny = processed3['activityIncrease']\n\n\n(2018, 35427)\n\n\nC:\\Users\\corwi\\AppData\\Local\\Temp\\ipykernel_34132\\1696908631.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  processed2['activityIncrease'] = processed2['deltaActivity'].apply(lambda x: int(x &gt; 0) )\n\n\n\n\nCode\n# Clustering code from the labs, used as the start for this page\n\n\n# Define a function to generate 2D clusters with normal distributions.\ndef generate_2D_normal_clusters(points_per_cluster=200, num_cluster=3, correlated=True):\n    # Initialize an empty dictionary to store the generated clusters.\n    sets = {}\n\n    # Loop to generate multiple clusters.\n    for cluster in range(0, num_cluster):\n        # Randomly select the number of points for this cluster within a range.\n\n        # Define parameters for a 2D normal distribution.\n        L = 10  # A scaling factor\n        # Randomly select the mean (center) of the distribution in X and Y.\n        ux = np.random.uniform(-L, L, size=1)[0]\n        uy = np.random.uniform(-L, L, size=1)[0]\n        # Randomly select the standard deviation in X and Y.\n        sx = np.random.uniform(0.1 * L, 0.25 * L, size=1)[0]\n        sy = np.random.uniform(0.1 * L, 0.25 * L, size=1)[0]\n        # Randomly select the correlation between X and Y (Pearson correlation).\n        rho = np.random.uniform(0.0, 0.99, size=1)[0]\n\n        # Create the mean vector and covariance matrix for the distribution.\n        u = np.array([ux, uy])\n        S = np.array([[sx**2.0, rho * sy * sx], [rho * sy * sx, sy**2.0]])\n\n        # If 'correlated' is False, set the off-diagonal elements of the covariance matrix to 0.\n        if correlated == False:\n            S[0, 1] = 0\n            S[1, 0] = 0\n\n        # Generate points from the multivariate normal distribution defined by 'u' and 'S'.\n        x1, x2 = np.random.multivariate_normal(u, S, points_per_cluster).T\n\n        # Create or concatenate the data and labels for the clusters.\n        if cluster == 0:\n            X = np.array([x1, x2]).T  # Data\n            y = cluster * np.ones(points_per_cluster)  # Labels\n        else:\n            X = np.concatenate((X, np.array([x1, x2]).T), axis=0)\n            y = np.concatenate((y, cluster * np.ones(points_per_cluster)), axis=0)\n\n    # Return the generated data (X) and labels (y).\n    return X, y\n\n# Generate the data for machine learning.\nnum_clusters_real = 1 + int(7 * np.random.uniform(0, 1))  # Determine the number of clusters.\nprint(num_clusters_real)\n\n\n# Print the shape of the generated data and labels.\nprint(X.shape, y.shape)\n\n# Make sure that the data is stored as a contiguous array for efficient memory access.\nX = np.ascontiguousarray(X)\n\n\n3\n(2018, 35402) (2018,)\n\n\n\n\nCode\n# helper function to visualize clusters in a given dataset, used as an example here \ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(processed2.loc[:,'deltaActivity'], processed2.loc[:,'date_utc'],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    plt.xlabel('Daily Change in Retail Investor Activity')\n    plt.ylabel('Date')\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n\nplot(X,y)\n\n\n\n\n\n\n\nCode\n# THIS WILL ITERATE OVER ONE HYPER-PARAMETER (GRID SEARCH)\n# AND RETURN THE CLUSTER RESULT THAT OPTIMIZES THE SILHOUETTE SCORE\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.25*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue\n\n        if(i_print): print(param,sil_scores[-1])\n\n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=10, i_plot=True)\n\n\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nc:\\Users\\corwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\nCode\nplot(X,opt_labels)\n\n\n\n\n\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"ag\",nmax=4, i_plot=True)\nplot(X,opt_labels)\n\n\nOPTIMAL PARAMETER = 2\n\n\n\n\n\n\n\n\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"dbscan\",nmax=10, i_plot=True)\nplot(X,opt_labels)\n\n\nOPTIMAL PARAMETER = 0.25"
  },
  {
    "objectID": "clustering/clustering.html#hyper-parameter-tuning-and-results",
    "href": "clustering/clustering.html#hyper-parameter-tuning-and-results",
    "title": "Introduction",
    "section": "Hyper Parameter Tuning and Results",
    "text": "Hyper Parameter Tuning and Results\nAcross the models, hyper parameter tuning was accomplished with silhouette plots, where I looked for the maximum silhouette value for each model. Hierarchical clustering struggled both computationally and in its performance. While the ideal hyperparameter in the silhouette plot was 2 for the maximum value of N, the agglomerative clustering did not produce meaningful clusters no matter what axes I viewed them on. I believe this may be because the structure of the data prevented the hierarchical approach from working well, given that I have many points which could not be related to any other points given that they might have had totally unique language and thus total uniqueness in the dataset. While the other methods simply grouped most points together and identified ones which stood out from the pack, perhaps hierarchical clustering could not accomplish the same feat and struggled to accomodate the many virtually patternless points.\nDBSAN did not change in its performance regardless of the hyperparameter selected. This is expected as the hyperparameter is not the same as the other two models, because DBSAN does not require us to input the number of clusters. However, DBSAN did reveal interesting features in the data. Plotting the clusters vs. the change in retail activity on the X axis and time on the Y axis (chart above) showed that DBSAN had identified small clusters that were very closely related in time. I believe the reason for this is that DBSAN creates clusters based on density, and given that our feature space is the language used in reddit posts, this means that DBSAN has identified clusters of similar posts throughout the data. These posts actually make intuitive sense, because reddits sometimes work like echo chambers, where particular stocks such as GAMESTOP or AMC become very popular to discuss and trade for a small period of time. This would lead posts from this period to become more similar, as they are discussing the same point, which then makes the clustering algorithm identify them together. When viewed over time, we then see that the clusters have selected small points in time, due to the way the data was generated. DBSAN revealed this pattern in the data, which I found quite interesting.\nThe silhouette plot for K-means was very clear: The value decreased for each additional mean that was added to the model. As such, the smallest value, with only 2 clusters, appeared to be the ideal choice for the data. But upon reviewing the clusters that were returned along many axes, there did not appear to be a meaningful interpretation of the data they represented, even with the ideal K value. This was mainly because one cluster contained almost all of the data in the dataset, which meant that the secondary cluster did not yield many interesting results, and was also largely impossible to pick out in any of the charts. I believe this shows that the data did not lend itself to linear separability, as DBSAN performed much better than Kmeans at identifying interesting features."
  },
  {
    "objectID": "clustering/clustering.html#conclusions",
    "href": "clustering/clustering.html#conclusions",
    "title": "Introduction",
    "section": "Conclusions",
    "text": "Conclusions\nOut of the three clustering methods, DBSCAN provided the most interesting results for my study. In particular, the DBSCAN plot showed stratified clusters of points within the timeline of the data. I believe these could correspond to events or moments of excitement about particular stocks, which led to repeated posts using similar language, which in turn made those posts seem more similar to the clustering algorithm. Because DBSCAN focuses on areas of relative density and sparseness, it was able to identify these small moments within the larger dataset. In contrast, K means might have been confused by nonlinearity, or the relative rarity of these datapoints within the corpus."
  },
  {
    "objectID": "clustering/clustering.html#references",
    "href": "clustering/clustering.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\nNo outside works used that need to be cited. Lab code was used for the clustering functions."
  },
  {
    "objectID": "datacleaning/datacleaning.html",
    "href": "datacleaning/datacleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "The most important structuring element when cleaning the datasets and tying them together is the timing of the data. The outcome variable, retail activity, is collected daily, but only on days where the stock market is open. The first input dataset, the investor sentiment survey, is collected at the end of every week about the following week. And the Reddit posts are scraped from the website with a date, but it is unknown when in the day the post was made. Ideally, if we were trying to predict retail activity on a particular day, such as a Tuesday, we would look at the survey data from the previous friday, but the activity and Reddit data from the proceeding Monday. If the Monday were a federal holiday when the market was closed, then we would look at the Reddit and activity data from the previous Friday. In cleaning the data, I chose to address this challenge at the inital stage, by moving the outcome data back a day so that all future references to the activity increase/decrease variable can be made without needing to adjust for a delay."
  },
  {
    "objectID": "datacleaning/datacleaning.html#structuring-data-to-investigate-retail-activity",
    "href": "datacleaning/datacleaning.html#structuring-data-to-investigate-retail-activity",
    "title": "Data Cleaning",
    "section": "",
    "text": "The most important structuring element when cleaning the datasets and tying them together is the timing of the data. The outcome variable, retail activity, is collected daily, but only on days where the stock market is open. The first input dataset, the investor sentiment survey, is collected at the end of every week about the following week. And the Reddit posts are scraped from the website with a date, but it is unknown when in the day the post was made. Ideally, if we were trying to predict retail activity on a particular day, such as a Tuesday, we would look at the survey data from the previous friday, but the activity and Reddit data from the proceeding Monday. If the Monday were a federal holiday when the market was closed, then we would look at the Reddit and activity data from the previous Friday. In cleaning the data, I chose to address this challenge at the inital stage, by moving the outcome data back a day so that all future references to the activity increase/decrease variable can be made without needing to adjust for a delay."
  },
  {
    "objectID": "datacleaning/datacleaning.html#retail-activity",
    "href": "datacleaning/datacleaning.html#retail-activity",
    "title": "Data Cleaning",
    "section": "Retail Activity",
    "text": "Retail Activity\nClean Record Data in R\n\nView Raw Data\nView Clean Data\n\nCleaning the retail activity record data meant:\n\nReading in the data nad ordering by date (10 observations per day)\nCleaning up various columns to ensure they were stored in the correct data format\nDividing the data up by ticker, so that each stock can be compared against itself\nLooking at the next-most-recent day for each ticker, and subtracting that from today’s total to get the difference\nRepeating the operation to get the change in sentiment\nFlag the ticker as new to the top 10 if no immediately proceeding date was found\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(reticulate)\n\ndata &lt;- read.csv('../data/00-raw-data/ndaqRT.csv')\n\n# filter to the desired year\ncleanData &lt;- data %&gt;% \n    mutate(date = ymd(date)) %&gt;%\n    filter(year(date) &gt; 2019)\n\n# adjust data types as needed\ncleanData &lt;- cleanData[order(cleanData$date),]\ncleanData$deltaActivity &lt;- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$deltaSentiment &lt;- vector(mode = \"numeric\", length = nrow(cleanData))\ncleanData$newEntry &lt;- vector(mode = 'logical', length = nrow(cleanData))\n\n\n# Looping through all entries in the dataset, generate the daily change in activity and lag the result by one day.\nfor(i in 1:nrow(cleanData)) {\n    if(i %% 1000 == 0) {\n        print(i)\n    }\n\n    # Make sure that we are comparing each stock against itself day over day\n    prevDay &lt;- cleanData %&gt;%\n        filter(day(date) == day(cleanData$date[i]) - 1) %&gt;%\n        filter(ticker == cleanData$ticker[i])\n\n    if(nrow(prevDay) &gt; 0) {\n\n        # calculate the day over day change and whether the stock is new to the top 10\n        cleanData$deltaActivity[i] = cleanData$activity[i] - prevDay$activity[1]\n        cleanData$deltaSentiment[i] = cleanData$sentiment[i] - prevDay$sentiment[1]\n        cleanData$newEntry[i] = FALSE\n    \n    }else {\n        cleanData$newEntry[i] = TRUE\n    }\n}\n\ncolnames(cleanData)\nhist(cleanData$deltaActivity)\n\nhead(cleanData, 100)"
  },
  {
    "objectID": "datacleaning/datacleaning.html#investor-sentiment",
    "href": "datacleaning/datacleaning.html#investor-sentiment",
    "title": "Data Cleaning",
    "section": "Investor Sentiment",
    "text": "Investor Sentiment\n\nView raw data\nView clean data\n\nGathering the investor sentiment data was easy, the challenging part was merging it with the daily retail trading activity, as the investor sentiment data was weekly. To properly join the data, I needed to have the investor sentiment survey apply to the previous week’s observations. I did this by:\n\nReading in the data and ensuring the date of the survey was stored in proper format.\nCalculating the week of the year that each survey was taken on.\nAdding one to the week, resetting past 53, and joining it with the year to form a unique week-year identifier.\nJoining this identifier with the week-year of the activity data as needed.\n\n\n\nCode\n#redditIn &lt;- read.xlsx(\"../data/01-modified-data/sampleRedditText.xlsx\", sheetName = \"Sheet1\")\nsentimentIn &lt;- read.csv(\"../data/00-raw-data/sentiment_aaii.csv\")\nrtatIn &lt;- read.csv(\"../data/01-modified-data/cleanRTAT.csv\")\n\n\n# Ensure dates can be joined\ncolnames(sentimentIn)[1] &lt;- \"date\"\n\nsentimentIn &lt;- sentimentIn |&gt;\n    mutate(date = mdy(date)) |&gt;\n    mutate(week = week(date)) |&gt;\n    mutate(lagweek = week + 1) |&gt;\n    mutate(weekyear = paste(year(date), lagweek )) |&gt;\n    mutate(Bullish = as.numeric(str_replace(Bullish, \"%\", \"\"))/100 ) |&gt;\n    mutate(Neutral = as.numeric(str_replace(Neutral, \"%\", \"\"))/100 ) |&gt;\n    mutate(Spread = as.numeric(str_replace(Spread, \"%\", \"\"))/100 ) |&gt;\n    mutate(Bearish = as.numeric(str_replace(Bearish, \"%\", \"\"))/100 ) \n\n\nrtatIn &lt;- rtatIn %&gt;%\n    mutate(date = ymd(date)) %&gt;%\n    mutate(week = week(date)) %&gt;%\n    mutate(weekyear = paste(year(date), week))\n\n#head(rtatIn)\n\n\n#joinedSentiment &lt;- left_join(rtatIn, sentimentIn, by = \"weekyear\")"
  },
  {
    "objectID": "datacleaning/datacleaning.html#reddit-text-corpus-in-python",
    "href": "datacleaning/datacleaning.html#reddit-text-corpus-in-python",
    "title": "Data Cleaning",
    "section": "Reddit Text Corpus in Python",
    "text": "Reddit Text Corpus in Python\n\nView Original Data\nView Cleaned Data\n\nReddit forums dedicated to the stock market have millions of users, and organized impact on markets The Reddit API promises to allow text scraping. However, it only allowed access to a small subset of posts, and timed out after a few hundred requests. Chaining many request together, and targeting the most active posts, I gathered 7,463 Reddit posts from the studied time period on r/stocks. Once vectorized, these posts contained around 56,000 unique words, reduced by about 7,000 through stemming. Then, I filtered out words/columns which had only appeared once in the dataset, and this ultimately reduced the size of the text data down to around 35,000 columns.\nCleaning the data meant:\n\nStarting from raw text data, which was largely contained in the title of the Reddit posts\nSplit the text column into individual words and apply a stemmer\nVectorize the resulting column\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\nimport os\nimport subprocess\nimport sys\nfrom nltk.stem import PorterStemmer\n\n\n#subprocess.call([sys.executable, '-m', 'pip', 'install', 'nltk'])\n\nos.getcwd()\n\nredditTexts = pd.read_excel(\"../data/01-modified-data/sampleRedditText.xlsx\", sheet_name=\"Sheet1\")\n\nredditTexts.head()\n#positiveLingo &lt;- c(\"call\", \"calls\", \"long\", \"hold\", \"buy\", \"bull\", \"bullish\", \"bulls\", \"support\", \"strong\")\n#negativeLingo &lt;- c(\"put\", \"puts\", \"short\", \"shorters\", \"short-sellers\", \"sell\", \"sellers\", \"bear\", \"bears\", \"bearish\", \"weakness\", \"weak\")\n\nremoveNAtexts = redditTexts.dropna(subset = ['text'])\n\n# initailize stemmer\nstemmer = PorterStemmer()\n\n# split strings into individual words\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: x.split())\n\n# stem the words\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: [stemmer.stem(y) for y in x]) \n# join the words back together\nremoveNAtexts[\"text\"] = removeNAtexts[\"text\"].apply(lambda x: ' '.join(x)) \n\n\n# use count vectorizer to combine the data\nvectorizer = CountVectorizer()\nXs  =  vectorizer.fit_transform(removeNAtexts[\"text\"])   \n\ncol_names=vectorizer.get_feature_names_out()\n\n# take only the needed columns\nsmallReddit = removeNAtexts[[\"date_utc\", \"title\", \"ticker\", \"comments\"]]\n\n#smallReddit.index\n\nb = pd.DataFrame(Xs.toarray(), columns=col_names, index= smallReddit.index)\n\nredditCV = pd.concat([smallReddit, b], axis=1)\n\n#redditCV.head()\n\n\n#redditCV.to_csv('./data/01-modified-data/vectorizedReddit.csv', index=False)\n#print(df)"
  },
  {
    "objectID": "datagathering/datagathering.html",
    "href": "datagathering/datagathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "While retail investor trading activity in individual companies has aroused great amounts of interest in the academic literature, practical methods for accessing this data are surprisingly limited. Even measuring which trades are truly by retail investors is a disputed matter that raises questions about the validity of any study into the subject. In this regard, the paper by Boehmer, Jones, Zhang, and Zhan (Boehmer et al. 2020) is widely referenced and considered the “gold standard” in identifying retail trades. The authors make use of a strategy based on the tenths of a cent that are involved in a trade, alleging that trades with particular trailing digits can be flagged as made by individual investors. I pursued this strategy initially but found two problems: First, the accuracy of the method itself is disputed, with another study finding it only identified 35% of verified retail trades they placed on different accounts (Barber et al. 2023). Second, the process of gathering the orimary data needed for the method from government disclosures is difficult in itself, and no third parties appeared to offer data generated with exactly this method.\nAfter reviewing many financial databases, I ultimately identified a new data product offered by Nasdaq’s data link service, which was termed the “Retail Trading Activity Tracker” and provided by The Applied Research Company. This provided two essential metrics: the porportion by dollar value of the stock’s daily volume that was traded by retail investors, and the relative ranking of each stock by this proportion. I believed this data to be a high standard due to its valuation as a purchasable product, and its promotion by a prominent stock market actor in Nasdaq. Unfortunately, the full dataset required paid access, but I was able to access the top-10 stocks which had the highest retail trader activity proportion each day. By looking at the available proportion of activity, and whether each stock remained in the top-10 day over day, I was able to construct a daily binary outcome from whether each ticker increased or decreased in proportionate retail trader activity from the day before.\n\n\n\nThe next question was how to gather meaningful inputs to compare with retail activity. My research question was centered on social media data and sentiment towards financial instruments, and I intended to gather data from online communities know for discussing investments such as Reddit, Stocktwits, and Robinhood. While past studies had found a wealth of data on these subjects, I quickly realized that this data had recently become less accessible. Since roughly the start of 2020:\n\nRobinhood had shut down its API completely, which also closed associated datasources such as Robintrack which were used in many research papers.\nReddit had begun charging for access to its API, which it had previously offered for free.\nStocktwits had also shut down its API completely, and and associated 3rd party websites called Stocksera which had collected the data appeared to go offline during the project\n\nI mitigated these shortages by finding two datasets, one tabular and one textual, that contained information about retail traders’ sentiment and social media activity. For the tabular data, I found the American Association of Individual Investors conducts a weekly survey of its members’ sentiment about the market in the upcoming week (the three options being bullish, bearish, or neutral). For the textual data, I was able to use the Reddit API to make a limited number of free queries, which I used to gather the most popular posts from the largest stock trading forum. While I had originally hoped to gather an exhaustive list of posts from a variety of investment-focused forums, the limited dataset of around ~900 posts contained at least 2000 references to top-10 stocks in the desired time period, enough coverage for a meaningful investigation."
  },
  {
    "objectID": "datagathering/datagathering.html#introduction",
    "href": "datagathering/datagathering.html#introduction",
    "title": "Data Gathering",
    "section": "",
    "text": "While retail investor trading activity in individual companies has aroused great amounts of interest in the academic literature, practical methods for accessing this data are surprisingly limited. Even measuring which trades are truly by retail investors is a disputed matter that raises questions about the validity of any study into the subject. In this regard, the paper by Boehmer, Jones, Zhang, and Zhan (Boehmer et al. 2020) is widely referenced and considered the “gold standard” in identifying retail trades. The authors make use of a strategy based on the tenths of a cent that are involved in a trade, alleging that trades with particular trailing digits can be flagged as made by individual investors. I pursued this strategy initially but found two problems: First, the accuracy of the method itself is disputed, with another study finding it only identified 35% of verified retail trades they placed on different accounts (Barber et al. 2023). Second, the process of gathering the orimary data needed for the method from government disclosures is difficult in itself, and no third parties appeared to offer data generated with exactly this method.\nAfter reviewing many financial databases, I ultimately identified a new data product offered by Nasdaq’s data link service, which was termed the “Retail Trading Activity Tracker” and provided by The Applied Research Company. This provided two essential metrics: the porportion by dollar value of the stock’s daily volume that was traded by retail investors, and the relative ranking of each stock by this proportion. I believed this data to be a high standard due to its valuation as a purchasable product, and its promotion by a prominent stock market actor in Nasdaq. Unfortunately, the full dataset required paid access, but I was able to access the top-10 stocks which had the highest retail trader activity proportion each day. By looking at the available proportion of activity, and whether each stock remained in the top-10 day over day, I was able to construct a daily binary outcome from whether each ticker increased or decreased in proportionate retail trader activity from the day before.\n\n\n\nThe next question was how to gather meaningful inputs to compare with retail activity. My research question was centered on social media data and sentiment towards financial instruments, and I intended to gather data from online communities know for discussing investments such as Reddit, Stocktwits, and Robinhood. While past studies had found a wealth of data on these subjects, I quickly realized that this data had recently become less accessible. Since roughly the start of 2020:\n\nRobinhood had shut down its API completely, which also closed associated datasources such as Robintrack which were used in many research papers.\nReddit had begun charging for access to its API, which it had previously offered for free.\nStocktwits had also shut down its API completely, and and associated 3rd party websites called Stocksera which had collected the data appeared to go offline during the project\n\nI mitigated these shortages by finding two datasets, one tabular and one textual, that contained information about retail traders’ sentiment and social media activity. For the tabular data, I found the American Association of Individual Investors conducts a weekly survey of its members’ sentiment about the market in the upcoming week (the three options being bullish, bearish, or neutral). For the textual data, I was able to use the Reddit API to make a limited number of free queries, which I used to gather the most popular posts from the largest stock trading forum. While I had originally hoped to gather an exhaustive list of posts from a variety of investment-focused forums, the limited dataset of around ~900 posts contained at least 2000 references to top-10 stocks in the desired time period, enough coverage for a meaningful investigation."
  },
  {
    "objectID": "datagathering/datagathering.html#methods",
    "href": "datagathering/datagathering.html#methods",
    "title": "Data Gathering",
    "section": "Methods",
    "text": "Methods\nReviewing the methods nessecary for gathering the datasets mentioned above: 1. Daily NASDAQ data on the top-10 retail investor held companies (Python API through Quandl Package, R code for the equivalent package also listed below) 2. Text data from popular stock-trading subreddits (Reddit API through R and RedditExtractoR package) 3. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV)\n\nDataset 1: Nasdaq Retail Investor Activity\n\nView raw data\nView clean data\n\nThe Nasdaq API had a lengthy signup process, and required I generate an API key. After receiving my key, I was able to use packages in either R or Python that made use of the Quandly api to query Nasdaq’s data. I did this in both Python and R, for good measure, and got the same result. The R version is below.\n\n\nCode\nlibrary(tidyverse)\nlibrary(Quandl)\nlibrary(lubridate)\nlibrary(RedditExtractoR)\nlibrary(reticulate)\nlibrary(xlsx)\n# Here I loaded in my API key and queried the data (API Key hidden in this version of the document)\nQuandl.api_key(\"NNNNNNNNNNNN\")\n#data &lt;- Quandl.datatable(\"NDAQ/RTAT10\", paginate = TRUE)\n\n\nThe Python version of the NASDAQ api ultimately yielded the data I used in my project. The code below has been commented out because it was initially run in a separate, Python notebook. But you can review the simplicity of requesting the retail activity data below, which only took a few lines. I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.\n\nPython Code\n\n\nimport pandas as pd\n\n\nimport numpy as np\n\n\nimport quandl\n\n\nimport nasdaqdatalink\n\n\nquandl.ApiConfig.api_key = ‘NNNNNNNNNNNNNNN’\n\n\ntable1 = quandl.get_table(‘NDAQ/RTAT10’, date=‘2023-09-28,2023-09-27,2023-09-26’, ticker=‘TSLA,TQQQ,SQQQ’)\n\n\nEnd Python Code\n\n\n\nDataset 2: Text Data from Reddit\n\nView raw data\nView clean data\n\nInitially, I planned to get a particular stock’s mentions in titles in each of the 7 major subreddits dedicated to investing. I had originally created this set of forums based on measuring their overlap with known investment subreddits through user overlap See tool here. However, due to API changes I was not able to loop through all posts from these subreddits, and instead was limited to taking popular posts from a single forum. For this purpose, I chose the page “stocks” as it is dedicated solely to the discussion of particular stocks and not other topics. I used an R package to query Reddit’s API (in the small amounts that were allowed), until I had a reasonable amount of data to proceed in the project, although far less than I had hoped.\n\n\nCode\n#install.packages(\"RedditExtractoR\", repos='https://cloud.r-project.org/')\n\n# The original list of 7 subreddits I planned to use\nstockSubreddits &lt;- c(\n    \"wallstreetbets\",\n    \"stocks\",\n    \"options\",\n    \"investing\",\n    \"stockamrket\",\n    \"superstonk\",\n    \"wallstreetbetsnew\"\n)\n\n# The function which initially flags the URLS of posts that contain a given string in the title\nlinks &lt;- find_thread_urls(keywords = \"QQQ\", subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n\n# Testing formatting with a single example\nlinks &lt;- tibble(links)\nlinks$ticker = \"QQQ\"\n\n# Getting all unique stocks in the dataset of top retail-traded securities\ntempClean &lt;- read.csv('../data/01-modified-data/cleanRTAT.csv')\n\n# looking for the top 100 most popular stocks by retail activity, to find their relevant reddit posts (limited from doing all stocks by API bandwidth)\ntopStocks &lt;- sort(table(tempClean$ticker), decreasing = TRUE)[1:100]\n\nuqTickers &lt;- rownames(topStocks)\n\nprint(uqTickers)\n\n# Looping through the common top-10 stocks and pulling any top threads which referenced them in the title\nfor(i in 1:length(uqTickers)) {\n    print(i / nrow(uqTickers))\n    \n    templinks &lt;- find_thread_urls(keywords = uqTickers[i], subreddit = \"stocks\", sort_by = \"top\", period = \"all\")\n    templinks$ticker &lt;- uqTickers[i]\n    links &lt;- rbind(links, templinks)\n}\n\n#write.xlsx(links,\"../data/01-modified-data/sampleRedditText.xlsx\")\n\n\n\n\nDataset 3: AAII Investor Sentiment\n\nView raw data\nView clean data\n\nFinally, the dataset for individual investors’ weekly sentiment was readily available as a CSV on the AAII’s website. I downloaded this and read it into the file.\n\n\nCode\n# AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results\n# read in as csv \n\naaWeekly &lt;- read.csv(\"./data/00-raw-data/sentiment_aaii.csv\")\nhead(aaWeekly)"
  },
  {
    "objectID": "dimreduction/dimreduction.html",
    "href": "dimreduction/dimreduction.html",
    "title": "Dimensionality Reduction: Introduction",
    "section": "",
    "text": "The objective of this project is to reduce the tabular data’s dimensionality, breaking down the range of disparate values into vectors which combine the meaninfgul signal contained in the data sources I have aggregated. This can enable faster processing of the data in other analyses such as decision trees or regression models, and it will also enable better performance by excluding noise from the data where possible. In terms of tools: I will use Python’s scikit learn and its sub-package scikit learn.metrics, particularly the PCA and tSNE methods, to analyze the data and attempt dimensionality reduction.\nThe record dataset I will be analyzing for dimensionality reduction includes:\n1 - The daily level of retail activity for the top 10 most active stock tickers 2 - The daily change in the level of retail activity for the top 10 most active stock tickers 3 - Daily individual investor survey data (columns for the percent of respondents that were bearish, bullish, or neutral) 4 - Daily price of the S&P 500\nThe dataset described above was selected by using all of the available record data that I gathered for this project. This is because data pertaining to retail investor sentiment and trading is exceptionally scarce online, and so it makes sense to use all available data for topics such as dimensionality reduction. Text data was not used due to its fundamental differences from the record data, and computational requirements given the 10,000 columns."
  },
  {
    "objectID": "dimreduction/dimreduction.html#project-proposal",
    "href": "dimreduction/dimreduction.html#project-proposal",
    "title": "Dimensionality Reduction: Introduction",
    "section": "",
    "text": "The objective of this project is to reduce the tabular data’s dimensionality, breaking down the range of disparate values into vectors which combine the meaninfgul signal contained in the data sources I have aggregated. This can enable faster processing of the data in other analyses such as decision trees or regression models, and it will also enable better performance by excluding noise from the data where possible. In terms of tools: I will use Python’s scikit learn and its sub-package scikit learn.metrics, particularly the PCA and tSNE methods, to analyze the data and attempt dimensionality reduction.\nThe record dataset I will be analyzing for dimensionality reduction includes:\n1 - The daily level of retail activity for the top 10 most active stock tickers 2 - The daily change in the level of retail activity for the top 10 most active stock tickers 3 - Daily individual investor survey data (columns for the percent of respondents that were bearish, bullish, or neutral) 4 - Daily price of the S&P 500\nThe dataset described above was selected by using all of the available record data that I gathered for this project. This is because data pertaining to retail investor sentiment and trading is exceptionally scarce online, and so it makes sense to use all available data for topics such as dimensionality reduction. Text data was not used due to its fundamental differences from the record data, and computational requirements given the 10,000 columns."
  },
  {
    "objectID": "dimreduction/dimreduction.html#code-implementation",
    "href": "dimreduction/dimreduction.html#code-implementation",
    "title": "Dimensionality Reduction: Introduction",
    "section": "Code Implementation:",
    "text": "Code Implementation:\n\n\nCode\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport pandas as pd\nimport seaborn as sns\n\n\nimport os\nos.getcwd()\n\n\n\n'c:\\\\Users\\\\corwi\\\\Documents\\\\dsan-5000-project-corwindark\\\\dsan-website\\\\5000-website\\\\dimreduction'\n\n\n\n\nCode\n# Read data in:\ntabData = pd.read_csv(\"../data/01-modified-data/joinedSentiment.csv\") \ntabData.shape\n\n\n(9560, 23)\n\n\n\n\nCode\n# Prep data for PCA and T-SME\nnonNumericColumns = ['X', \"Unnamed: 0\", 'week.y', 'lagweek', 'sentiment',  'date.x', 'Total', 'X8.week', 'date.x', 'ticker', 'date.y', 'deltaSentiment',  'week.x', 'weekyear', 'newEntry']\n\n# Drop non-numeric columns\nx = tabData.drop(columns = nonNumericColumns)\nx = x.dropna()\n\n# save an early copy of the dataset to reference later\nxsave = x\n\n# Drop the column which created the outcome feature\nx = x.drop(columns = ['deltaActivity'])\n\n# Clean the data from the survey which included commas, so it can be turned into float data\nx.SPYCloseWK = x.SPYCloseWK.apply(lambda x: float(x.replace(\",\", \"\") ))\nx.SPYHighWk = x.SPYHighWk.apply(lambda x: float(x.replace(\",\", \"\") ))\nx.SPYLowWK = x.SPYLowWK.apply(lambda x: float(x.replace(\",\", \"\") ))\n\n\n\n\nCode\n# Implement PCA, with 3 components\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(x)\nprint('\\nPCA')\nprint(pca.components_)\n\nprint(pca.explained_variance_ratio_ * 100)\n\n\n\nPCA\n[[-1.68248660e-06 -9.38301381e-06 -2.92182977e-05  3.85961909e-05\n  -4.79948736e-05 -5.59590301e-01 -5.97065315e-01 -5.74779697e-01]\n [ 1.02733629e-05 -6.50141479e-04 -2.83986205e-04  9.34058655e-04\n  -1.58436440e-03  7.83508069e-01 -6.07178267e-01 -1.32082442e-01]\n [-3.33404703e-06 -4.95640335e-05 -6.83867804e-05  1.18117062e-04\n  -1.67786032e-04 -2.70132741e-01 -5.24257322e-01  8.07578178e-01]]\n[99.45726907  0.34201376  0.20071211]\n\n\nAfter running an initial PCA model with three components, we can see that the first component explains more than 99% of the variance. In the interesting of avoiding overcomplication, let’s rerun the PCA with only two components and see if the performance is similar.\n\n\nCode\n# Given that the first component explains so much of the variance, let's do PCA with only 2 components\npca = PCA(n_components=2)\npca.fit(x)\nprint('\\nPCA')\nprint(pca.components_)\n\nxpca = pca.fit_transform(x)\n\nprint(pca.explained_variance_ratio_ * 100)\n\n\n\nPCA\n[[-1.68248660e-06 -9.38301381e-06 -2.92182977e-05  3.85961909e-05\n  -4.79948736e-05 -5.59590301e-01 -5.97065315e-01 -5.74779697e-01]\n [ 1.02733629e-05 -6.50141479e-04 -2.83986205e-04  9.34058655e-04\n  -1.58436440e-03  7.83508069e-01 -6.07178267e-01 -1.32082442e-01]]\n[99.45726907  0.34201376]\n\n\nThe first variable still explains equally as much in the second run of PCA, but now we have only two dimensions to deal with, which still provide similar explanatory power of the variance in the data.\n\n\nCode\n# Implement TSNE\nfrom sklearn.manifold import TSNE\n\n# get the KL divergence\ntsne = TSNE(n_components=3, random_state=21)\nX_tsne = tsne.fit_transform(x)\n#tsne.kl_divergence_\n\n\n-3.659614086151123\n\n\n\n\nCode\n# Parameter tuning for TSNE\nkl_divergences =  []  \n# first val is perplexity, second is KL divergence\n\n# Iterate through possible perplexity values, and calculate the KL divergence from each one\nfor i in range(1,10):\n    #print(i)\n    tsne = TSNE(n_components=2, perplexity= i, random_state=21)\n    X_tsne = tsne.fit_transform(x)\n    kl = tsne.kl_divergence_\n    kl_divergences.append([ [i], [kl] ])\n\n#print(kl_divergences)\n\n\n\n\nCode\nkls = pd.DataFrame(kl_divergences)\n#print(kls.shape)\nkls.columns = ['Perplexity', 'KL_Divergence']\n#print(kls.head())\nkls.Perplexity = kls.Perplexity.apply(lambda x: x[0])\nkls.KL_Divergence = kls.KL_Divergence.apply(lambda x: x[0])\n\n#print(kls.head())\n#kls = pd.DataFrame(kl_divergences, columns = ['Perplexity', 'KL_Divergence']) \nsns.lineplot(kls, x = 'Perplexity', y = 'KL_Divergence').set(title = \"KL Divergence of TNSE with Differing Perplexity Values\")\n\n\n\n\n[Text(0.5, 1.0, 'KL Divergence of TNSE with Differing Perplexity Values')]\n\n\n\n\n\nFrom looking at the plot of perplexity versus KL divergence, it appears that the lower values of perplexity performed better and yielded a KL divergence closer to 0. As such, I would use perplexity = 1 for this model."
  },
  {
    "objectID": "dimreduction/dimreduction.html#project-report",
    "href": "dimreduction/dimreduction.html#project-report",
    "title": "Dimensionality Reduction: Introduction",
    "section": "Project Report",
    "text": "Project Report\nLet’s start with the results of the principle component analysis:\n\n\nCode\n# Take the saved data from before to make datasets split by increase/decrease in activity, for the plots below\nincreaseActivity = (xsave['deltaActivity'] &gt; 0)\nincreaseActivity.head()\n\n#plt.hist(xpca[:,0], hue=increaseActivity)\n#plt.show()\n\n\n# Create histograms along the 1st principal component\nplt.hist(xpca[increaseActivity,0],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(xpca[~(increaseActivity),0],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \nplt.title(\"Distribution of Increased/Decreased Activity Across the First Component\")\nplt.legend()\nplt.show()\n\n# Create histograms along the 2nd principal component\nplt.hist(xpca[increaseActivity,1],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(xpca[~(increaseActivity),1],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \n\nplt.title(\"Distribution of Increased/Decreased Activity Across the Second Component\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Run ttest on the two populations from above along the new dimension\nstats.ttest_ind(xpca[increaseActivity,1], xpca[~(increaseActivity),1] )\n\n\nTtestResult(statistic=8.740053737340556, pvalue=2.7359772711515378e-18, df=9268.0)\n\n\nAs seen above, the PCA method produces principal components which are somewhat good at splitting our data into the classes we care about. There is a noticeable difference between the classes in the first principle component especially, where higher values had more decreases in activity, and lower values had less. This should be enough on its own to outperform a simple random classifier. Indeed, a T-Test where the two samples are the two possible values of the outcome variable (increase/decrease), and the means were of the values of the first principle component for each ovservation, returned a p value close to zero. This demonstrates the statistically significant difference along the new axes.\nInterestingly, of the 8 variables included in the tabular data, the first axis had negative coefficients for seven of them. This is particularly surprising because the first principle component also explained so much of the variation in the data at &gt;95%. However, I believe the reason for this became apparent on further investigation: All 7 features that shared negative coefficients were in some way related to positive sentiment or excitment around a stock’s value. The closing value of the S&P 500, the positive sentiment in investor surveys, and the number of reddit comments all relate to investors being excited about the stock in particular or the market in general. In contrast, the one positive coefficient was for the investors who responded with negative sentiment in the survey, and predictably a higher value in the first component was tied with a greater likelihood of retail investor activity decreases.\nIn contrast, TSNE’s reduced dimensions had little interpretable meaning, see below:\n\n\nCode\nincreaseActivity = (xsave['deltaActivity'] &gt; 0)\nincreaseActivity.head()\n\nincreaseActivity = increaseActivity[0:X_tsne.shape[0]]\n#plt.hist(xpca[:,0], hue=increaseActivity)\n#plt.show()\n\n# Create histograms along the 1st principal component\nplt.hist(X_tsne[increaseActivity,0],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(X_tsne[~(increaseActivity),0],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \nplt.title(\"Distribution of Increased/Decreased Activity Across the First Component (TSNE)\")\nplt.legend()\nplt.show()\n\n# Create histograms along the 2st principal component\nplt.hist(X_tsne[increaseActivity,1],  \n         alpha=0.5, # the transaparency parameter \n         label='Increased Activity') \n  \nplt.hist(X_tsne[~(increaseActivity),1],  \n         alpha=0.5, # the transaparency parameter \n         label='Decreased Activity') \n\nplt.title(\"Distribution of Increased/Decreased Activity Across the Second Component (TSNE)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWhile the TSNE result did not produce a dimension that separated the outcome variable as cleanly as PCA, it did create distributions along the new dimensions that were far more normal and less skewed in a particular dimension. Compared to the PCA dimensions that had clear skew, the TSNE distributions were symmetrical.\n\n\nCode\nplt.scatter( x= X_tsne[increaseActivity,0], y = X_tsne[increaseActivity,1],\n         alpha=0.5, # the transaparency parameter\n         color = \"Orange\", \n         label='Increased Activity') \n  \nplt.scatter( x= X_tsne[~(increaseActivity),0], y = X_tsne[~(increaseActivity),1],\n         alpha=0.5, # the transaparency parameter\n         color = \"blue\", \n         label='Decreased Activity') \n  \n# Visualize TSNE distribution\n  \nplt.title(\"Distribution of Increased/Decreased Activity Across Components\")\nplt.legend()\nplt.show()\n\n\n\n\n\nWith regard to the components themselves, a visualization shows how the selected perplexity of one has created a cluster with a dense center and sparse exterior. Coloring the points by whether there was an increase in retail trader activity, we do not see much patter, as the two distributions are intertwined. However, we can also see that the distribution of both new dimensions is clearly normal, as identified earlier when reviewing the two dimensions individually."
  },
  {
    "objectID": "dimreduction/dimreduction.html#comparing-and-contrasting-pca-and-tsne",
    "href": "dimreduction/dimreduction.html#comparing-and-contrasting-pca-and-tsne",
    "title": "Dimensionality Reduction: Introduction",
    "section": "Comparing and Contrasting PCA and TSNE",
    "text": "Comparing and Contrasting PCA and TSNE\nBetween the two methods, I believe that PCA has produced a more useful result for my research question. Because PCA expresses the new dimension as a linear combination of the input features, I was able to identify a novel relationship in the data. In particular, this relationship was that the negative sentiment aspect of the investor survey was an oppositely-valenced feature from the others in my tabular dataset. The PCA model was easy to fit overall, and the elbow plot clearly showed that the majority of variance in the data was explained by just one principle component (~95%), with a second component being the maximum I would consider. This made it a relatively easy choice to choose 2 components for PCA, but after visualizing both of them I believe the second could perhaps be considered spurious in the future.\nFor TSNE, I found the method useful for visualizing my data in only two dimensions, after having 8 dimensions to begin with. The resulting distributions were not skewed and were easy to cross-reference with my outcome variable. Plotting the perplexity values vs. the KL Divergence score of the models showed that as perplexity increased, KL Divergence got further from 0. This also made hyperparameter selection a relatively easy choice, as I was able to conclude that the lowest perplexity score was also the best one in terms of KL divergence performance. As visualized above, this did not lead to the most meaningful conclusions, but the approach was an interesting contrast with PCA nonetheless.\nIn conclusion, I think that when visualization is required T-SNE is a better choice, but when you need to understand linear relationships between your variables, PCA is a better choice, as evidenced by what each method produced for my dataset."
  },
  {
    "objectID": "Introduction/introduction.html",
    "href": "Introduction/introduction.html",
    "title": "Introduction - Why Retail Investing Matters",
    "section": "",
    "text": "Left: Anonymous, “I was addicted to Robinhood and contemplated suicide after wiping out my $70,000 savings. Here’s why I blame the investing app.,” Business Insider. Accessed: Nov. 28, 2023. [Online]. Available: https://www.businessinsider.com/addicted-to-robinhood-wallstreetbets-day-trading-lost-everything-2021-2. Right: By Mike Mozart from Funny YouTube, USA - GameStop, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=89854717."
  },
  {
    "objectID": "Introduction/introduction.html#introduction",
    "href": "Introduction/introduction.html#introduction",
    "title": "Introduction - Why Retail Investing Matters",
    "section": "Introduction",
    "text": "Introduction\nFinancial markets have become substantially more available to a wider class of investors in recent years. Brokers have developed mobile apps that can run on anyone’s personal device, trading fees have been reduced to zero in many cases, and new fintech companies have emerged that seek to make investing more available to everyday people, such as Robinhood, Acorns, Wealthfront and others. These developments came to a head in 2020, when so-called retail investing (investing by individuals who buy stocks directly and have smaller balances) exploded during the pandemic. (“New Research: Global Pandemic Brings Surge of New and Experienced Retail Investors Into the Stock Market  FINRA.org” n.d.)\nWhile broader access to financial markets could distribute the gains from investing more fairly, it also increases the risk that everyday investors will lose money through bad investments. Without the backing of institutions that is often found in retirement accounts, nothing protects indvididual investors from taking on extreme risk or buying into companies without sufficient information. Further, retail investors have a spillover effect on markets as a whole, because the sums of money involved can affect companies’ valuations and financial conditions overall. As such, understanding retail investors’ impact has become crucial to staying informed on the forces guiding market dynamics in today’s world.\nUnsurprisingly, researchers have begun to study this phenomena from several angles. Academics have found that including sentiment from retail investor forums helps their models predict prices more accurately (Jing, Wu, and Wang 2021), relying on social media data sources that have become increasingly important in financial analysis (Khan et al. 2022). Khan et. al also found that the impact of social media hype and retail investors varied in importance based on the particular stock market exchange or security being studied. Dividing up the market into smaller segments is certainly a promising area of analysis, based on this work. Overall, data science methods are well suited to address this question, and some areas such as deep learning have become embedded across topics in finance and banking (Huang, Chai, and Cho 2020)."
  },
  {
    "objectID": "Introduction/introduction.html#approach",
    "href": "Introduction/introduction.html#approach",
    "title": "Introduction - Why Retail Investing Matters",
    "section": "Approach",
    "text": "Approach\nRather than study the impact of retail investors on the stock market directly, I focused this project on studying the impact of varied factors on retail investors themselves. In particular, from the outset I knew I wanted to investigate the role of social media and sentiment on retail investors. Social media and online sentiment have been discussed as catalysts for the type of volatile, herd mentality that has allegedly overcome individual investors for particular stocks.\nTo address these questions, I will look at common measures of retail investor interest, including forum posts, social media messages and sentiment, and trading patterns for particular financial instruments."
  },
  {
    "objectID": "Introduction/introduction.html#research-questions",
    "href": "Introduction/introduction.html#research-questions",
    "title": "Introduction - Why Retail Investing Matters",
    "section": "Research Questions",
    "text": "Research Questions\nIn this project, I am seeking to investigate the following research questions:\n\nMy central question: To what extent do individual investors’ sentiments and conversations around stock tickers affect changes in trading activity in the post COVID era?\nHas the impact of online retail investor sentiment changed over time?\nWhich social media sites show the most promise for measuring retail investor interest?\nWhich types of stocks are most likely to attract retail investor interest?\nTo what extent is it possible to predict how long retail investor interest will last in a particular stock?\nIs positive or negative sentiment more consequential in impacting retail investor activity?\nWhat characteristics of social media posts determine if they will have an impact on retail activity?\nDo surveys of individual investors’ sentiment accurately reflect the sentiment of all retail investors?\nHas the total amount of retail investor activity increased or decreased overtime?\nDoes the language used in social media posts matter for their affect on retail trader activity?"
  },
  {
    "objectID": "Introduction/introduction.html#hypotheses",
    "href": "Introduction/introduction.html#hypotheses",
    "title": "Introduction - Why Retail Investing Matters",
    "section": "Hypotheses",
    "text": "Hypotheses\nIn selecting this project, I held a few theories about how the data would be connected. Firstly, I did believe in the theory that individual investors were guided by a herd-style mentality, to the extent that it might mean particular stocks saw increased interest based on social media activity. However, I did not expect this to be the sole, or even major, determinant of individual investor behavior. Based on those beliefs, I hypothesized a noticeable, but slight, effect of social media activity on the involvement of retail investors on a given stock.\nIn terms of the content of online discussions, not just their size or frequency, I felt that bullish or positive sentiment was more likely to be linked to increased activity in a stock than negative sentiment. Further I did not believe that the connection between sentiment (as measured in surveys) and trading activity changes would be strong enough to detect with the methods used for this project. Finally, I also felt doubtful of whether it would be possible to accurately predict increases or decreases in activity through online discussions and sentiment, as I had been led to believe that financial data was overwhelmingly random and difficult to predict. Nonetheless, I began the project hopeful of finding meaningful answers to the questions above."
  }
]