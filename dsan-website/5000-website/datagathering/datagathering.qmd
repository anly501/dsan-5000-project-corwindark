---
title: "Data Gathering"
engine: knitr
execute:
    freeze: true
---

## Introduction

### Finding Meaningful Retail Investor Data

While retail investor trading activity in individual companies has aroused great amounts of interest in the academic literature, practical methods for accessing this data are surprisingly limited. Even measuring which trades are truly by retail investors is a disputed matter that raises questions about the validity of any study into the subject. In this regard, the paper by Boehmer, Jones, Zhang, and Zhan [@boehmer_tracking_2020] is widely referenced and considered the "gold standard" in identifying retail trades. The authors make use of a strategy based on the tenths of a cent that are involved in a trade, alleging that trades with particular trailing digits can be flagged as made by individual investors. I pursued this strategy initially but found two problems: First, the accuracy of the method itself is disputed, with another study finding it only identified 35% of verified retail trades they placed on different accounts [@barber_subpenny_2023]. Second, the process of gathering the orimary data needed for the method from government disclosures is difficult in itself, and no third parties appeared to offer data generated with exactly this method.

After reviewing many financial databases, I ultimately identified a new data product offered by Nasdaq's data link service, which was termed the "Retail Trading Activity Tracker" and provided by The Applied Research Company. This provided two essential metrics: the porportion by dollar value of the stock's daily volume that was traded by retail investors, and the relative ranking of each stock by this proportion. I believed this data to be a high standard due to its valuation as a purchasable product, and its promotion by a prominent stock market actor in Nasdaq. Unfortunately, the full dataset required paid access, but I was able to access the top-10 stocks which had the highest retail trader activity proportion each day. By looking at the available proportion of activity, and whether each stock remained in the top-10 day over day, I was able to construct a daily binary outcome from whether each ticker increased or decreased in proportionate retail trader activity from the day before.

### A Shortage of Public Social Media Data

The next question was how to gather meaningful inputs to compare with retail activity. My research question was centered on social media data and sentiment towards financial instruments, and I intended to gather data from online communities know for discussing investments such as Reddit, Stocktwits, and Robinhood. While past studies had found a wealth of data on these subjects, I quickly realized that this data had recently become less accessible. Since roughly the start of 2020:

- Robinhood had shut down its API completely, which also closed associated datasources such as Robintrack which were used in many research papers. 
- Reddit had begun charging for access to its API, which it had previously offered for free.
- Stocktwits had also shut down its API completely, and and associated 3rd party websites called Stocksera which had collected the data appeared to go offline during the project

I mitigated these shortages by finding two datasets, one tabular and one textual, that contained information about retail traders' sentiment and social media activity. For the tabular data, I found the American Association of Individual Investors conducts a weekly survey of its members' sentiment about the market in the upcoming week (the three options being bullish, bearish, or neutral). For the textual data, I was able to use the Reddit API to make a limited number of free queries, which I used to gather the most popular posts from the largest stock trading forum. While I had originally hoped to gather an exhaustive list of posts from a variety of investment-focused forums, the limited dataset of around ~900 posts contained at least 2000 references to top-10 stocks in the desired time period, enough coverage for a meaningful investigation.

## Methods

Reviewing the methods nessecary for gathering the datasets mentioned above:
1. Daily NASDAQ data on the top-10 retail investor held companies (Python API through Quandl Package, R code for the equivalent package also listed below)
2. Text data from popular stock-trading subreddits (Reddit API through R and RedditExtractoR package)
3. Weekly polls of investor sentiment from the American Association of Individual Investors (downloaded as CSV)

### Dataset 1: Nasdaq Retail Investor Activity

R version of the NASDAQ api.
```{r}
library(tidyverse)
library(Quandl)
library(lubridate)
library(RedditExtractoR)
library(reticulate)
library(xlsx)


Quandl.api_key("psq4nx69HDimf2kQcxZJ")
data <- Quandl.datatable("NDAQ/RTAT10", paginate = TRUE)

```

Python version of the NASDAQ api.
```{python}

import pandas as pd
import numpy as np
import quandl
import nasdaqdatalink


quandl.ApiConfig.api_key = 'psq4nx69HDimf2kQcxZJ'
table1 = quandl.get_table('NDAQ/RTAT10', date='2023-09-28,2023-09-27,2023-09-26', ticker='TSLA,TQQQ,SQQQ')

print(table1)
```

Here I put my api key into the quandl function and I am able to get data going back to 2016 with no issue.



### Dataset 2: Text Data from Reddit

```{r}

# get a particular stock's mentions in titles in each of the 7 subreddits
# found subreddits by measuring overlap from https://subredditstats.com/subreddit-user-overlaps/superstonk
# which is calculated based on how likely users are to post on one or the other subs
# targeting investment related subs with >400k users
# loop through subreddits and get titles from past month
# Limited to just one of the subreddits, so I chose Wallstreet bets, the most famous

#install.packages("RedditExtractoR", repos='https://cloud.r-project.org/')


stockSubreddits <- c(
    "wallstreetbets",
    "stocks",
    "options",
    "investing",
    "stockamrket",
    "superstonk",
    "wallstreetbetsnew"
)



links <- find_thread_urls(keywords = "QQQ", subreddit = "stocks", sort_by = "top", period = "all")

links <- tibble(links)

links$ticker = "QQQ"

# get all unique stocks in the dataset of top retail-traded securities
tempClean <- read.csv('../data/01-modified-data/cleanRTAT.csv')


topStocks <- sort(table(tempClean$ticker), decreasing = TRUE)[1:100]

uqTickers <- rownames(topStocks)

print(uqTickers)

for(i in 1:length(uqTickers)) {
    print(i / nrow(uqTickers))
    
    templinks <- find_thread_urls(keywords = uqTickers[i], subreddit = "stocks", sort_by = "top", period = "all")
    templinks$ticker <- uqTickers[i]
    links <- rbind(links, templinks)
}



#write.xlsx(links,"../data/01-modified-data/sampleRedditText.xlsx")

```

Here I have gathered a list of 7 stock trading subreddits, which I created by searching for subreddits which mentioned individual stocks and investing, which also had more than ~400k subscribers. Then, I can use the find_thread_urls command to get a list of threads which mention particular keywords. For this initial gathering step, I simply gather posts with they keyword "SPY," a ticker for the S&P 500, from r/stocks, a major investing subreddit.

Updated Method: using r/stocks only, get posts for the 100 most common tickers in the most-held datatable.


### Dataset 3: AAII Investor Sentiment

```{r}
# AAII weekly survey data https://www.aaii.com/sentimentsurvey/sent_results
# read in as csv 

aaWeekly <- read.csv("./data/00-raw-data/sentiment_aaii.csv")
head(aaWeekly)


```


